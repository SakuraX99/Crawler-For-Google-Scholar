nohup: ignoring input
-------------------------------------------------
2020-08-05 09:01:23
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://178.128.28.54:3128
Trying new proxy
Working proxy: http://178.128.28.54:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://51.38.134.168:3128
Trying new proxy
Working proxy: http://35.240.132.233:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://51.38.134.168:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://35.240.132.233:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://89.218.155.78:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://51.38.134.168:3128
Trying new proxy
Working proxy: http://45.76.199.25:3128
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://51.38.134.168:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Got the results of the query
-------------------------------------------------
2020-08-05 09:17:36
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://51.38.134.168:3128
Trying new proxy
Working proxy: http://45.76.199.25:3128
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://191.232.170.36:80
Trying new proxy
Working proxy: http://45.76.199.25:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Got the results of the query
{'bib': {'abstract': 'Natural language is hierarchically structured: smaller '
                     'units (eg, phrases) are nested within larger units (eg, '
                     'clauses). When a larger constituent ends, all of the '
                     'smaller constituents that are nested within it must also '
                     'be closed. While the standard LSTM architecture allows '
                     'different neurons to track information at different time '
                     'scales, it does not have an explicit bias towards '
                     'modeling a hierarchy of constituents. This paper '
                     'proposes to add such an inductive bias by ordering the '
                     'neurons; a vector of master input and forget gates '
                     'ensures that when a',
         'author': ['Y Shen', 'S Tan', 'A Sordoni', 'A Courville'],
         'cites': '86',
         'eprint': 'https://arxiv.org/pdf/1810.09536',
         'gsrank': '1',
         'title': 'Ordered neurons: Integrating tree structures into recurrent '
                  'neural networks',
         'url': 'https://arxiv.org/abs/1810.09536',
         'venue': 'arXiv preprint arXiv:1810.09536',
         'year': '2018'},
 'citations_link': '/scholar?cites=18012332994072296158&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOrdered%2BNeurons:%2BIntegrating%2BTree%2BStructures%2Binto%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=3jq6t2yp-PkJ&ei=bXoqX6fZAaiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:3jq6t2yp-PkJ:scholar.google.com/&output=citation&scisdr=CgU4ogiHGAA:AAGBfm0AAAAAXyp8yRExUD9YWnbjLBy_K40U_-Q7biq6&scisig=AAGBfm0AAAAAXyp8ySdDU5stwzseZIyUD9uRkfsfNYfh&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
86
-------------------------------------------------
2020-08-05 09:22:57
Got the results of the query
{'bib': {'abstract': 'Despite recent progress in generative image modeling, '
                     'successfully generating high-resolution, diverse samples '
                     'from complex datasets such as ImageNet remains an '
                     'elusive goal. To this end, we train Generative '
                     'Adversarial Networks at the largest scale yet attempted, '
                     'and study the instabilities specific to such scale. We '
                     'find that applying orthogonal regularization to the '
                     'generator renders it amenable to a simple" truncation '
                     'trick," allowing fine control over the trade-off between '
                     'sample fidelity and variety by reducing the variance of',
         'author': ['A Brock', 'J Donahue', 'K Simonyan'],
         'cites': '1031',
         'eprint': 'https://arxiv.org/pdf/1809.11096',
         'gsrank': '1',
         'title': 'Large scale gan training for high fidelity natural image '
                  'synthesis',
         'url': 'https://arxiv.org/abs/1809.11096',
         'venue': 'arXiv preprint arXiv:1809.11096',
         'year': '2018'},
 'citations_link': '/scholar?cites=9573828555610570748&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLarge%2BScale%2BGAN%2BTraining%2Bfor%2BHigh%2BFidelity%2BNatural%2BImage%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=_L-o1VgS3YQJ&ei=dXoqX_qUKZqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:_L-o1VgS3YQJ:scholar.google.com/&output=citation&scisdr=CgWyM-pZGAA:AAGBfm0AAAAAXyp80TC22y-3Rl-_qX6QjdLBwwVOyo0M&scisig=AAGBfm0AAAAAXyp80ev_tX6r1kkgjMZM7U3bXLfAr08X&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
1031
-------------------------------------------------
2020-08-05 09:23:05
Trying new proxy
Working proxy: http://81.201.60.130:80
Got the results of the query
-------------------------------------------------
2020-08-05 09:24:23
Got the results of the query
{'bib': {'abstract': 'Feature selection is a pervasive problem. The discovery '
                     'of relevant features can be as important for performing '
                     'a particular task (such as to avoid overfitting in '
                     'prediction) as it can be for understanding the '
                     'underlying processes governing the true label (such as '
                     'discovering relevant genetic factors for a disease). '
                     'Machine learning driven feature selection can enable '
                     'discovery from large, high-dimensional, non-linear '
                     'observational datasets by creating a subset of features '
                     'for experts to focus on. In order to use expert time '
                     'most efficiently, we',
         'author': ['J Jordon', 'J Yoon', 'M van der Schaar'],
         'cites': '15',
         'eprint': 'https://openreview.net/pdf?id=ByeZ5jC5YQ',
         'gsrank': '1',
         'title': 'KnockoffGAN: Generating knockoffs for feature selection '
                  'using generative adversarial networks',
         'url': 'https://openreview.net/forum?id=ByeZ5jC5YQ&source=post_page---------------------------',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=2283856155498112552&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKnockoffGAN:%2BGenerating%2BKnockoffs%2Bfor%2BFeature%2BSelection%2Busing%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=KOaNshLjsR8J&ei=0noqX9L6AovrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:KOaNshLjsR8J:scholar.google.com/&output=citation&scisdr=CgWyM-pZGAA:AAGBfm0AAAAAXyp9ML1dzrGCjLvNMSXzzKUuVVQCE4I5&scisig=AAGBfm0AAAAAXyp9MGSBkHMpnUpHA0DUU_upBGhcqAOd&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 09:24:40
Got the results of the query
-------------------------------------------------
2020-08-05 09:24:57
Got the results of the query
{'bib': {'abstract': 'Convolutional Neural Networks (CNNs) are commonly '
                     'thought to recognise objects by learning increasingly '
                     'complex representations of object shapes. Some recent '
                     'studies suggest a more important role of image textures. '
                     'We here put these conflicting hypotheses to a '
                     'quantitative test by evaluating CNNs and human observers '
                     'on images with a texture-shape cue conflict. We show '
                     'that ImageNet-trained CNNs are strongly biased towards '
                     'recognising textures rather than shapes, which is in '
                     'stark contrast to human behavioural',
         'author': ['R Geirhos', 'P Rubisch', 'C Michaelis', 'M Bethge'],
         'cites': '365',
         'eprint': 'https://arxiv.org/pdf/1811.12231',
         'gsrank': '1',
         'title': 'ImageNet-trained CNNs are biased towards texture; '
                  'increasing shape bias improves accuracy and robustness',
         'url': 'https://arxiv.org/abs/1811.12231',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=14190455085351957023&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImageNet-trained%2BCNNs%2Bare%2Bbiased%2Btowards%2Btexture%253B%2Bincreasing%2Bshape%2Bbias%2Bimproves%2Baccuracy%2Band%2Brobustness%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=H_KJQ7ef7sQJ&ei=-HoqX6PqKoS0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:H_KJQ7ef7sQJ:scholar.google.com/&output=citation&scisdr=CgWW_y2-GAA:AAGBfm0AAAAAXyp9VLX1Gy1kbCbK4pjxlPT0QInepTaD&scisig=AAGBfm0AAAAAXyp9VF7IsQRiHNhN77fItCtq-rl7Wjb4&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
365
-------------------------------------------------
2020-08-05 09:25:16
Got the results of the query
{'bib': {'abstract': 'There is growing interest in geometrically-inspired '
                     'embeddings for learning hierarchies, partial orders, and '
                     'lattice structures, with natural applications to '
                     'transitive relational data such as entailment graphs. '
                     'Recent work has extended these ideas beyond '
                     'deterministic hierarchies to probabilistically '
                     'calibrated models, which enable learning from uncertain '
                     'supervision and inferring soft-inclusions among '
                     'concepts, while maintaining the geometric inductive bias '
                     'of hierarchical embedding models. We build on the Box '
                     'Lattice model of Vilnis',
         'author': ['X Li', 'L Vilnis', 'D Zhang', 'M Boratko'],
         'cites': '11',
         'eprint': 'https://openreview.net/pdf?id=H1xSNiRcF7',
         'gsrank': '1',
         'title': 'Smoothing the geometry of probabilistic box embeddings',
         'url': 'https://openreview.net/forum?id=H1xSNiRcF7&source=post_page-----c51f6102a87a----------------------',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8166135549126473331&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSmoothing%2Bthe%2BGeometry%2Bof%2BProbabilistic%2BBox%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=c3ZfEQTxU3EJ&ei=AnsqX4itBobuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:c3ZfEQTxU3EJ:scholar.google.com/&output=citation&scisdr=CgWW_y2-GAA:AAGBfm0AAAAAXyp9X_dqMCiPzhUygeWuQixF-HdXM7eL&scisig=AAGBfm0AAAAAXyp9XzjAqCzb8Z2zyVZW4bUEq1qbnjaY&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
11
-------------------------------------------------
2020-08-05 09:25:27
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Got the results of the query
-------------------------------------------------
2020-08-05 09:26:28
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://206.198.131.172:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://45.76.199.25:3128
Trying new proxy
Working proxy: http://45.76.199.25:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://45.76.199.25:3128
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://81.201.60.130:80
Got the results of the query
{'bib': {'abstract': 'A major goal of unsupervised learning is to discover '
                     'data representations that are useful for subsequent '
                     'tasks, without access to supervised labels during '
                     'training. Typically, this involves minimizing a '
                     'surrogate objective, such as the negative log likelihood '
                     'of a generative model, with the hope that '
                     'representations useful for subsequent tasks will arise '
                     'as a side effect. In this work, we propose instead to '
                     'directly target later desired tasks by meta-learning an '
                     'unsupervised learning rule which leads to '
                     'representations useful for those tasks',
         'author': ['L Metz', 'N Maheswaranathan', 'B Cheung'],
         'cites': '31',
         'eprint': 'https://arxiv.org/pdf/1804.00222',
         'gsrank': '1',
         'title': 'Meta-learning update rules for unsupervised representation '
                  'learning',
         'url': 'https://arxiv.org/abs/1804.00222',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=5989711063339819997&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-Learning%2BUpdate%2BRules%2Bfor%2BUnsupervised%2BRepresentation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=3eOYLXq6H1MJ&ei=e3wqX8fjEYKTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:3eOYLXq6H1MJ:scholar.google.com/&output=citation&scisdr=CgVGh4u6GAA:AAGBfm0AAAAAXyp-18xfzo90oXthB-MIYyl6R7xVk3W0&scisig=AAGBfm0AAAAAXyp-1zp7A9JV9GxYmjcSu4Lbt4LgWglz&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
31
-------------------------------------------------
2020-08-05 09:31:43
Got the results of the query
-------------------------------------------------
2020-08-05 09:32:05
Got the results of the query
{'bib': {'abstract': 'The unconditional generation of high fidelity images is '
                     'a longstanding benchmark for testing the performance of '
                     'image decoders. Autoregressive image models have been '
                     'able to generate small images unconditionally, but the '
                     'extension of these methods to large images where '
                     'fidelity can be more readily assessed has remained an '
                     'open problem. Among the major challenges are the '
                     'capacity to encode the vast previous context and the '
                     'sheer difficulty of learning a distribution that '
                     'preserves both global semantic coherence and',
         'author': ['J Menick', 'N Kalchbrenner'],
         'cites': '42',
         'eprint': 'https://arxiv.org/pdf/1812.01608',
         'gsrank': '1',
         'title': 'Generating high fidelity images with subscale pixel '
                  'networks and multidimensional upscaling',
         'url': 'https://arxiv.org/abs/1812.01608',
         'venue': 'arXiv preprint arXiv:1812.01608',
         'year': '2018'},
 'citations_link': '/scholar?cites=6361435766800029488&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerating%2BHigh%2Bfidelity%2BImages%2Bwith%2Bsubscale%2Bpixel%2BNetworks%2Band%2BMultidimensional%2BUpscaling%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=MJeUJSBcSFgJ&ei=mXwqX7y1K4jHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:MJeUJSBcSFgJ:scholar.google.com/&output=citation&scisdr=CgU4ogn6GAA:AAGBfm0AAAAAXyp-9r4bcC-bH6S8sME5TJFC53LyWt_h&scisig=AAGBfm0AAAAAXyp-9rIuwhHOp7oNGtfwYKgw1F3NtYXV&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
42
-------------------------------------------------
2020-08-05 09:32:14
Got the results of the query
{'bib': {'abstract': 'To act and plan in complex environments, we posit that '
                     'agents should have a mental simulator of the world with '
                     'three characteristics:(a) it should build an abstract '
                     'state representing the condition of the world;(b) it '
                     'should form a belief which represents uncertainty on the '
                     'world;(c) it should go beyond simple step-by-step '
                     'simulation, and exhibit temporal abstraction. Motivated '
                     'by the absence of a model satisfying all these '
                     'requirements, we propose TD-VAE, a generative sequence '
                     'model that learns representations containing',
         'author': ['K Gregor', 'G Papamakarios', 'F Besse', 'L Buesing'],
         'cites': '38',
         'eprint': 'https://arxiv.org/pdf/1806.03107',
         'gsrank': '1',
         'title': 'Temporal difference variational auto-encoder',
         'url': 'https://arxiv.org/abs/1806.03107',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=2182933855734309592&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTemporal%2BDifference%2BVariational%2BAuto-Encoder%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=2KI3r8ZWSx4J&ei=pHwqXySxlMvWD4nAibAM',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:2KI3r8ZWSx4J:scholar.google.com/&output=citation&scisdr=CgU4ogn6GAA:AAGBfm0AAAAAXyp-_32zoR5d_tDdvFvceonBNEv074qY&scisig=AAGBfm0AAAAAXyp-_6il-l-yYOFKjBZnP_WSgVc6ZZ7H&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
38
-------------------------------------------------
2020-08-05 09:32:23
Got the results of the query
{'bib': {'abstract': 'The visual system is hierarchically organized to process '
                     'visual information in successive stages. Neural '
                     'representations vary drastically across the first stages '
                     'of visual processing: at the output of the retina, '
                     'ganglion cell receptive fields (RFs) exhibit a clear '
                     'antagonistic center-surround structure, whereas in the '
                     'primary visual cortex, typical RFs are sharply tuned to '
                     'a precise orientation. There is currently no unified '
                     'theory explaining these differences in representations '
                     'across layers. Here, using a deep convolutional neural '
                     'network trained on',
         'author': ['J Lindsey', 'SA Ocko', 'S Ganguli', 'S Deny'],
         'cites': '21',
         'eprint': 'https://arxiv.org/pdf/1901.00945',
         'gsrank': '1',
         'title': 'A unified theory of early visual representations from '
                  'retina to cortex through anatomically constrained deep CNNs',
         'url': 'https://arxiv.org/abs/1901.00945',
         'venue': 'arXiv preprint arXiv:1901.00945',
         'year': '2019'},
 'citations_link': '/scholar?cites=2073469512347644047&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BUnified%2BTheory%2Bof%2BEarly%2BVisual%2BRepresentations%2Bfrom%2BRetina%2Bto%2BCortex%2Bthrough%2BAnatomically%2BConstrained%2BDeep%2BCNNs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=j3AVKIlxxhwJ&ei=rnwqX4lIwrCYAfq9qagM',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:j3AVKIlxxhwJ:scholar.google.com/&output=citation&scisdr=CgXpiidhGAA:AAGBfm0AAAAAXyp_CwCguGBgg4QpzMP1A6nRBWLxOF5A&scisig=AAGBfm0AAAAAXyp_C-VnEBG_nlFGPtWI3crxlXa_kBiB&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
21
-------------------------------------------------
2020-08-05 09:32:35
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://45.76.199.25:3128
Trying new proxy
Working proxy: http://206.198.131.172:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://49.0.82.190:8080
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Got the results of the query
{'bib': {'abstract': 'Self-attention is a useful mechanism to build generative '
                     'models for language and images. It determines the '
                     'importance of context elements by comparing each element '
                     'to the current time step. In this paper, we show that a '
                     'very lightweight convolution can perform competitively '
                     'to the best reported self-attention results. Next, we '
                     'introduce dynamic convolutions which are simpler and '
                     'more efficient than self-attention. We predict separate '
                     'convolution kernels based solely on the current '
                     'time-step in order to determine the',
         'author': ['F Wu', 'A Fan', 'A Baevski', 'YN Dauphin', 'M Auli'],
         'cites': '136',
         'eprint': 'https://arxiv.org/pdf/1901.10430',
         'gsrank': '1',
         'title': 'Pay less attention with lightweight and dynamic '
                  'convolutions',
         'url': 'https://arxiv.org/abs/1901.10430',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=3358231780148394025&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPay%2BLess%2BAttention%2Bwith%2BLightweight%2Band%2BDynamic%2BConvolutions%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=KTgs6QLWmi4J&ei=1n4qX83OBYvrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:KTgs6QLWmi4J:scholar.google.com/&output=citation&scisdr=CgUmJmqGGAA:AAGBfm0AAAAAXyqBNNNPIRYtwBJTVATMk3yEz_7gHLmc&scisig=AAGBfm0AAAAAXyqBNK7k5T5sW13bv2yrXocu9XThc5m5&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
136
-------------------------------------------------
2020-08-05 09:41:48
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://51.38.134.168:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://128.199.98.232:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://67.205.175.175:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://67.205.175.175:3128
Trying new proxy
Working proxy: http://34.105.59.26:80
Got the results of the query
{'bib': {'abstract': 'Generating musical audio directly with neural networks '
                     'is notoriously difficult because it requires coherently '
                     'modeling structure at many different timescales. '
                     'Fortunately, most music is also highly structured and '
                     'can be represented as discrete note events played on '
                     'musical instruments. Herein, we show that by using notes '
                     'as an intermediate representation, we can train a suite '
                     'of models capable of transcribing, composing, and '
                     'synthesizing audio waveforms with coherent musical '
                     'structure on timescales spanning six orders of magnitude',
         'author': ['C Hawthorne', 'A Stasyuk', 'A Roberts', 'I Simon'],
         'cites': '55',
         'eprint': 'https://arxiv.org/pdf/1810.12247',
         'gsrank': '1',
         'title': 'Enabling factorized piano music modeling and generation '
                  'with the MAESTRO dataset',
         'url': 'https://arxiv.org/abs/1810.12247',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=13653431951208907586&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnabling%2BFactorized%2BPiano%2BMusic%2BModeling%2Band%2BGeneration%2Bwith%2Bthe%2BMAESTRO%2BDataset%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=QoPGSAW8er0J&ei=d4AqX8-XKouayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:QoPGSAW8er0J:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyrkMswTkmj4QpE:AAGBfm0AAAAAXyqC09EZGGjfWpHN3jCkRudSUyNVyC3T&scisig=AAGBfm0AAAAAXyqC0ygvyDGPoCxRCPaebefn2mekIYd6&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
55
-------------------------------------------------
2020-08-05 09:48:43
Got the results of the query
{'bib': {'abstract': 'Memory-augmented neural networks consisting of a neural '
                     'controller and an external memory have shown potentials '
                     'in long-term sequential learning. Current RAM-like '
                     'memory models maintain memory accessing every timesteps, '
                     'thus they do not effectively leverage the short-term '
                     'memory held in the controller. We hypothesize that this '
                     'scheme of writing is suboptimal in memory utilization '
                     'and introduces redundant computation. To validate our '
                     'hypothesis, we derive a theoretical bound on the amount '
                     'of information stored in a RAM-like',
         'author': ['H Le', 'T Tran', 'S Venkatesh'],
         'cites': '14',
         'eprint': 'https://arxiv.org/pdf/1901.01347',
         'gsrank': '1',
         'title': 'Learning to remember more with less memorization',
         'url': 'https://arxiv.org/abs/1901.01347',
         'venue': 'arXiv preprint arXiv:1901.01347',
         'year': '2019'},
 'citations_link': '/scholar?cites=10822310561181575410&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BRemember%2BMore%2Bwith%2BLess%2BMemorization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=8qgvyB2SMJYJ&ei=fYAqX4SWHZmG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:8qgvyB2SMJYJ:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyrkO4_Mrg0hESE:AAGBfm0AAAAAXyqC2pLGJA0GCSEE3K98CAiaBXf5qtlP&scisig=AAGBfm0AAAAAXyqC2uVf-0OlxTmU2QL_-6csFzByhaPd&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
14
-------------------------------------------------
2020-08-05 09:48:50
Got the results of the query
{'bib': {'abstract': 'Despite impressive performance as evaluated on iid '
                     'holdout data, deep neural networks depend heavily on '
                     'superficial statistics of the training data and are '
                     'liable to break under distribution shift. For example, '
                     'subtle changes to the background or texture of an image '
                     'can break a seemingly powerful classifier. Building on '
                     'previous work on domain generalization, we hope to '
                     'produce a classifier that will generalize to previously '
                     'unseen domains, even when domain identifiers are not '
                     'available during training. This setting is challenging',
         'author': ['H Wang', 'Z He', 'ZC Lipton', 'EP Xing'],
         'cites': '25',
         'eprint': 'https://arxiv.org/pdf/1903.06256',
         'gsrank': '1',
         'title': 'Learning robust representations by projecting superficial '
                  'statistics out',
         'url': 'https://arxiv.org/abs/1903.06256',
         'venue': 'arXiv preprint arXiv:1903.06256',
         'year': '2019'},
 'citations_link': '/scholar?cites=10744255442611850331&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BRobust%2BRepresentations%2Bby%2BProjecting%2BSuperficial%2BStatistics%2BOut%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=W8zvSGZDG5UJ&ei=hoAqX-qACIuayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:W8zvSGZDG5UJ:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyrkHo_p0o9t2OY:AAGBfm0AAAAAXyqC_5LjWI9KwOZmNziS-yn1ndpCaiGP&scisig=AAGBfm0AAAAAXyqC_2AOo2_jgeMmzyavHkkoWX7DgkcK&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
25
-------------------------------------------------
2020-08-05 09:49:27
Got the results of the query
{'bib': {'abstract': 'As Machine Learning (ML) gets applied to '
                     'security-critical or sensitive domains, there is a '
                     'growing need for integrity and privacy for outsourced ML '
                     'computations. A pragmatic solution comes from Trusted '
                     'Execution Environments (TEEs), which use hardware and '
                     'software protections to isolate sensitive computations '
                     'from the untrusted software stack. However, these '
                     'isolation guarantees come at a price in performance, '
                     'compared to untrusted alternatives. This paper initiates '
                     'the study of high performance execution of Deep Neural',
         'author': ['F Tramer', 'D Boneh'],
         'cites': '57',
         'eprint': 'https://arxiv.org/pdf/1806.03287',
         'gsrank': '1',
         'title': 'Slalom: Fast, verifiable and private execution of neural '
                  'networks in trusted hardware',
         'url': 'https://arxiv.org/abs/1806.03287',
         'venue': 'arXiv preprint arXiv:1806.03287',
         'year': '2018'},
 'citations_link': '/scholar?cites=7461531422951047390&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSlalom:%2BFast,%2BVerifiable%2Band%2BPrivate%2BExecution%2Bof%2BNeural%2BNetworks%2Bin%2BTrusted%2BHardware%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=3oBp2UuvjGcJ&ei=s4AqX_rrIYS0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:3oBp2UuvjGcJ:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyrlxe_sNiAKuqY:AAGBfm0AAAAAXyqDJPLmvCAtoqZp5_hm8FEqS4bHTQ8S&scisig=AAGBfm0AAAAAXyqDJOPTSpTGaatM7h1HNRzjZQssHb4C&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
57
-------------------------------------------------
2020-08-05 09:50:05
Got the results of the query
{'bib': {'abstract': 'We propose the Neuro-Symbolic Concept Learner (NS-CL), a '
                     'model that learns visual concepts, words, and semantic '
                     'parsing of sentences without explicit supervision on any '
                     'of them; instead, our model learns by simply looking at '
                     'images and reading paired questions and answers. Our '
                     'model builds an object-based scene representation and '
                     'translates sentences into executable, symbolic programs. '
                     'To bridge the learning of two modules, we use a '
                     'neuro-symbolic reasoning module that executes these '
                     'programs on the latent scene',
         'author': ['J Mao', 'C Gan', 'P Kohli', 'JB Tenenbaum', 'J Wu'],
         'cites': '92',
         'eprint': 'https://arxiv.org/pdf/1904.12584',
         'gsrank': '1',
         'title': 'The neuro-symbolic concept learner: Interpreting scenes, '
                  'words, and sentences from natural supervision',
         'url': 'https://arxiv.org/abs/1904.12584',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=8837128214653317831&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BNeuro-Symbolic%2BConcept%2BLearner:%2BInterpreting%2BScenes,%2BWords,%2Band%2BSentences%2BFrom%2BNatural%2BSupervision%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=xzL-RErJo3oJ&ei=0IAqX6eZLYbuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:xzL-RErJo3oJ:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyrlzPcGMFVxTkk:AAGBfm0AAAAAXyqDLeoMulVWVkmAaS_JBj1TBoSjTALv&scisig=AAGBfm0AAAAAXyqDLTvH7cyk68ik_tp4jVpw3v9Cnn9j&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
92
-------------------------------------------------
2020-08-05 09:50:14
Got the results of the query
{'bib': {'abstract': 'Neural network pruning techniques can reduce the '
                     'parameter counts of trained networks by over 90%, '
                     'decreasing storage requirements and improving '
                     'computational performance of inference without '
                     'compromising accuracy. However, contemporary experience '
                     'is that the sparse architectures produced by pruning are '
                     'difficult to train from the start, which would similarly '
                     'improve training performance. We find that a standard '
                     'pruning technique naturally uncovers subnetworks whose '
                     'initializations made them capable of training '
                     'effectively',
         'author': ['J Frankle', 'M Carbin'],
         'cites': '144',
         'eprint': 'https://arxiv.org/pdf/1803.03635',
         'gsrank': '1',
         'title': 'The lottery ticket hypothesis: Finding sparse, trainable '
                  'neural networks',
         'url': 'https://arxiv.org/abs/1803.03635',
         'venue': 'arXiv preprint arXiv:1803.03635',
         'year': '2018'},
 'citations_link': '/scholar?cites=14267585926782353027&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BLottery%2BTicket%2BHypothesis:%2BFinding%2BSparse,%2BTrainable%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=g345uM6lAMYJ&ei=44AqX6SyHJmG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:g345uM6lAMYJ:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyrloUek6j-fMNQ:AAGBfm0AAAAAXyqDQFquYD-4KNS06Fgejzb1SmWynbQt&scisig=AAGBfm0AAAAAXyqDQB5BJR4S3PnitCp_-81-iVCiRYoZ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
144
-------------------------------------------------
2020-08-05 09:50:32
Got the results of the query
{'bib': {'abstract': 'A promising class of generative models maps points from '
                     'a simple distribution to a complex distribution through '
                     'an invertible neural network. Likelihood-based training '
                     'of these models requires restricting their architectures '
                     'to allow cheap computation of Jacobian determinants. '
                     'Alternatively, the Jacobian trace can be used if the '
                     'transformation is specified by an ordinary differential '
                     "equation. In this paper, we use Hutchinson's trace "
                     'estimator to give a scalable unbiased estimate of the '
                     'log-density. The result is a continuous-time invertible '
                     'generative',
         'author': ['W Grathwohl', 'RTQ Chen', 'J Bettencourt'],
         'cites': '196',
         'eprint': 'https://arxiv.org/pdf/1810.01367',
         'gsrank': '1',
         'title': 'Ffjord: Free-form continuous dynamics for scalable '
                  'reversible generative models',
         'url': 'https://arxiv.org/abs/1810.01367',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=12849237214531885593&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFFJORD:%2BFree-Form%2BContinuous%2BDynamics%2Bfor%2BScalable%2BReversible%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=GRrnHiupUbIJ&ei=E4EqX47LHYbuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:GRrnHiupUbIJ:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyrlZR9RE74okRk:AAGBfm0AAAAAXyqDhAJbmb4PiRm1-1W3jfaT8Kt_9hje&scisig=AAGBfm0AAAAAXyqDhAcrAH5rm5DkzW3oavLQ6Gacav3N&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
196
-------------------------------------------------
2020-08-05 09:51:40
Got the results of the query
{'bib': {'abstract': 'Graph Neural Networks (GNNs) are an effective framework '
                     'for representation learning of graphs. GNNs follow a '
                     'neighborhood aggregation scheme, where the '
                     'representation vector of a node is computed by '
                     'recursively aggregating and transforming representation '
                     'vectors of',
         'author': ['K Xu', 'W Hu', 'J Leskovec', 'S Jegelka'],
         'cites': '609',
         'eprint': 'https://arxiv.org/pdf/1810.00826',
         'gsrank': '1',
         'title': 'How powerful are graph neural networks?',
         'url': 'https://arxiv.org/abs/1810.00826',
         'venue': 'arXiv preprint arXiv:1810.00826',
         'year': '2018'},
 'citations_link': '/scholar?cites=9955904491400591671&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHow%2BPowerful%2Bare%2BGraph%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=N_ml42J6KooJ&ei=QoEqX76qBobuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:N_ml42J6KooJ:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyrlf3iOd9GBpB8:AAGBfm0AAAAAXyqDnmWE_dGmvB_0_DRe9jXKXZCeGMb6&scisig=AAGBfm0AAAAAXyqDnuq5iMH_WMz8HG0KE0dZ3iewt74j&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
609
-------------------------------------------------
2020-08-05 09:52:06
Got the results of the query
{'bib': {'abstract': 'Convolutional Neural Networks (CNN) have been successful '
                     'in processing data signals that are uniformly sampled in '
                     'the spatial domain (eg, images). However, most data '
                     'signals do not natively exist on a grid, and in the '
                     'process of being sampled onto a uniform physical grid '
                     'suffer significant aliasing error and information loss. '
                     'Moreover, signals can exist in different topological '
                     'structures as, for example, points, lines, surfaces and '
                     'volumes. It has been challenging to analyze signals with '
                     'mixed topologies (for example, point cloud with surface',
         'author': ['C Jiang', 'D Wang', 'J Huang', 'P Marcus'],
         'cites': '6',
         'eprint': 'https://arxiv.org/pdf/1901.02070',
         'gsrank': '1',
         'title': 'Convolutional neural networks on non-uniform geometrical '
                  'signals using euclidean spectral transformation',
         'url': 'https://arxiv.org/abs/1901.02070',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=15609737605726839540&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DConvolutional%2BNeural%2BNetworks%2Bon%2BNon-uniform%2BGeometrical%2BSignals%2BUsing%2BEuclidean%2BSpectral%2BTransformation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=9KJnAqbtoNgJ&ei=SoEqX526C532ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:9KJnAqbtoNgJ:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyrlWThRmgWS1Bs:AAGBfm0AAAAAXyqDuCVbEAW1zBubi1QXk-xwwTeTgyY2&scisig=AAGBfm0AAAAAXyqDuPwq4f-PVMyjwhtrvPI3eKxh0lj-&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 09:52:32
Got the results of the query
{'bib': {'abstract': 'Training a model to perform a task typically requires a '
                     'large amount of data from the domains in which the task '
                     'will be applied. However, it is often the case that data '
                     'are abundant in some domains but scarce in others. '
                     'Domain adaptation deals with the challenge of adapting a '
                     'model trained from a data-rich source domain to perform '
                     'well in a data-poor target domain. In general, this '
                     'requires learning plausible mappings between domains. '
                     'CycleGAN is a powerful framework that efficiently learns '
                     'to map inputs from one',
         'author': ['E Hosseini-Asl', 'Y Zhou', 'C Xiong', 'R Socher'],
         'cites': '13',
         'eprint': 'https://arxiv.org/pdf/1807.00374',
         'gsrank': '1',
         'title': 'Augmented cyclic adversarial learning for low resource '
                  'domain adaptation',
         'url': 'https://arxiv.org/abs/1807.00374',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11441136156642140785&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAugmented%2BCyclic%2BAdversarial%2BLearning%2Bfor%2BLow%2BResource%2BDomain%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=cWKIe7cUx54J&ei=boEqX-v_NIKTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:cWKIe7cUx54J:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyrlKwh60vm6Fe0:AAGBfm0AAAAAXyqDyhVwWPmdDe0eer0eCoRkuU5le4ap&scisig=AAGBfm0AAAAAXyqDyrGDe-DK8VDZtwBZYH5tRp1FgsCT&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
13
-------------------------------------------------
2020-08-05 09:52:50
Got the results of the query
{'bib': {'abstract': 'Ordinary stochastic neural networks mostly rely on the '
                     'expected values of their weights to make predictions, '
                     'whereas the induced noise is mostly used to capture the '
                     'uncertainty, prevent overfitting and slightly boost the '
                     'performance through test-time averaging. In this paper, '
                     'we introduce variance layers, a different kind of '
                     'stochastic layers. Each weight of a variance layer '
                     'follows a zero-mean distribution and is only '
                     'parameterized by its variance. We show that such layers '
                     'can learn surprisingly well, can serve as an efficient '
                     'exploration',
         'author': ['K Neklyudov', 'D Molchanov', 'A Ashukha'],
         'cites': '10',
         'eprint': 'https://arxiv.org/pdf/1803.03764',
         'gsrank': '1',
         'title': 'Variance networks: When expectation does not meet your '
                  'expectations',
         'url': 'https://arxiv.org/abs/1803.03764',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=3938870273847182783&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariance%2BNetworks:%2BWhen%2BExpectation%2BDoes%2BNot%2BMeet%2BYour%2BExpectations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=v8UBIaWtqTYJ&ei=foEqX7fEC4KTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:v8UBIaWtqTYJ:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyrlOqcvLlKteRk:AAGBfm0AAAAAXyqD27olpFKKYRk6oH5VnLgQjIxDB-BZ&scisig=AAGBfm0AAAAAXyqD2xs0O9lWt68JvGeQNmSWuzG1JipO&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
10
-------------------------------------------------
2020-08-05 09:53:07
Got the results of the query
{'bib': {'abstract': 'Deep neural networks are almost universally trained with '
                     'reverse-mode automatic differentiation (aka '
                     'backpropagation). Biological networks, on the other '
                     'hand, appear to lack any mechanism for sending gradients '
                     'back to their input neurons, and thus cannot be learning '
                     'in this way. In response to this, Scellier & Bengio '
                     '(2017) proposed Equilibrium Propagation-a method for '
                     'gradient-based train-ing of neural networks which uses '
                     'only local learning rules and, crucially, does not rely '
                     'on neurons having a mechanism for back',
         'author': ["P O'Connor", 'E Gavves', 'M Welling'],
         'cites': '1',
         'eprint': 'https://openreview.net/pdf?id=B1GMDsR5tm',
         'gsrank': '1',
         'title': 'Initialized equilibrium propagation for backprop-free '
                  'training',
         'url': 'https://openreview.net/forum?id=B1GMDsR5tm',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15931102343166725964&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInitialized%2BEquilibrium%2BPropagation%2Bfor%2BBackprop-Free%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=TI_YrCylFt0J&ei=kIEqX5LIK8yXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:TI_YrCylFt0J:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyrlDUAdBfhTXbI:AAGBfm0AAAAAXyqD7F0Xj_h0RbJaMRvQm44GZ5otEsDy&scisig=AAGBfm0AAAAAXyqD7MfNZPICRcKKiAwFERrEEDH2sPDv&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
1
-------------------------------------------------
2020-08-05 09:53:24
Got the results of the query
{'bib': {'abstract': 'When an image classifier makes a prediction, which parts '
                     'of the image are relevant and why? We can rephrase this '
                     'question to ask: which parts of the image, if they were '
                     'not seen by the classifier, would most change its '
                     'decision? Producing an answer requires marginalizing '
                     "over images that could have been seen but weren't. We "
                     'can sample plausible image in-fills by conditioning a '
                     'generative model on the rest of the image. We then '
                     'optimize to find the image regions that most change the '
                     "classifier's decision after in-fill. Our approach",
         'author': ['CH Chang', 'E Creager', 'A Goldenberg'],
         'cites': '35',
         'eprint': 'https://arxiv.org/pdf/1807.08024',
         'gsrank': '1',
         'title': 'Explaining image classifiers by counterfactual generation',
         'url': 'https://arxiv.org/abs/1807.08024',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=6313449476805696850&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExplaining%2BImage%2BClassifiers%2Bby%2BCounterfactual%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Uh1hXNngnVcJ&ei=mYEqX_zvB4buygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Uh1hXNngnVcJ:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyrlFIu7MVM8Q5Q:AAGBfm0AAAAAXyqD9Zaxu1MbW5QmOwFHylHb-QnFa_gI&scisig=AAGBfm0AAAAAXyqD9ciYp6dCiUyw8SozlZlqTCw4aN0O&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
35
-------------------------------------------------
2020-08-05 09:53:33
Got the results of the query
{'bib': {'abstract': 'Pruning large neural networks while maintaining their '
                     'performance is often desirable due to the reduced space '
                     'and time complexity. In existing methods, pruning is '
                     'done within an iterative optimization procedure with '
                     'either heuristically designed pruning schedules or '
                     'additional hyperparameters, undermining their utility. '
                     'In this work, we present a new approach that prunes a '
                     'given network once at initialization prior to training. '
                     'To achieve this, we introduce a saliency criterion based '
                     'on connection sensitivity that identifies structurally',
         'author': ['N Lee', 'T Ajanthan', 'PHS Torr'],
         'cites': '91',
         'eprint': 'https://arxiv.org/pdf/1810.02340',
         'gsrank': '1',
         'title': 'Snip: Single-shot network pruning based on connection '
                  'sensitivity',
         'url': 'https://arxiv.org/abs/1810.02340',
         'venue': 'arXiv preprint arXiv:1810.02340',
         'year': '2018'},
 'citations_link': '/scholar?cites=9820036975414969048&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSnip:%2Bsingle-Shot%2BNetwork%2BPruning%2Bbased%2Bon%2BConnection%2Bsensitivity%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=2ILPsJnHR4gJ&ei=q4EqX-q2MYS0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:2ILPsJnHR4gJ:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyri5ze2NN1O924:AAGBfm0AAAAAXyqEBiq8vt1p727e0hR4e7VDRH0ZZcx-&scisig=AAGBfm0AAAAAXyqEBngerFZsMlk1yGoWNCO6EfW6Issw&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
91
-------------------------------------------------
2020-08-05 09:53:50
Got the results of the query
{'bib': {'abstract': 'Although variational autoencoders (VAEs) represent a '
                     'widely influential deep generative model, many aspects '
                     'of the underlying energy function remain poorly '
                     'understood. In particular, it is commonly believed that '
                     'Gaussian encoder/decoder assumptions reduce the '
                     'effectiveness of VAEs in generating realistic samples. '
                     'In this regard, we rigorously analyze the VAE objective, '
                     'differentiating situations where this belief is and is '
                     'not actually true. We then leverage the corresponding '
                     'insights to develop a simple VAE enhancement that',
         'author': ['B Dai', 'D Wipf'],
         'cites': '78',
         'eprint': 'https://arxiv.org/pdf/1903.05789',
         'gsrank': '1',
         'title': 'Diagnosing and enhancing VAE models',
         'url': 'https://arxiv.org/abs/1903.05789',
         'venue': 'arXiv preprint arXiv:1903.05789',
         'year': '2019'},
 'citations_link': '/scholar?cites=15377413262741867924&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiagnosing%2Band%2BEnhancing%2BVAE%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=lA0csuOLZ9UJ&ei=vIEqX_j7G5mG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:lA0csuOLZ9UJ:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyri-4I5U0M_hbc:AAGBfm0AAAAAXyqEGp8z2UMYnbcdNMGz28xUJnDB8Oeq&scisig=AAGBfm0AAAAAXyqEGgaa5R9Z4AdgrdpaT6JNX1dN76UR&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
78
-------------------------------------------------
2020-08-05 09:54:10
Got the results of the query
{'bib': {'abstract': 'We propose a novel framework, called Disjoint Mapping '
                     'Network (DIMNet), for cross-modal biometric matching, in '
                     'particular of voices and faces. Different from the '
                     'existing methods, DIMNet does not explicitly learn the '
                     'joint relationship between the modalities. Instead, '
                     'DIMNet learns a shared representation for different '
                     'modalities by mapping them individually to their common '
                     'covariates. These shared representations can then be '
                     'used to find the correspondences between the modalities. '
                     'We show empirically that DIMNet is able to',
         'author': ['Y Wen', 'MA Ismail', 'W Liu', 'B Raj', 'R Singh'],
         'cites': '16',
         'eprint': 'https://arxiv.org/pdf/1807.04836',
         'gsrank': '1',
         'title': 'Disjoint mapping network for cross-modal matching of voices '
                  'and faces',
         'url': 'https://arxiv.org/abs/1807.04836',
         'venue': 'arXiv preprint arXiv:1807.04836',
         'year': '2018'},
 'citations_link': '/scholar?cites=5133289555977246135&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDisjoint%2BMapping%2BNetwork%2Bfor%2BCross-modal%2BMatching%2Bof%2BVoices%2Band%2BFaces%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=txU_YacbPUcJ&ei=0YEqX6aJB532ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:txU_YacbPUcJ:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyri2RqotY-ZME4:AAGBfm0AAAAAXyqEOAeiP4--KE657e04xdydWlgAe7gq&scisig=AAGBfm0AAAAAXyqEOGZ4JC9lZNKNdItFrd86KdXezJJ2&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
16
-------------------------------------------------
2020-08-05 09:54:40
Got the results of the query
{'bib': {'abstract': 'A generally intelligent learner should generalize to '
                     'more complex tasks than it has previously encountered, '
                     'but the two common paradigms in machine learning--either '
                     'training a separate learner per task or training a '
                     'single learner for all tasks--both have difficulty with '
                     'such generalization because they do not leverage the '
                     'compositional structure of the task distribution. This '
                     'paper introduces the compositional problem graph as a '
                     'broadly applicable formalism to relate tasks of '
                     'different complexity in terms of problems with shared',
         'author': ['MB Chang', 'A Gupta', 'S Levine', 'TL Griffiths'],
         'cites': '24',
         'eprint': 'https://arxiv.org/pdf/1807.04640',
         'gsrank': '1',
         'title': 'Automatically composing representation transformations as a '
                  'means for generalization',
         'url': 'https://arxiv.org/abs/1807.04640',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=2301953604663446405&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAutomatically%2BComposing%2BRepresentation%2BTransformations%2Bas%2Ba%2BMeans%2Bfor%2BGeneralization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=hQtlb5su8h8J&ei=9oEqX4OlKIuayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:hQtlb5su8h8J:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyrisaoTzsoePTo:AAGBfm0AAAAAXyqEULcZRMo5JTreXwl6noM0eq5d7iPi&scisig=AAGBfm0AAAAAXyqEUNrdJTQAr6dwORRQxAzkiti5KE2Z&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
24
-------------------------------------------------
2020-08-05 09:55:04
Got the results of the query
{'bib': {'abstract': 'Humans learn to solve tasks of increasing complexity by '
                     'building on top of previously acquired knowledge. '
                     'Typically, there exists a natural progression in the '
                     'tasks that we learn-most do not require completely '
                     'independent solutions, but can be broken down into '
                     'simpler subtasks. We propose to represent a solver for '
                     'each task as a neural module that calls existing modules '
                     '(solvers for simpler tasks) in a functional program-like '
                     'manner. Lower modules are a black box to the calling '
                     'module, and communicate only via a query and an',
         'author': ['SW Kim', 'M Tapaswi', 'S Fidler'],
         'cites': '3',
         'eprint': 'https://arxiv.org/pdf/1806.02453',
         'gsrank': '1',
         'title': 'Visual reasoning by progressive module networks',
         'url': 'https://arxiv.org/abs/1806.02453',
         'venue': 'arXiv preprint arXiv:1806.02453',
         'year': '2018'},
 'citations_link': '/scholar?cites=12652585625953214305&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVisual%2BReasoning%2Bby%2BProgressive%2BModule%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=YZ8nLZcDl68J&ei=BIIqX_vNK4S0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:YZ8nLZcDl68J:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyrikrjuVNk7nx4:AAGBfm0AAAAAXyqEc6Xk3tkchx7z3xA3uEZOJ3e6Sl8L&scisig=AAGBfm0AAAAAXyqEc3oWd2nazZg0pKCMlnc7s1n5zmET&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
3
-------------------------------------------------
2020-08-05 09:55:39
Got the results of the query
{'bib': {'abstract': 'There is a previously identified equivalence between '
                     'wide fully connected neural networks (FCNs) and Gaussian '
                     'processes (GPs). This equivalence enables, for instance, '
                     'test set predictions that would have resulted from a '
                     'fully Bayesian, infinitely wide trained FCN to be '
                     'computed without ever instantiating the FCN, but by '
                     'instead evaluating the corresponding GP. In this work, '
                     'we derive an analogous equivalence for multi-layer '
                     'convolutional neural networks (CNNs) both with and '
                     'without pooling layers, and achieve state of the art '
                     'results on',
         'author': ['R Novak',
                    'L Xiao',
                    'J Lee',
                    'Y Bahri',
                    'G Yang',
                    'J Hron'],
         'cites': '69',
         'eprint': 'https://arxiv.org/pdf/1810.05148',
         'gsrank': '1',
         'title': 'Bayesian deep convolutional networks with many channels are '
                  'gaussian processes',
         'url': 'https://arxiv.org/abs/1810.05148',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=18154043069761963462&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBayesian%2BDeep%2BConvolutional%2BNetworks%2Bwith%2BMany%2BChannels%2Bare%2BGaussian%2BProcesses%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=xsVN2vwd8PsJ&ei=NYIqX_vPBcmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:xsVN2vwd8PsJ:scholar.google.com/&output=citation&scisdr=ChF83YoiCsMATyricKAa1O2eNTc:AAGBfm0AAAAAXyqEkb0QXu25LTeUWFGnYXo5JMKAam16&scisig=AAGBfm0AAAAAXyqEkYd5S60L1GR4jZ0reifWvgf4GT_f&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
69
-------------------------------------------------
2020-08-05 09:56:09
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://67.205.175.175:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://208.94.38.2:3128
Trying new proxy
Working proxy: http://51.38.134.168:3128
Trying new proxy
Working proxy: http://51.38.134.168:3128
Trying new proxy
Working proxy: http://125.212.218.43:8888
Got the results of the query
{'bib': {'abstract': 'Lack of performance when it comes to continual learning '
                     'over non-stationary distributions of data remains a '
                     'major challenge in scaling neural network learning to '
                     'more human realistic settings. In this work we propose a '
                     'new conceptualization of the continual learning problem '
                     'in terms of a temporally symmetric trade-off between '
                     'transfer and interference that can be optimized by '
                     'enforcing gradient alignment across examples. We then '
                     'propose a new algorithm, Meta-Experience Replay (MER), '
                     'that directly exploits this view by combining',
         'author': ['M Riemer', 'I Cases', 'R Ajemian', 'M Liu', 'I Rish'],
         'cites': '69',
         'eprint': 'https://arxiv.org/pdf/1810.11910',
         'gsrank': '1',
         'title': 'Learning to learn without forgetting by maximizing transfer '
                  'and minimizing interference',
         'url': 'https://arxiv.org/abs/1810.11910',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1577299111936747730&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BLearn%2Bwithout%2BForgetting%2Bby%2BMaximizing%2BTransfer%2Band%2BMinimizing%2BInterference%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=0miHdy-x4xUJ&ei=XIMqX46FJ4uayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:0miHdy-x4xUJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqFu4J9Z1ZID9ywGbXGnHE1laAZ2mwI&scisig=AAGBfm0AAAAAXyqFu2qLwmQOxGW0SRu8UKfDA4KzNI_B&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
69
-------------------------------------------------
2020-08-05 10:01:07
Got the results of the query
{'bib': {'abstract': 'A dynamical neural network consists of a set of '
                     'interconnected neurons that interact over time '
                     'continuously. It can exhibit computational properties in '
                     "the sense that the dynamical system's evolution and/or "
                     'limit points in the associated state space can '
                     'correspond to numerical solutions to certain '
                     'mathematical optimization or learning problems. Such a '
                     'computational system is particularly attractive in that '
                     'it can be mapped to a massively parallel computer '
                     'architecture for power and throughput efficiency, '
                     'especially if each neuron',
         'author': ['TH Lin', 'PTP Tang'],
         'cites': '0',
         'eprint': 'https://openreview.net/pdf?id=B1gstsCqt7',
         'gsrank': '1',
         'title': 'Sparse Dictionary Learning by Dynamical Neural Networks',
         'url': 'https://openreview.net/forum?id=B1gstsCqt7',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSparse%2BDictionary%2BLearning%2Bby%2BDynamical%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=9EcBOeqhMOsJ&ei=aoMqX_O0Ks2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:9EcBOeqhMOsJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqFzKqKxz5LQAtUotYfHvsZddvfY_04&scisig=AAGBfm0AAAAAXyqFzAW12owyJhT_LX1cUbwhZV8u6fMd&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
0
-------------------------------------------------
2020-08-05 10:01:24
Got the results of the query
{'bib': {'abstract': 'Spatiotemporal predictive learning, though long '
                     'considered to be a promising self-supervised feature '
                     'learning method, seldom shows its effectiveness beyond '
                     'future video prediction. The reason is that it is '
                     'difficult to learn good representations for both '
                     'short-term frame dependency and long-term high-level '
                     'relations. We present a new model, Eidetic 3D LSTM '
                     '(E3D-LSTM), that integrates 3D convolutions into RNNs. '
                     'The encapsulated 3D-Conv makes local perceptrons of RNNs '
                     'motion-aware and enables the memory cell to store better',
         'author': ['Y Wang', 'L Jiang', 'MH Yang', 'LJ Li', 'M Long'],
         'cites': '32',
         'eprint': 'https://openreview.net/pdf?id=B1lKS2AqtX',
         'gsrank': '1',
         'title': 'Eidetic 3d lstm: A model for video prediction and beyond',
         'url': 'https://openreview.net/forum?id=B1lKS2AqtX',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1521149270382251505&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEidetic%2B3D%2BLSTM:%2BA%2BModel%2Bfor%2BVideo%2BPrediction%2Band%2BBeyond%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=8Y16tzM1HBUJ&ei=eoMqX7e_O532ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:8Y16tzM1HBUJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqF2zplMRVT66p9MRZIoQuJKNUZ7sY2&scisig=AAGBfm0AAAAAXyqF22up6-6NTJ0jb2T5rtxzi1CZRo9P&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
32
-------------------------------------------------
2020-08-05 10:01:40
Got the results of the query
{'bib': {'abstract': 'Deep neural networks based on unfolding an iterative '
                     'algorithm, for example, LISTA (learned iterative '
                     'shrinkage thresholding algorithm), have been an '
                     'empirical success for sparse signal recovery. The '
                     'weights of these neural networks are currently '
                     'determined by data-driven “black-box” training. In this '
                     'work, we propose Analytic LISTA (ALISTA), where the '
                     'weight matrix in LISTA is computed as the solution to a '
                     'data-free optimization problem, leaving only the '
                     'stepsize and threshold parameters to data-driven '
                     'learning. This significantly',
         'author': ['J Liu', 'X Chen', 'Z Wang', 'W Yin'],
         'cites': '38',
         'eprint': 'https://openreview.net/pdf?id=B1lnzn0ctQ',
         'gsrank': '1',
         'title': 'ALISTA: Analytic weights are as good as learned weights in '
                  'LISTA',
         'url': 'https://openreview.net/forum?id=B1lnzn0ctQ',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=10887587722259826347&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DALISTA:%2BAnalytic%2BWeights%2BAre%2BAs%2BGood%2BAs%2BLearned%2BWeights%2Bin%2BLISTA%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=q4YzNll7GJcJ&ei=i4MqX7vdJsmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:q4YzNll7GJcJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqF6FtaRDmMDOXl0wfylAqtZumGKDLg&scisig=AAGBfm0AAAAAXyqF6AcCOCW0mx4jMGPcWceKBva2_314&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
38
-------------------------------------------------
2020-08-05 10:01:53
Got the results of the query
{'bib': {'abstract': 'Weight decay is one of the standard tricks in the neural '
                     'network toolbox, but the reasons for its regularization '
                     'effect are poorly understood, and recent results have '
                     'cast doubt on the traditional interpretation in terms of '
                     '$ L_2 $ regularization. Literal weight decay has been '
                     'shown to outperform $ L_2 $ regularization for '
                     'optimizers for which they differ. We empirically '
                     'investigate weight decay for three optimization '
                     'algorithms (SGD, Adam, and K-FAC) and a variety of '
                     'network architectures. We identify three distinct '
                     'mechanisms by which',
         'author': ['G Zhang', 'C Wang', 'B Xu', 'R Grosse'],
         'cites': '48',
         'eprint': 'https://arxiv.org/pdf/1810.12281',
         'gsrank': '1',
         'title': 'Three mechanisms of weight decay regularization',
         'url': 'https://arxiv.org/abs/1810.12281',
         'venue': 'arXiv preprint arXiv:1810.12281',
         'year': '2018'},
 'citations_link': '/scholar?cites=15919782238590351977&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThree%2BMechanisms%2Bof%2BWeight%2BDecay%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=aTrAcJlt7twJ&ei=loMqX5WdKZ32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:aTrAcJlt7twJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqF9-ZYD_ti-66XLtgakYLWpfoRG4w6&scisig=AAGBfm0AAAAAXyqF944mdPXWJs813I_GF64Kyg9CUZur&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
48
-------------------------------------------------
2020-08-05 10:02:08
Got the results of the query
{'bib': {'abstract': 'We view molecular optimization as a graph-to-graph '
                     'translation problem. The goal is to learn to map from '
                     'one molecular graph to another with better properties '
                     'based on an available corpus of paired molecules. Since '
                     'molecules can be optimized in different ways, there are '
                     'multiple viable translations for each input graph. A key '
                     'challenge is therefore to model diverse translation '
                     'outputs. Our primary contributions include a junction '
                     'tree encoder-decoder for learning diverse graph '
                     'translations along with a novel adversarial training',
         'author': ['W Jin', 'K Yang', 'R Barzilay', 'T Jaakkola'],
         'cites': '46',
         'eprint': 'https://arxiv.org/pdf/1812.01070',
         'gsrank': '1',
         'title': 'Learning multimodal graph-to-graph translation for '
                  'molecular optimization',
         'url': 'https://arxiv.org/abs/1812.01070',
         'venue': 'arXiv preprint arXiv:1812.01070',
         'year': '2018'},
 'citations_link': '/scholar?cites=5726195198168837773&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BMultimodal%2BGraph-to-Graph%2BTranslation%2Bfor%2BMolecule%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ja5mrTGId08J&ei=qYMqX-OYEsmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ja5mrTGId08J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqGB7ZKzB4wZRcnkZy83zpmdmh14lT1&scisig=AAGBfm0AAAAAXyqGB0Y5hqNGhJjIq6HDxksFPN516ZD4&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
46
-------------------------------------------------
2020-08-05 10:02:24
Got the results of the query
{'bib': {'abstract': 'In this paper, we focus on two challenges which offset '
                     'the promise of sparse signal representation, sensing, '
                     'and recovery. First, real-world signals can seldom be '
                     'described as perfectly sparse vectors in a known basis, '
                     'and traditionally used random measurement schemes are '
                     'seldom optimal for sensing them. Second, existing signal '
                     'recovery algorithms are usually not fast enough to make '
                     'them applicable to real-time problems. In this paper, we '
                     'address these two challenges by presenting a novel '
                     'framework based on deep learning. For',
         'author': ['A Mousavi', 'G Dasarathy', 'RG Baraniuk'],
         'cites': '11',
         'eprint': 'https://openreview.net/pdf?id=B1xVTjCqKQ',
         'gsrank': '1',
         'title': 'A data-driven and distributed approach to sparse signal '
                  'representation and recovery',
         'url': 'https://openreview.net/forum?id=B1xVTjCqKQ&noteId=BJlTjLcglV',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=9327114211220436628&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BData-Driven%2Band%2BDistributed%2BApproach%2Bto%2BSparse%2BSignal%2BRepresentation%2Band%2BRecovery%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=lLZKO_WQcIEJ&ei=tYMqX9yXDM2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:lLZKO_WQcIEJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqGFHseD4DR6Kdg3t0YpF4nVAkuQ-oF&scisig=AAGBfm0AAAAAXyqGFJHI7xYOQRs2isdXolLY_Iy1GJS9&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
11
-------------------------------------------------
2020-08-05 10:02:36
Got the results of the query
{'bib': {'abstract': 'Empirical risk minimization (ERM), with proper loss '
                     'function and regularization, is the common practice of '
                     'supervised classification. In this paper, we study '
                     'training arbitrary (from linear to deep) binary '
                     'classifier from only unlabeled (U) data by ERM. We prove '
                     'that it is impossible to estimate the risk of an '
                     'arbitrary binary classifier in an unbiased manner given '
                     'a single set of U data, but it becomes possible given '
                     'two sets of U data with different class priors. These '
                     'two facts answer a fundamental question---what the '
                     'minimal supervision is for',
         'author': ['N Lu', 'G Niu', 'AK Menon', 'M Sugiyama'],
         'cites': '19',
         'eprint': 'https://arxiv.org/pdf/1808.10585',
         'gsrank': '1',
         'title': 'On the minimal supervision for training any binary '
                  'classifier from only unlabeled data',
         'url': 'https://arxiv.org/abs/1808.10585',
         'venue': 'arXiv preprint arXiv:1808.10585',
         'year': '2018'},
 'citations_link': '/scholar?cites=12632779449090033610&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BMinimal%2BSupervision%2Bfor%2BTraining%2BAny%2BBinary%2BClassifier%2Bfrom%2BOnly%2BUnlabeled%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ysfdRPqlUK8J&ei=xoMqX8SyIp32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ysfdRPqlUK8J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqGKK4xfKACzWUC968OkekGxgYo5958&scisig=AAGBfm0AAAAAXyqGKGOH1i511FEAfNOHA7zB6qhAGkv1&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
19
-------------------------------------------------
2020-08-05 10:02:57
Got the results of the query
{'bib': {'abstract': 'We propose the Neural Logic Machine (NLM), a '
                     'neural-symbolic architecture for both inductive learning '
                     'and logic reasoning. NLMs exploit the power of both '
                     'neural networks---as function approximators, and logic '
                     'programming---as a symbolic processor for objects with',
         'author': ['H Dong', 'J Mao', 'T Lin', 'C Wang', 'L Li', 'D Zhou'],
         'cites': '32',
         'eprint': 'https://arxiv.org/pdf/1904.11694',
         'gsrank': '1',
         'title': 'Neural logic machines',
         'url': 'https://arxiv.org/abs/1904.11694',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=4525183211642569463&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BLogic%2BMachines%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=90KE0SeuzD4J&ei=1oMqX_ylJIbuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:90KE0SeuzD4J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqGNfiWJQeweMfI65sA98v9PYlRKGIP&scisig=AAGBfm0AAAAAXyqGNQYf7RDuXwl6mVPbAaEHwquNCg4m&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
32
-------------------------------------------------
2020-08-05 10:03:09
Got the results of the query
{'bib': {'abstract': 'Recurrent neural networks (RNNs) can model natural '
                     "language by sequentially'reading'input tokens and "
                     'outputting a distributed representation of each token. '
                     'Due to the sequential nature of RNNs, inference time is '
                     'linearly dependent on the input length, and all inputs '
                     'are read regardless of their importance. Efforts to '
                     "speed up this inference, known as' neural speed "
                     "reading', either ignore or skim over part of the input. "
                     'We present Structural-Jump-LSTM: the first neural speed '
                     'reading model to both skip and jump text during '
                     'inference. The',
         'author': ['C Hansen', 'C Hansen', 'S Alstrup', 'JG Simonsen'],
         'cites': '8',
         'eprint': 'https://arxiv.org/pdf/1904.00761',
         'gsrank': '1',
         'title': 'Neural speed reading with structural-jump-lstm',
         'url': 'https://arxiv.org/abs/1904.00761',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=10699754124824317847&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BSpeed%2BReading%2Bwith%2BStructural-Jump-LSTM%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=l48s0q8pfZQJ&ei=4YMqX9PlL8yXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:l48s0q8pfZQJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqGQMZ-XUaliN_73Y3wWb5ixE48JSfn&scisig=AAGBfm0AAAAAXyqGQI5yOLlzMqm5Jb9HGKSvsOlqkZ35&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
8
-------------------------------------------------
2020-08-05 10:03:21
Got the results of the query
{'bib': {'abstract': 'This paper addresses the problem of evaluating learning '
                     'systems in safety critical domains such as autonomous '
                     'driving, where failures can have catastrophic '
                     'consequences. We focus on two problems: searching for '
                     'scenarios when learned agents fail and assessing their '
                     'probability of failure. The standard method for agent '
                     'evaluation in reinforcement learning, Vanilla Monte '
                     'Carlo, can miss failures entirely, leading to the '
                     'deployment of unsafe agents. We demonstrate this is an '
                     'issue for current agents, where even matching the '
                     'compute used',
         'author': ['J Uesato', 'A Kumar', 'C Szepesvari', 'T Erez'],
         'cites': '16',
         'eprint': 'https://arxiv.org/pdf/1812.01647',
         'gsrank': '1',
         'title': 'Rigorous agent evaluation: An adversarial approach to '
                  'uncover catastrophic failures',
         'url': 'https://arxiv.org/abs/1812.01647',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=13064865884841591859&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRigorous%2BAgent%2BEvaluation:%2BAn%2BAdversarial%2BApproach%2Bto%2BUncover%2BCatastrophic%2BFailures%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=M0QaOE26T7UJ&ei=74MqX9PmNoKTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:M0QaOE26T7UJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqGTMDvtIKT88xyr7ct2uqmnbE6Y9yC&scisig=AAGBfm0AAAAAXyqGTIYeQDRRHNuiQaOP_eRrwic3M_85&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
16
-------------------------------------------------
2020-08-05 10:03:32
Got the results of the query
{'bib': {'abstract': 'Learning policies on data synthesized by models can in '
                     'principle quench the thirst of reinforcement learning '
                     'algorithms for large amounts of real experience, which '
                     'is often costly to acquire. However, simulating '
                     'plausible experience de novo is a hard problem for many '
                     'complex environments, often resulting in biases for '
                     'model-based policy evaluation and search. Instead of de '
                     'novo synthesis of data, here we assume logged, real '
                     'experience and model alternative outcomes of this '
                     'experience under counterfactual actions, actions that',
         'author': ['L Buesing', 'T Weber', 'Y Zwols', 'S Racaniere'],
         'cites': '36',
         'eprint': 'https://arxiv.org/pdf/1811.06272',
         'gsrank': '1',
         'title': 'Woulda, coulda, shoulda: Counterfactually-guided policy '
                  'search',
         'url': 'https://arxiv.org/abs/1811.06272',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=12981655284011501176&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWoulda,%2BCoulda,%2BShoulda:%2BCounterfactually-Guided%2BPolicy%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=eAYUeLMaKLQJ&ei=-oMqX47xEYS0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:eAYUeLMaKLQJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqGW-H-l5pOTk4fbb8qbbxYTAVj7FFq&scisig=AAGBfm0AAAAAXyqGW7N-9Hbo8wNMfaq9ct2GdI5icvv3&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
36
-------------------------------------------------
2020-08-05 10:03:49
Got the results of the query
{'bib': {'abstract': 'In this paper, we design and analyze a new zeroth-order '
                     '(ZO) stochastic optimization algorithm, ZO-signSGD, '
                     'which enjoys dual advantages of gradient-free operations '
                     'and signSGD. The latter requires only the sign '
                     'information of gradient estimates but is able to',
         'author': ['S Liu', 'PY Chen', 'X Chen', 'M Hong'],
         'cites': '17',
         'eprint': 'https://openreview.net/pdf?id=BJe-DsC5Fm',
         'gsrank': '1',
         'title': 'signSGD via zeroth-order oracle',
         'url': 'https://openreview.net/forum?id=BJe-DsC5Fm',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8111379770941762893&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DsignSGD%2Bvia%2BZeroth-Order%2BOracle%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Te1F_exokXAJ&ei=D4QqX_rkDIS0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Te1F_exokXAJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqGb7eIKQtKHDYYKpU3nFXhwDyM6Pd_&scisig=AAGBfm0AAAAAXyqGb0pPrMoSDq4EGR9TF6Fb1474jXHX&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
17
-------------------------------------------------
2020-08-05 10:04:08
Got the results of the query
{'bib': {'abstract': 'Due to the phenomenon of" posterior collapse," current '
                     'latent variable generative models pose a challenging '
                     'design choice that either weakens the capacity of the '
                     'decoder or requires augmenting the objective so it does '
                     'not only maximize the likelihood of the data. In',
         'author': ['A Razavi', 'A Oord', 'B Poole', 'O Vinyals'],
         'cites': '39',
         'eprint': 'https://arxiv.org/pdf/1901.03416',
         'gsrank': '1',
         'title': 'Preventing posterior collapse with delta-vaes',
         'url': 'https://arxiv.org/abs/1901.03416',
         'venue': 'arXiv preprint arXiv:1901.03416',
         'year': '2019'},
 'citations_link': '/scholar?cites=11040116821853696419&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPreventing%2BPosterior%2BCollapse%2Bwith%2Bdelta-VAEs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=o3FnpcFfNpkJ&ei=IYQqX42nOIuayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:o3FnpcFfNpkJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqGgVuZufqWlvDdTGMsLZFsPvACGxHM&scisig=AAGBfm0AAAAAXyqGgelaGZsksPgGwR_vZ5JyS4Ss_pgV&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
39
-------------------------------------------------
2020-08-05 10:04:26
Got the results of the query
{'bib': {'abstract': 'Model-based reinforcement learning (RL) is considered to '
                     'be a promising approach to reduce the sample complexity '
                     'that hinders model-free RL. However, the theoretical '
                     'understanding of such methods has been rather limited. '
                     'This paper introduces a novel algorithmic framework for '
                     'designing and analyzing model-based RL algorithms with '
                     'theoretical guarantees. We design a meta-algorithm with '
                     'a theoretical guarantee of monotone improvement to a '
                     'local maximum of the expected reward. The meta-algorithm',
         'author': ['Y Luo', 'H Xu', 'Y Li', 'Y Tian', 'T Darrell', 'T Ma'],
         'cites': '37',
         'eprint': 'https://arxiv.org/pdf/1807.03858',
         'gsrank': '1',
         'title': 'Algorithmic framework for model-based deep reinforcement '
                  'learning with theoretical guarantees',
         'url': 'https://arxiv.org/abs/1807.03858',
         'venue': 'arXiv preprint arXiv:1807.03858',
         'year': '2018'},
 'citations_link': '/scholar?cites=3175696566467828309&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAlgorithmic%2BFramework%2Bfor%2BModel-based%2BDeep%2BReinforcement%2BLearning%2Bwith%2BTheoretical%2BGuarantees%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=VRq-njNXEiwJ&ei=MYQqX7vSDc2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:VRq-njNXEiwJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqGjwYw99Ud7GgJfcTO2GXQ2JZFMKEQ&scisig=AAGBfm0AAAAAXyqGjz3KLCabAyVMzVv5pczfiWb3sK2O&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
37
-------------------------------------------------
2020-08-05 10:04:39
Got the results of the query
{'bib': {'abstract': 'A zoo of deep nets is available these days for almost '
                     'any given task, and it is increasingly unclear which net '
                     'to start with when addressing a new task, or which net '
                     'to use as an initialization for fine-tuning a new model. '
                     'To address this issue, in this paper, we develop '
                     "knowledge flow which moves' knowledge'from multiple deep "
                     'nets, referred to as teachers, to a new deep net model, '
                     'called the student. The structure of the teachers and '
                     'the student can differ arbitrarily and they can be '
                     'trained on entirely different tasks with different '
                     'output',
         'author': ['IJ Liu', 'J Peng', 'AG Schwing'],
         'cites': '6',
         'eprint': 'https://arxiv.org/pdf/1904.05878',
         'gsrank': '1',
         'title': 'Knowledge flow: Improve upon your teachers',
         'url': 'https://arxiv.org/abs/1904.05878',
         'venue': 'arXiv preprint arXiv:1904.05878',
         'year': '2019'},
 'citations_link': '/scholar?cites=12134336518601639221&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKnowledge%2BFlow:%2BImprove%2BUpon%2BYour%2BTeachers%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=NQVN8MXSZagJ&ei=PoQqX4eXMM2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:NQVN8MXSZagJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqGn5eHbjxQS54fcoxeaD29kB9LQtLg&scisig=AAGBfm0AAAAAXyqGn3s5La8qXRf_6AsLQOwLboP3_rOF&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 10:04:56
Got the results of the query
{'bib': {'abstract': 'The use of imitation learning to learn a single policy '
                     'for a complex task that has multiple modes or '
                     'hierarchical structure can be challenging. In fact, '
                     'previous work has shown that when the modes are known, '
                     'learning separate policies for each mode or sub-task can '
                     'greatly improve the performance of imitation learning. '
                     'In this work, we discover the interaction between '
                     'sub-tasks from their resulting state-action trajectory '
                     'sequences using a directed graphical model. We propose a '
                     'new algorithm based on the generative adversarial',
         'author': ['A Sharma', 'M Sharma', 'N Rhinehart'],
         'cites': '15',
         'eprint': 'https://arxiv.org/pdf/1810.01266',
         'gsrank': '1',
         'title': 'Directed-info GAIL: Learning hierarchical policies from '
                  'unsegmented demonstrations using directed information',
         'url': 'https://arxiv.org/abs/1810.01266',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=5359860145732970662&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDirected-Info%2BGAIL:%2BLearning%2BHierarchical%2BPolicies%2Bfrom%2BUnsegmented%2BDemonstrations%2Busing%2BDirected%2BInformation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=pugvoGcMYkoJ&ei=TYQqX52HHYuayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:pugvoGcMYkoJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqGrBB9NZEqbXatP0R4kcpgOaO4_okB&scisig=AAGBfm0AAAAAXyqGrID4jjpcInNMj8x-bYNWdvaxkN6g&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 10:05:09
Got the results of the query
{'bib': {'abstract': 'We develop a framework for understanding and improving '
                     'recurrent neural networks (RNNs) using max-affine spline '
                     'operators (MASOs). We prove that RNNs using piecewise '
                     'affine and convex nonlinearities can be written as a '
                     'simple piecewise affine spline operator. The resulting '
                     'representation provides several new perspectives for '
                     'analyzing RNNs, three of which we study in this paper. '
                     'First, we show that an RNN internally partitions the '
                     'input space during training and that it builds up the '
                     'partition through time. Second, we show that the',
         'author': ['Z Wang', 'R Balestriero', 'R Baraniuk'],
         'cites': '2',
         'eprint': 'https://openreview.net/pdf?id=BJej72AqF7',
         'gsrank': '1',
         'title': 'A max-affine spline perspective of recurrent neural '
                  'networks',
         'url': 'https://openreview.net/forum?id=BJej72AqF7',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8468021192773155169&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BMax-Affine%2BSpline%2BPerspective%2Bof%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=YTm5yWh0hHUJ&ei=X4QqX7OJGobuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:YTm5yWh0hHUJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqGxMUYO3oUvExVY5SSU6Dqozk3Rfo-&scisig=AAGBfm0AAAAAXyqGxJiBBoSos0xfgZRCX3OpcGwcQutV&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
2
-------------------------------------------------
2020-08-05 10:05:33
Got the results of the query
{'bib': {'abstract': 'Learning in environments with large state and action '
                     'spaces, and sparse rewards, can hinder a Reinforcement '
                     "Learning (RL) agent's learning through trial-and-error. "
                     'For instance, following natural language instructions on '
                     'the Web (such as booking a flight ticket) leads to',
         'author': ['I Gur', 'U Rueckert', 'A Faust', 'D Hakkani-Tur'],
         'cites': '5',
         'eprint': 'https://arxiv.org/pdf/1812.09195',
         'gsrank': '1',
         'title': 'Learning to navigate the web',
         'url': 'https://arxiv.org/abs/1812.09195',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=7234609565333107792&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BNavigate%2Bthe%2BWeb%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ULRSrhF_ZmQJ&ei=c4QqX7HvMJ32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ULRSrhF_ZmQJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqG0lrUBIEOUIN5apOLk0Ek1a5o7Lp4&scisig=AAGBfm0AAAAAXyqG0hDcL3jUqEVjdU5cztY41Uvxvk_D&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 10:05:46
Got the results of the query
{'bib': {'abstract': 'We explore the concept of co-design in the context of '
                     'neural network verification. Specifically, we aim to '
                     'train deep neural networks that not only are robust to '
                     'adversarial perturbations but also whose robustness can '
                     'be verified more easily. To this end, we identify two '
                     'properties of network models-weight sparsity and '
                     'so-called ReLU stability-that turn out to significantly '
                     'impact the complexity of the corresponding verification '
                     'task. We demonstrate that improving weight sparsity '
                     'alone already enables us to turn computationally '
                     'intractable',
         'author': ['KY Xiao', 'V Tjeng', 'NM Shafiullah', 'A Madry'],
         'cites': '64',
         'eprint': 'https://arxiv.org/pdf/1809.03008',
         'gsrank': '1',
         'title': 'Training for faster adversarial robustness verification via '
                  'inducing relu stability',
         'url': 'https://arxiv.org/abs/1809.03008',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11696009804149879522&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2Bfor%2BFaster%2BAdversarial%2BRobustness%2BVerification%2Bvia%2BInducing%2BReLU%2BStability%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=4vLW4fKSUKIJ&ei=f4QqX5S2L532ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:4vLW4fKSUKIJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqG3ScAjERCTUQYqaumvACLS-nTCnPd&scisig=AAGBfm0AAAAAXyqG3Xlvm_M1yXVtkKhIHrAbn5n5Wodx&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
64
-------------------------------------------------
2020-08-05 10:05:58
Got the results of the query
{'bib': {'abstract': 'Neural networks can learn to extract statistical '
                     'properties from data, but they seldom make use of '
                     'structured information from the label space to help '
                     'representation learning. Although some label structure '
                     'can implicitly be obtained when training on huge amounts '
                     'of data, in a few-shot learning context where little '
                     'data is available, making explicit use of the label '
                     'structure can inform the model to reshape the '
                     'representation space to reflect a global sense of class '
                     'dependencies. We propose a meta-learning framework, '
                     'Conditional class-Aware',
         'author': ['X Jiang', 'M Havaei', 'F Varno', 'G Chartrand'],
         'cites': '16',
         'eprint': 'https://openreview.net/pdf?id=BJfOXnActQ',
         'gsrank': '1',
         'title': 'Learning to learn with conditional class dependencies',
         'url': 'https://openreview.net/forum?id=BJfOXnActQ&noteId=Sye7irOggE',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=5550923310171285377&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BLearn%2Bwith%2BConditional%2BClass%2BDependencies%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=gecxLlfXCE0J&ei=joQqX--GJIKTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:gecxLlfXCE0J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqG7Y4vnPefaK2XJxgyb19xl24s-LAf&scisig=AAGBfm0AAAAAXyqG7SkKDr5x4UtypqDSahrfKOjCPRHB&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
16
-------------------------------------------------
2020-08-05 10:06:13
Got the results of the query
{'bib': {'abstract': 'We aim to build complex humanoid agents that integrate '
                     'perception, motor control, and memory. In this work, we '
                     'partly factor this problem into low-level motor control '
                     'from proprioception and high-level coordination of the '
                     'low-level skills informed by vision. We develop an '
                     'architecture capable of surprisingly flexible, '
                     'task-directed motor control of a relatively high-DoF '
                     'humanoid body by combining pre-training of low-level '
                     'motor controllers with a high-level, task-focused '
                     'controller that switches among low-level sub-policies. '
                     'The',
         'author': ['J Merel', 'A Ahuja', 'V Pham', 'S Tunyasuvunakool'],
         'cites': '36',
         'eprint': 'https://arxiv.org/pdf/1811.09656',
         'gsrank': '1',
         'title': 'Hierarchical visuomotor control of humanoids',
         'url': 'https://arxiv.org/abs/1811.09656',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=16014664282742845985&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BVisuomotor%2BControl%2Bof%2BHumanoids%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=IfIF2lCEP94J&ei=n4QqX8TaN82iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:IfIF2lCEP94J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqHAAldeSlBfAN75MsXYTw0mHipS3Pu&scisig=AAGBfm0AAAAAXyqHAAciJPpWJ7KbGprFs7-E9tu2VuH1&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
36
-------------------------------------------------
2020-08-05 10:06:33
Got the results of the query
{'bib': {'abstract': 'We address the problem of recovering an underlying '
                     'signal from lossy, inaccurate observations in an '
                     'unsupervised setting. Typically, we consider situations '
                     'where there is little to no background knowledge on the '
                     'structure of the underlying signal, no access to '
                     'signal-measurement pairs, nor even unpaired '
                     'signal-measurement data. The only available information '
                     'is provided by the observations and the measurement '
                     'process statistics. We cast the problem as finding '
                     'the\\textit {maximum a posteriori} estimate of the '
                     'signal given each',
         'author': ['A Pajot', 'E de Bezenac', 'P Gallinari'],
         'cites': '7',
         'eprint': 'https://openreview.net/pdf?id=BJg4Z3RqF7',
         'gsrank': '1',
         'title': 'Unsupervised adversarial image reconstruction',
         'url': 'https://openreview.net/forum?id=BJg4Z3RqF7',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=9310513454377536472&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BAdversarial%2BImage%2BReconstruction%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=2AurVKiWNYEJ&ei=s4QqX5nRBIS0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:2AurVKiWNYEJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqHERq1iNLLuaVkmy0jfDbUT54RXbnR&scisig=AAGBfm0AAAAAXyqHEZ_cCws6X2HaqghpwRqODF8o74ID&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
7
-------------------------------------------------
2020-08-05 10:06:50
Got the results of the query
{'bib': {'abstract': 'Eliciting labels from crowds is a potential way to '
                     'obtain large labeled data. Despite a variety of methods '
                     'developed for learning from crowds, a key challenge '
                     'remains unsolved:\\emph {learning from crowds without '
                     'knowing the information structure among the crowds a '
                     'priori, when some people of the crowds make highly '
                     'correlated mistakes and some of them label effortlessly '
                     '(eg randomly)}. We propose an information theoretic '
                     'approach, Max-MIG, for joint learning from crowds, with '
                     'a common assumption: the crowdsourced labels and the '
                     'data are',
         'author': ['P Cao', 'Y Xu', 'Y Kong', 'Y Wang'],
         'cites': '5',
         'eprint': 'https://arxiv.org/pdf/1905.13436',
         'gsrank': '1',
         'title': 'Max-MIG: An information theoretic approach for joint '
                  'learning from crowds',
         'url': 'https://arxiv.org/abs/1905.13436',
         'venue': 'arXiv preprint arXiv:1905.13436',
         'year': '2019'},
 'citations_link': '/scholar?cites=14993809510724823282&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMax-MIG:%2Ban%2BInformation%2BTheoretic%2BApproach%2Bfor%2BJoint%2BLearning%2Bfrom%2BCrowds%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=8migNE-2FNAJ&ei=wYQqX4uoDYKTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:8migNE-2FNAJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqHH9qerN6v8MYqc_tGg0xohabJtXck&scisig=AAGBfm0AAAAAXyqHH8okljQsYZDcYB0ywsIfqsjgcx1i&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 10:07:04
Got the results of the query
{'bib': {'abstract': 'Many machine learning problems involve iteratively and '
                     'alternately optimizing different task objectives with '
                     'respect to different sets of parameters. Appropriately '
                     'scheduling the optimization of a task objective or a set '
                     'of parameters is usually crucial to the quality of '
                     'convergence. In this paper, we present AutoLoss, a '
                     'meta-learning framework that automatically learns and '
                     'determines the optimization schedule. AutoLoss provides '
                     'a generic way to represent and learn the discrete '
                     'optimization schedule from metadata, allows for a',
         'author': ['H Xu', 'H Zhang', 'Z Hu', 'X Liang', 'R Salakhutdinov'],
         'cites': '8',
         'eprint': 'https://arxiv.org/pdf/1810.02442',
         'gsrank': '1',
         'title': 'Autoloss: Learning discrete schedules for alternate '
                  'optimization',
         'url': 'https://arxiv.org/abs/1810.02442',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15351780571596212720&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAutoLoss:%2BLearning%2BDiscrete%2BSchedule%2Bfor%2BAlternate%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=8Em8txd7DNUJ&ei=0IQqX-6BBJ32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:8Em8txd7DNUJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqHMaZ8z4XLokQSEYgCWOZ-R5RR8L6I&scisig=AAGBfm0AAAAAXyqHMQcwL9XwJt0BZQqGerNsXAzVKtnW&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
8
-------------------------------------------------
2020-08-05 10:07:21
Got the results of the query
{'bib': {'abstract': 'Methods and results In the west of Scotland a database '
                     'records all courses and attendances at accredited '
                     'meetings. 2 General practitioners can also join an '
                     'educational scheme for one annual payment which entitles '
                     'them to attend as many courses as they wish for no extra',
         'author': ['TS Murray', 'LM Campbell'],
         'cites': '48',
         'eprint': 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2127264/pdf/9270459.pdf',
         'gsrank': '1',
         'title': 'Finance, not learning needs, makes general practitioners '
                  'attend courses: a database survey',
         'url': 'https://www.bmj.com/content/315/7104/353.extract',
         'venue': 'BMJ',
         'year': '1997'},
 'citations_link': '/scholar?cites=10494084225279279453&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bwhat%2Band%2Bwhere%2Bto%2Battend%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=XUHNugB6opEJ&ei=4oQqX5a0OIS0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:XUHNugB6opEJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqHQ50y7A_PSMaSQf9qAFewqSiJpLXp&scisig=AAGBfm0AAAAAXyqHQ2w3plfqvpBRiQ_xLvzFzRLEhW41&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
48
-------------------------------------------------
2020-08-05 10:07:39
Got the results of the query
{'bib': {'abstract': "Robust estimation under Huber's $\\epsilon "
                     '$-contamination model has become an important topic in '
                     'statistics and theoretical computer science. '
                     "Rate-optimal procedures such as Tukey's median and other "
                     'estimators based on statistical depth functions are '
                     'impractical because of their computational '
                     'intractability. In this paper, we establish an '
                     'intriguing connection between f-GANs and various depth '
                     'functions through the lens of f-Learning. Similar to the '
                     'derivation of f-GAN, we show that these depth functions '
                     'that lead to rate',
         'author': ['GAO Chao', 'YAO Yuan', 'ZHU Weizhi'],
         'cites': '3',
         'eprint': 'https://openreview.net/pdf?id=BJgRDjR9tQ',
         'gsrank': '1',
         'title': 'Robust estimation via generative adversarial networks',
         'url': 'https://openreview.net/forum?id=BJgRDjR9tQ',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15221755775674009849&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRobust%2Bestimation%2Bvia%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=-cjMyjqKPtMJ&ei=9IQqX47YNcyXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:-cjMyjqKPtMJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqHVC3fESJ5cYFFL_76yZ7HVFLnwJ8u&scisig=AAGBfm0AAAAAXyqHVNkSCKhVTV-Xp6ScgBz9MYsMCAAF&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
3
-------------------------------------------------
2020-08-05 10:07:57
Got the results of the query
{'bib': {'abstract': 'The advent of big data brings with it data with more and '
                     'more dimensions and thus a growing need to be able to '
                     'efficiently select which features to use for a variety '
                     'of problems. While global feature selection has been a '
                     'well-studied problem for quite some time, only recently '
                     'has the paradigm of instance-wise feature selection been '
                     'developed. In this paper, we propose a new instance-wise '
                     'feature selection method, which we term INVASE. INVASE '
                     'consists of 3 neural networks, a selector network, a '
                     'predictor network and a baseline',
         'author': ['J Yoon', 'J Jordon', 'M van der Schaar'],
         'cites': '11',
         'eprint': 'https://openreview.net/pdf?id=BJg_roAcK7',
         'gsrank': '1',
         'title': 'INVASE: Instance-wise variable selection using neural '
                  'networks',
         'url': 'https://openreview.net/forum?id=BJg_roAcK7',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=3318146487055213130&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DINVASE:%2BInstance-wise%2BVariable%2BSelection%2Busing%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=SuKiUaZsDC4J&ei=BIUqX6qpEJmG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:SuKiUaZsDC4J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqHY4ytbXShw92RUj8GCXsZQAg7E8ag&scisig=AAGBfm0AAAAAXyqHYzU7TJJnUq69xl9YhDoG2IwkE8vf&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
11
-------------------------------------------------
2020-08-05 10:08:11
Got the results of the query
{'bib': {'abstract': 'Gradient-based meta-learning techniques are both widely '
                     'applicable and proficient at solving challenging '
                     'few-shot learning and fast adaptation problems. However, '
                     'they have practical difficulties when operating on '
                     'high-dimensional parameter spaces in extreme low-data '
                     'regimes. We show that it is possible to bypass these '
                     'limitations by learning a data-dependent latent '
                     'generative representation of model parameters, and '
                     'performing gradient-based meta-learning in this '
                     'low-dimensional latent space. The resulting approach, '
                     'latent',
         'author': ['AA Rusu', 'D Rao', 'J Sygnowski', 'O Vinyals'],
         'cites': '240',
         'eprint': 'https://arxiv.org/pdf/1807.05960',
         'gsrank': '1',
         'title': 'Meta-learning with latent embedding optimization',
         'url': 'https://arxiv.org/abs/1807.05960',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11552536411545683614&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-Learning%2Bwith%2BLatent%2BEmbedding%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=nkbyiKjaUqAJ&ei=HYUqX57dHp32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:nkbyiKjaUqAJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqHgtrWnilvAeiTrqiBnKo_9FNb5Zxi&scisig=AAGBfm0AAAAAXyqHghvet6seZCfrncNdR6KUmuGpoNqu&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
240
-------------------------------------------------
2020-08-05 10:08:42
Got the results of the query
{'bib': {'abstract': 'Modern neural networks are highly overparameterized, '
                     'with capacity to substantially overfit to training data. '
                     'Nevertheless, these networks often generalize well in '
                     'practice. It has also been observed that trained '
                     'networks can often be" compressed" to much smaller '
                     'representations. The purpose of this paper is to connect '
                     'these two empirical observations. Our main technical '
                     'result is a generalization bound for compressed networks '
                     'based on the compressed size. Combined with '
                     'off-the-shelf compression algorithms, the bound leads to',
         'author': ['W Zhou', 'V Veitch', 'M Austern', 'RP Adams'],
         'cites': '44',
         'eprint': 'https://arxiv.org/pdf/1804.05862',
         'gsrank': '1',
         'title': 'Non-vacuous generalization bounds at the imagenet scale: a '
                  'PAC-Bayesian compression approach',
         'url': 'https://arxiv.org/abs/1804.05862',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=12180551458196751211&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNon-vacuous%2BGeneralization%2BBounds%2Bat%2Bthe%2BImageNet%2BScale:%2Ba%2BPAC-Bayesian%2BCompression%2BApproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=awMvEgQDCqkJ&ei=MYUqX8rMG4uayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:awMvEgQDCqkJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqHke70co5Am-kk8JX6fwVMgq2_fnkV&scisig=AAGBfm0AAAAAXyqHkaHeQGmv9uswBB7ippnFrREwBMyx&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
44
-------------------------------------------------
2020-08-05 10:08:58
Got the results of the query
{'bib': {'abstract': 'We introduce the problem of learning distributed '
                     'representations of edits. By combining a" neural editor" '
                     'with an" edit encoder", our models learn to represent '
                     'the salient information of an edit and can be used to '
                     'apply edits to new inputs. We experiment on natural '
                     'language',
         'author': ['P Yin', 'G Neubig', 'M Allamanis', 'M Brockschmidt'],
         'cites': '25',
         'eprint': 'https://arxiv.org/pdf/1810.13337',
         'gsrank': '1',
         'title': 'Learning to represent edits',
         'url': 'https://arxiv.org/abs/1810.13337',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15643648406405720624&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BRepresent%2BEdits%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=MCrrjldnGdkJ&ei=QIUqX-DBJ8mdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:MCrrjldnGdkJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqHov5AysIC4-glPUwxv5XTbxckDB2I&scisig=AAGBfm0AAAAAXyqHorw1utJtsJQh5KbI-EWr0yxDmkPg&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
25
-------------------------------------------------
2020-08-05 10:09:14
Got the results of the query
{'bib': {'abstract': 'We focus on the problem of learning a single motor '
                     'module that can flexibly express a range of behaviors '
                     'for the control of high-dimensional physically simulated '
                     'humanoids. To do this, we propose a motor architecture '
                     'that has the general structure of an inverse model with '
                     'a latent-variable bottleneck. We show that it is '
                     'possible to train this model entirely offline to '
                     'compress thousands of expert policies and learn a motor '
                     'primitive embedding space. The trained neural '
                     'probabilistic motor primitive system can perform '
                     'one-shot imitation of whole',
         'author': ['J Merel', 'L Hasenclever', 'A Galashov', 'A Ahuja'],
         'cites': '27',
         'eprint': 'https://arxiv.org/pdf/1811.11711',
         'gsrank': '1',
         'title': 'Neural probabilistic motor primitives for humanoid control',
         'url': 'https://arxiv.org/abs/1811.11711',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11172180957185308522&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BProbabilistic%2BMotor%2BPrimitives%2Bfor%2BHumanoid%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=avdpR2OPC5sJ&ei=U4UqX4JOi5rIBPa5gfAK',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:avdpR2OPC5sJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqHs6AMNiYrphC-c1jtxNy7fQoMPg0N&scisig=AAGBfm0AAAAAXyqHs0pr3oWadpqNqCGKJl2aj5qPMTWm&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
27
-------------------------------------------------
2020-08-05 10:09:33
Got the results of the query
{'bib': {'abstract': 'Human annotation for syntactic parsing is expensive, and '
                     'large resources are available only for a fraction of '
                     'languages. A question we ask is whether one can leverage '
                     'abundant unlabeled texts to improve syntactic parsers, '
                     'beyond just using the texts to obtain more generalisable '
                     'lexical features (ie beyond word embeddings). To this '
                     'end, we propose a novel latent-variable generative model '
                     'for semi-supervised syntactic dependency parsing. As '
                     'exact inference is intractable, we introduce a '
                     'differentiable relaxation to obtain',
         'author': ['C Corro', 'I Titov'],
         'cites': '20',
         'eprint': 'https://arxiv.org/pdf/1807.09875',
         'gsrank': '1',
         'title': 'Differentiable perturb-and-parse: Semi-supervised parsing '
                  'with a structured variational autoencoder',
         'url': 'https://arxiv.org/abs/1807.09875',
         'venue': 'arXiv preprint arXiv:1807.09875',
         'year': '2018'},
 'citations_link': '/scholar?cites=14755468217840863060&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDifferentiable%2BPerturb-and-Parse:%2BSemi-Supervised%2BParsing%2Bwith%2Ba%2BStructured%2BVariational%2BAutoencoder%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=VIf4kir0xcwJ&ei=Y4UqX-jxGMmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:VIf4kir0xcwJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqHx5tjOvDeCeWkOemCWigikCifxYZd&scisig=AAGBfm0AAAAAXyqHxxcjEFE46cZR70CmuUQymip_8x0C&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
20
-------------------------------------------------
2020-08-05 10:09:52
Got the results of the query
{'bib': {'abstract': 'We consider a simple and overarching representation for '
                     'permutation-invariant functions of sequences (or '
                     'multiset functions). Our approach, which we call Janossy '
                     'pooling, expresses a permutation-invariant function as '
                     'the average of a permutation-sensitive function applied '
                     'to all reorderings of the input sequence. This allows us '
                     'to leverage the rich and mature literature on '
                     'permutation-sensitive functions to construct novel and '
                     'flexible permutation-invariant functions. If carried out '
                     'naively, Janossy pooling can be computationally',
         'author': ['RL Murphy', 'B Srinivasan', 'V Rao', 'B Ribeiro'],
         'cites': '43',
         'eprint': 'https://arxiv.org/pdf/1811.01900',
         'gsrank': '1',
         'title': 'Janossy pooling: Learning deep permutation-invariant '
                  'functions for variable-size inputs',
         'url': 'https://arxiv.org/abs/1811.01900',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8977286222434486292&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DJanossy%2BPooling:%2BLearning%2BDeep%2BPermutation-Invariant%2BFunctions%2Bfor%2BVariable-Size%2BInputs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=FEg3b0G6lXwJ&ei=d4UqX6NCmYbqtA-x_KzwBw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:FEg3b0G6lXwJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqH13cfD67rjGehHkVXxN_fsbgbutg-&scisig=AAGBfm0AAAAAXyqH1zVi788U9V28HtvtFdYQ74GE0LOm&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
43
-------------------------------------------------
2020-08-05 10:10:07
Got the results of the query
{'bib': {'abstract': 'Inspired by the phenomenon of catastrophic forgetting, '
                     'we investigate the learning dynamics of neural networks '
                     'as they train on single classification tasks. Our goal '
                     'is to understand whether a related phenomenon occurs '
                     'when data does not undergo a clear distributional shift. '
                     "We define aforgetting event'to have occurred when an "
                     'individual training example transitions from being '
                     'classified correctly to incorrectly over the course of '
                     'learning. Across several benchmark data sets, we find '
                     'that:(i) certain examples are forgotten with high',
         'author': ['M Toneva', 'A Sordoni', 'RT Combes', 'A Trischler'],
         'cites': '35',
         'eprint': 'https://arxiv.org/pdf/1812.05159',
         'gsrank': '1',
         'title': 'An empirical study of example forgetting during deep neural '
                  'network learning',
         'url': 'https://arxiv.org/abs/1812.05159',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=14912040563601232331&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2BEmpirical%2BStudy%2Bof%2BExample%2BForgetting%2Bduring%2BDeep%2BNeural%2BNetwork%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=y9GqneI18s4J&ei=hoUqX9nfApmG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:y9GqneI18s4J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqH5QtqZAI02dfPXQEyighBtA5beq0f&scisig=AAGBfm0AAAAAXyqH5fRjK5vze2RkfKEfYRjM07BSs9fG&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
35
-------------------------------------------------
2020-08-05 10:10:22
Got the results of the query
{'bib': {'abstract': 'Recurrent neural networks (RNNs) can learn continuous '
                     'vector representations of symbolic structures such as '
                     'sequences and sentences; these representations often '
                     'exhibit linear regularities (analogies). Such '
                     'regularities motivate our hypothesis that RNNs that show '
                     'such regularities implicitly compile symbolic structures '
                     'into tensor product representations (TPRs; Smolensky, '
                     '1990), which additively combine tensor products of '
                     'vectors representing roles (eg, sequence positions) and '
                     'vectors representing fillers (eg, particular words). To '
                     'test this',
         'author': ['RT McCoy', 'T Linzen', 'E Dunbar'],
         'cites': '16',
         'eprint': 'https://arxiv.org/pdf/1812.08718',
         'gsrank': '1',
         'title': 'Rnns implicitly implement tensor product representations',
         'url': 'https://arxiv.org/abs/1812.08718',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8578120166770522666&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRNNs%2Bimplicitly%2Bimplement%2Btensor-product%2Brepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Krr4xNeaC3cJ&ei=lYUqX7OnN5mG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Krr4xNeaC3cJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqH9rTenhwWFJNmvAHlWaFtyuF60Mrc&scisig=AAGBfm0AAAAAXyqH9k-DVOyAZz0mxEl66s8hXFdP3Yw7&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
16
-------------------------------------------------
2020-08-05 10:10:39
Got the results of the query
{'bib': {'abstract': 'Recent efforts to combine Representation Learning with '
                     'Formal Methods, commonly known as the Neuro-Symbolic '
                     'Methods, have given rise to a new trend of applying rich '
                     'neural architectures to solve classical combinatorial '
                     'optimization problems. In this paper, we propose a '
                     'neural framework that can learn to solve the Circuit '
                     'Satisfiability problem. Our framework is built upon two '
                     'fundamental contributions: a rich embedding architecture '
                     'that encodes the problem structure and an end-to-end '
                     'differentiable training procedure that',
         'author': ['S Amizadeh', 'S Matusevych'],
         'cites': '19',
         'eprint': 'https://openreview.net/pdf?id=BJxgz2R9t7',
         'gsrank': '1',
         'title': 'Learning to solve circuit-SAT: An unsupervised '
                  'differentiable approach',
         'url': 'https://openreview.net/forum?id=BJxgz2R9t7',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8913283008212437757&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BSolve%2BCircuit-SAT:%2BAn%2BUnsupervised%2BDifferentiable%2BApproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=_QoH36tXsnsJ&ei=poUqX9eOLMyXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:_QoH36tXsnsJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqIBYW9ZYXhFPFa5y_4fnywnREWQXF0&scisig=AAGBfm0AAAAAXyqIBfejqIiUtEhwBI2mVTnam2xoXCIr&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
19
-------------------------------------------------
2020-08-05 10:10:54
Got the results of the query
{'bib': {'abstract': 'Making deep convolutional neural networks more accurate '
                     'typically comes at the cost of increased computational '
                     'and memory resources. In this paper, we reduce this cost '
                     'by exploiting the fact that the importance of features '
                     'computed by convolutional layers is highly '
                     'input-dependent, and propose feature boosting and '
                     'suppression (FBS), a new method to predictively amplify '
                     'salient convolutional channels and skip unimportant ones '
                     'at run-time. FBS introduces small auxiliary connections '
                     'to existing convolutional layers. In contrast to',
         'author': ['X Gao', 'Y Zhao', 'Ł Dudziak', 'R Mullins', 'C Xu'],
         'cites': '46',
         'eprint': 'https://arxiv.org/pdf/1810.05331',
         'gsrank': '1',
         'title': 'Dynamic channel pruning: Feature boosting and suppression',
         'url': 'https://arxiv.org/abs/1810.05331',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1895104173020407133&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamic%2BChannel%2BPruning:%2BFeature%2BBoosting%2Band%2BSuppression%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=XeU9ZTTDTBoJ&ei=tIUqX_qVHIS0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:XeU9ZTTDTBoJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqIF2ZLlDrKrfQOtzKZ8Dak6fZsrebO&scisig=AAGBfm0AAAAAXyqIF4kwUX9aUjq8RA3GxT8UCwRVepOA&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
46
-------------------------------------------------
2020-08-05 10:11:12
Got the results of the query
{'bib': {'abstract': 'Training neural networks on large datasets can be '
                     'accelerated by distributing the workload over a network '
                     'of machines. As datasets grow ever larger, networks of '
                     'hundreds or thousands of machines become economically '
                     'viable. The time cost of communicating gradients limits '
                     'the effectiveness of using such large machine counts, as '
                     'may the increased chance of network faults. We explore a '
                     'particularly simple algorithm for robust, '
                     'communication-efficient learning---signSGD. Workers '
                     'transmit only the sign of their gradient',
         'author': ['J Bernstein', 'J Zhao', 'K Azizzadenesheli'],
         'cites': '24',
         'eprint': 'https://arxiv.org/pdf/1810.05291',
         'gsrank': '1',
         'title': 'signSGD with majority vote is communication efficient and '
                  'fault tolerant',
         'url': 'https://arxiv.org/abs/1810.05291',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15973736581723285506&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DsignSGD%2Bwith%2BMajority%2BVote%2Bis%2BCommunication%2BEfficient%2Band%2BFault%2BTolerant%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=AmhH4ckcrt0J&ei=zYUqX5y3CYS0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:AmhH4ckcrt0J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqIK0g4wxIZS-MK33pdLcH3EPZsvG8i&scisig=AAGBfm0AAAAAXyqIKwiDTF5VEION2kjpHNXUxVtTxALG&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
24
-------------------------------------------------
2020-08-05 10:11:31
Got the results of the query
{'bib': {'abstract': 'We introduce an approach to model surface properties '
                     'governing bounces in everyday scenes. Our model learns '
                     'end-to-end, starting from sensor inputs, to predict '
                     'post-bounce trajectories and infer two underlying '
                     'physical properties that govern bouncing-restitution and '
                     'effective collision normals. Our model, Bounce and '
                     'Learn, comprises two modules--a Physics Inference Module '
                     '(PIM) and a Visual Inference Module (VIM). VIM learns to '
                     'infer physical parameters for locations in a scene given '
                     'a single still image, while PIM learns to',
         'author': ['S Purushwalkam', 'A Gupta', 'DM Kaufman'],
         'cites': '9',
         'eprint': 'https://arxiv.org/pdf/1904.06827',
         'gsrank': '1',
         'title': 'Bounce and learn: Modeling scene dynamics with real-world '
                  'bounces',
         'url': 'https://arxiv.org/abs/1904.06827',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=3030062067056637219&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBounce%2Band%2BLearn:%2BModeling%2BScene%2BDynamics%2Bwith%2BReal-World%2BBounces%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=I_VLYmXxDCoJ&ei=2YUqX5L4MIuayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:I_VLYmXxDCoJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqIOCE9ebRvA7b9CW-8xyYmWHQXhvMp&scisig=AAGBfm0AAAAAXyqIOA8KKyoMqgXGIozOXK0I3vyFqHSD&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 10:11:44
Got the results of the query
{'bib': {'abstract': 'We introduce a novel method that enables '
                     'parameter-efficient transfer and multi-task learning '
                     'with deep neural networks. The basic approach is to '
                     'learn a model patch-a small set of parameters-that will '
                     'specialize to each task, instead of fine-tuning the last '
                     'layer or the entire network. For instance, we show that '
                     'learning a set of scales and biases is sufficient to '
                     'convert a pretrained network to perform well on '
                     'qualitatively different problems (eg converting a Single '
                     'Shot MultiBox Detection (SSD) model into a 1000-class '
                     'image',
         'author': ['PK Mudrakarta', 'M Sandler', 'A Zhmoginov'],
         'cites': '12',
         'eprint': 'https://arxiv.org/pdf/1810.10703',
         'gsrank': '1',
         'title': 'K for the Price of 1: Parameter-efficient Multi-task and '
                  'Transfer Learning',
         'url': 'https://arxiv.org/abs/1810.10703',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=6019481069112213609&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DK%2Bfor%2Bthe%2BPrice%2Bof%2B1:%2BParameter-efficient%2BMulti-task%2Band%2BTransfer%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=aUCfyCN-iVMJ&ei=6YUqX6eaJIbuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:aUCfyCN-iVMJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqITo0ttScx7js8rQf53AhlJle_cm1k&scisig=AAGBfm0AAAAAXyqITvucSJ1Ds4UNWwr02I7TqwLNldR8&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 10:12:07
Got the results of the query
{'bib': {'abstract': 'The problem of $\\textit {visual metamerism} $ is '
                     'defined as finding a family of perceptually '
                     'indistinguishable, yet physically different images. In '
                     'this paper, we propose our NeuroFovea metamer model, a '
                     'foveated generative model that is based on a mixture of '
                     'peripheral representations and style transfer '
                     'forward-pass algorithms. Our gradient-descent free model '
                     'is parametrized by a foveated VGG19 encoder-decoder '
                     'which allows us to encode images in high dimensional '
                     'space and interpolate between the content and texture '
                     'information with',
         'author': ['A Deza', 'A Jonnalagadda', 'M Eckstein'],
         'cites': '5',
         'eprint': 'https://arxiv.org/pdf/1705.10041',
         'gsrank': '1',
         'title': 'Towards metamerism via foveated style transfer',
         'url': 'https://arxiv.org/abs/1705.10041',
         'venue': 'arXiv preprint arXiv:1705.10041',
         'year': '2017'},
 'citations_link': '/scholar?cites=17935865817929282522&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BMetamerism%2Bvia%2BFoveated%2BStyle%2BTransfer%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=2vO-4O7-6PgJ&ei=A4YqX7bXHJ32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:2vO-4O7-6PgJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqIYhcLLg_LbNBPnz_11cNUU4ZU_NHi&scisig=AAGBfm0AAAAAXyqIYtoyUKxouArenoPq4Qc5TygCrn7X&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 10:12:27
Got the results of the query
{'bib': {'abstract': 'Measuring divergence between two distributions is '
                     'essential in machine learning and statistics and has '
                     'various applications including binary classification, '
                     'change point detection, and two-sample test. '
                     'Furthermore, in the era of big data, designing '
                     'divergence measure that is interpretable and can handle '
                     'high-dimensional and complex data becomes extremely '
                     'important. In the paper, we propose a post selection '
                     'inference (PSI) framework for divergence measure, which '
                     'can select a set of statistically significant features '
                     'that',
         'author': ['M Yamada', 'D Wu', 'YHH Tsai', 'I Takeuchi'],
         'cites': '5',
         'eprint': 'https://arxiv.org/pdf/1802.06226',
         'gsrank': '1',
         'title': 'Post selection inference with incomplete maximum mean '
                  'discrepancy estimator',
         'url': 'https://arxiv.org/abs/1802.06226',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=3227686816764844914&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPost%2BSelection%2BInference%2Bwith%2BIncomplete%2BMaximum%2BMean%2BDiscrepancy%2BEstimator%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ctt6Dw8MyywJ&ei=FIYqX6agK82iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ctt6Dw8MyywJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqId8Ci8-mcR1-ozK6W1Ua2jBGRfuO6&scisig=AAGBfm0AAAAAXyqId1E53fDfgAX5akssXC4FF9vGOGKK&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 10:12:51
Got the results of the query
{'bib': {'abstract': 'We study the emergence of cooperative behaviors in '
                     'reinforcement learning agents by introducing a '
                     'challenging competitive multi-agent soccer environment '
                     'with continuous simulated physics. We demonstrate that '
                     'decentralized, population-based training with co',
         'author': ['S Liu', 'G Lever', 'J Merel', 'S Tunyasuvunakool'],
         'cites': '40',
         'eprint': 'https://arxiv.org/pdf/1902.07151',
         'gsrank': '1',
         'title': 'Emergent coordination through competition',
         'url': 'https://arxiv.org/abs/1902.07151',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=9035094704575368173&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergent%2BCoordination%2BThrough%2BCompetition%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=7Us6isMaY30J&ei=MYYqX-jKJpmG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:7Us6isMaY30J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqIkS2_TN6QMLiFxksXKCo6U3wRCRTW&scisig=AAGBfm0AAAAAXyqIkdjNgsUwrEl9R37jIKxISn8Ydk3s&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
40
-------------------------------------------------
2020-08-05 10:13:15
Got the results of the query
{'bib': {'abstract': 'We study the problem of generating adversarial examples '
                     'in a black-box setting in which only loss-oracle access '
                     'to a model is available. We introduce a framework that '
                     'conceptually unifies much of the existing work on '
                     'black-box attacks, and we demonstrate that the current '
                     'state-of-the-art methods are optimal in a natural sense. '
                     'Despite this optimality, we show how to improve '
                     'black-box attacks by bringing a new element into the '
                     'problem: gradient priors. We give a bandit '
                     'optimization-based algorithm that allows us to '
                     'seamlessly integrate any',
         'author': ['A Ilyas', 'L Engstrom', 'A Madry'],
         'cites': '86',
         'eprint': 'https://arxiv.org/pdf/1807.07978.pdf).',
         'gsrank': '1',
         'title': 'Prior convictions: Black-box adversarial attacks with '
                  'bandits and priors',
         'url': 'https://arxiv.org/abs/1807.07978',
         'venue': 'arXiv preprint arXiv:1807.07978',
         'year': '2018'},
 'citations_link': '/scholar?cites=4612085557896805496&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPrior%2BConvictions:%2BBlack-box%2BAdversarial%2BAttacks%2Bwith%2BBandits%2Band%2BPriors%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ePwlCGFrAUAJ&ei=RIYqX8jKC4KTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ePwlCGFrAUAJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqIpdXPtK0rQKFt59dXP4aucOUHEWp6&scisig=AAGBfm0AAAAAXyqIpWVKty2M6XPNShrnujWvL4vUtwuE&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
86
-------------------------------------------------
2020-08-05 10:13:33
Got the results of the query
{'bib': {'abstract': 'The goal of imitation learning (IL) is to enable a '
                     'learner to imitate expert behavior given expert '
                     'demonstrations. Recently, generative adversarial '
                     'imitation learning (GAIL) has shown significant progress '
                     'on IL for complex continuous tasks. However, GAIL and '
                     'its extensions require a large number of environment '
                     'interactions during training. In real-world '
                     'environments, the more an IL method requires the learner '
                     'to interact with the environment for better imitation, '
                     'the more training time it requires, and the more damage '
                     'it causes to the',
         'author': ['F Sasaki', 'T Yohira', 'A Kawaguchi'],
         'cites': '18',
         'eprint': 'https://openreview.net/pdf?id=BkN5UoAqF7',
         'gsrank': '1',
         'title': 'Sample efficient imitation learning for continuous control',
         'url': 'https://openreview.net/forum?id=BkN5UoAqF7&noteId=BkN5UoAqF7',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=18386892554747535428&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSample%2BEfficient%2BImitation%2BLearning%2Bfor%2BContinuous%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=RGAAUVxdK_8J&ei=V4YqX9C4IM2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:RGAAUVxdK_8J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqIt317SzmgqR21WGKgSfMkHAi4VLtq&scisig=AAGBfm0AAAAAXyqItycjuQ9ytxj990PLi7PtbftcfU4S&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
18
-------------------------------------------------
2020-08-05 10:13:51
Got the results of the query
{'bib': {'abstract': 'Generative models for source code are an interesting '
                     'structured prediction problem, requiring to reason about '
                     'both hard syntactic and semantic constraints as well as '
                     'about natural, likely programs. We present a novel model '
                     'for this problem that uses a graph to represent the '
                     'intermediate state of the generated output. The '
                     'generative procedure interleaves grammar-driven '
                     'expansion steps with graph augmentation and neural '
                     'message passing steps. An experimental evaluation shows '
                     'that our new model can generate',
         'author': ['M Brockschmidt', 'M Allamanis', 'AL Gaunt'],
         'cites': '38',
         'eprint': 'https://arxiv.org/pdf/1805.08490',
         'gsrank': '1',
         'title': 'Generative code modeling with graphs',
         'url': 'https://arxiv.org/abs/1805.08490',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=2376600485661149991&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2BCode%2BModeling%2Bwith%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=J4vayIxh-yAJ&ei=Z4YqX-KyMcmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:J4vayIxh-yAJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqIxNhX8mk7jmL3UZ7euVSucM_rzBqo&scisig=AAGBfm0AAAAAXyqIxK0Q6JMxwFiW6zL5xxUg9bMeHv_r&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
38
-------------------------------------------------
2020-08-05 10:14:07
Got the results of the query
{'bib': {'abstract': 'Similar to humans and animals, deep artificial neural '
                     'networks exhibit critical periods during which a '
                     'temporary stimulus deficit can impair the development of '
                     'a skill. The extent of the impairment depends on the '
                     'onset and length of the deficit window, as in animal '
                     'models, and on the size of the neural network. Deficits '
                     'that do not affect low-level statistics, such as '
                     'vertical flipping of the images, have no lasting effect '
                     'on performance and can be overcome with further '
                     'training. To better understand this phenomenon, we use '
                     'the Fisher Information of',
         'author': ['A Achille', 'M Rovere', 'S Soatto'],
         'cites': '18',
         'eprint': 'https://openreview.net/pdf?id=BkeStsCcKQ',
         'gsrank': '1',
         'title': 'Critical learning periods in deep networks',
         'url': 'https://openreview.net/forum?id=BkeStsCcKQ',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=18279151376576816648&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCritical%2BLearning%2BPeriods%2Bin%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=CCZ0dFSXrP0J&ei=f4YqX6OpNcmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:CCZ0dFSXrP0J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqI3qq__rO9A-gvpNBrB-mqgntfVD1E&scisig=AAGBfm0AAAAAXyqI3rFZAJf_ixDn8gt91RssKTX9FKdg&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
18
-------------------------------------------------
2020-08-05 10:14:31
Got the results of the query
{'bib': {'abstract': 'Deep neuroevolution and deep reinforcement learning '
                     '(deep RL) algorithms are two popular approaches to '
                     'policy search. The former is widely applicable and '
                     'rather stable, but suffers from low sample efficiency. '
                     'By contrast, the latter is more sample efficient, but '
                     'the most sample efficient variants are also rather '
                     'unstable and highly sensitive to hyper-parameter '
                     'setting. So far, these families of methods have mostly '
                     'been compared as competing tools. However, an emerging '
                     'approach consists in combining them so as to get',
         'author': ['A Pourchot', 'O Sigaud'],
         'cites': '34',
         'eprint': 'https://arxiv.org/pdf/1810.01222',
         'gsrank': '1',
         'title': 'CEM-RL: Combining evolutionary and gradient-based methods '
                  'for policy search',
         'url': 'https://arxiv.org/abs/1810.01222',
         'venue': 'arXiv preprint arXiv:1810.01222',
         'year': '2018'},
 'citations_link': '/scholar?cites=11981496156929972562&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCEM-RL:%2BCombining%2Bevolutionary%2Band%2Bgradient-based%2Bmethods%2Bfor%2Bpolicy%2Bsearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=UtmEJ0bTRqYJ&ei=jIYqX6DtOZmG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:UtmEJ0bTRqYJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqI7qoNRI9n_bcN0rAm-QKRvBiVRCv1&scisig=AAGBfm0AAAAAXyqI7rIg4nW34kXSU4zeuJ72h6tZjKrF&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
34
-------------------------------------------------
2020-08-05 10:14:47
Got the results of the query
{'bib': {'abstract': 'We propose the Lanczos network (LanczosNet), which uses '
                     'the Lanczos algorithm to construct low rank '
                     'approximations of the graph Laplacian for graph '
                     'convolution. Relying on the tridiagonal decomposition of '
                     'the Lanczos algorithm, we not only efficiently exploit '
                     'multi-scale information via fast approximated '
                     'computation of matrix power but also design learnable '
                     'spectral filters. Being fully differentiable, LanczosNet '
                     'facilitates both graph kernel learning as well as '
                     'learning node embeddings. We show the connection between '
                     'our',
         'author': ['R Liao', 'Z Zhao', 'R Urtasun', 'RS Zemel'],
         'cites': '51',
         'eprint': 'https://arxiv.org/pdf/1901.01484.pdf?utm_term=',
         'gsrank': '1',
         'title': 'Lanczosnet: Multi-scale deep graph convolutional networks',
         'url': 'https://arxiv.org/abs/1901.01484',
         'venue': 'arXiv preprint arXiv:1901.01484',
         'year': '2019'},
 'citations_link': '/scholar?cites=4668385491596284189&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLanczosNet:%2BMulti-Scale%2BDeep%2BGraph%2BConvolutional%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=HREv1d5vyUAJ&ei=noYqX6aNCIuayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:HREv1d5vyUAJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqI_JGbnbP_PQ9NwiW43CZFPWtwvY8D&scisig=AAGBfm0AAAAAXyqI_Egt7MXAC6k0CYtqm97KZxqkuyX6&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
51
-------------------------------------------------
2020-08-05 10:15:00
Got the results of the query
{'bib': {'abstract': 'Despite their impressive performance, deep neural '
                     'networks exhibit striking failures on '
                     'out-of-distribution inputs. One core idea of adversarial '
                     'example research is to reveal neural network errors '
                     'under such distribution shifts. We decompose these '
                     'errors into two complementary sources: sensitivity and '
                     'invariance. We show deep networks are not only too '
                     'sensitive to task-irrelevant changes of their input, as '
                     'is well-known from epsilon-adversarial examples, but are '
                     'also too invariant to a wide range of task-relevant '
                     'changes, thus making',
         'author': ['JH Jacobsen', 'J Behrmann', 'R Zemel'],
         'cites': '47',
         'eprint': 'https://arxiv.org/pdf/1811.00401',
         'gsrank': '1',
         'title': 'Excessive invariance causes adversarial vulnerability',
         'url': 'https://arxiv.org/abs/1811.00401',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=3121934433504047296&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExcessive%2BInvariance%2BCauses%2BAdversarial%2BVulnerability%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=wLARmdNWUysJ&ei=rYYqX_biM8mdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:wLARmdNWUysJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqJC_juNsgKXOOidGJ5iNYzkpHTZqyX&scisig=AAGBfm0AAAAAXyqJC0KumyvCUNpJhHwOQSYJhMvTu84-&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
47
-------------------------------------------------
2020-08-05 10:15:16
Got the results of the query
{'bib': {'abstract': 'A reinforcement learning agent that needs to pursue '
                     'different goals across episodes requires a '
                     'goal-conditional policy. In addition to their potential '
                     'to generalize desirable behavior to unseen goals, such '
                     'policies may also enable higher-level planning based on',
         'author': ['P Rauber', 'A Ummadisingu', 'F Mutz'],
         'cites': '27',
         'eprint': 'https://arxiv.org/pdf/1711.06006',
         'gsrank': '1',
         'title': 'Hindsight policy gradients',
         'url': 'https://arxiv.org/abs/1711.06006',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=15003179791243238208&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHindsight%2Bpolicy%2Bgradients%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=QFtfpocANtAJ&ei=vIYqX-OMBIKTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:QFtfpocANtAJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqJHz0dgm99wQDPszUnP-Pzfy_qQ_tj&scisig=AAGBfm0AAAAAXyqJH12CpbeGA0aPAgZ_7MGBSA6zfop2&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
27
-------------------------------------------------
2020-08-05 10:15:36
Got the results of the query
{'bib': {'abstract': 'Adaptive optimization methods such as AdaGrad, RMSprop '
                     'and Adam have been proposed to achieve a rapid training '
                     'process with an element-wise scaling term on learning '
                     'rates. Though prevailing, they are observed to '
                     'generalize poorly compared with SGD or even fail to '
                     'converge due to unstable and extreme learning rates. '
                     'Recent work has put forward some algorithms such as '
                     'AMSGrad to tackle this issue but they failed to achieve '
                     'considerable improvement over existing methods. In our '
                     'paper, we demonstrate that extreme learning',
         'author': ['L Luo', 'Y Xiong', 'Y Liu', 'X Sun'],
         'cites': '142',
         'eprint': 'https://arxiv.org/pdf/1902.09843',
         'gsrank': '1',
         'title': 'Adaptive gradient methods with dynamic bound of learning '
                  'rate',
         'url': 'https://arxiv.org/abs/1902.09843',
         'venue': 'arXiv preprint arXiv:1902.09843',
         'year': '2019'},
 'citations_link': '/scholar?cites=7531609261550586378&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaptive%2BGradient%2BMethods%2Bwith%2BDynamic%2BBound%2Bof%2BLearning%2BRate%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=CsrDHbimhWgJ&ei=0IYqX9XQD8yXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:CsrDHbimhWgJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqJLwt4lPauXAFxXzSQTQqhZwTEeJXF&scisig=AAGBfm0AAAAAXyqJL25tsahCb8HAnZ7JpFnyPXsT8Ai0&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
142
-------------------------------------------------
2020-08-05 10:15:51
Got the results of the query
{'bib': {'abstract': 'L $ _2 $ regularization and weight decay regularization '
                     'are equivalent for standard stochastic gradient descent '
                     '(when rescaled by the learning rate), but as we '
                     'demonstrate this is\\emph {not} the case for adaptive '
                     'gradient algorithms, such as Adam. While common',
         'author': ['I Loshchilov', 'F Hutter'],
         'cites': '259',
         'eprint': 'https://arxiv.org/pdf/1711.05101.pdf]',
         'gsrank': '1',
         'title': 'Decoupled weight decay regularization',
         'url': 'https://arxiv.org/abs/1711.05101',
         'venue': 'arXiv preprint arXiv:1711.05101',
         'year': '2017'},
 'citations_link': '/scholar?cites=5602734827563786057&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDecoupled%2BWeight%2BDecay%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=SR-mH6TpwE0J&ei=4IYqX4HCFcmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:SR-mH6TpwE0J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqJRlIL8Wd5pItukaQS6R0_ws1L4Y7Q&scisig=AAGBfm0AAAAAXyqJRknXrh66taMQdPvJAU7cHhIRXQv9&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
259
-------------------------------------------------
2020-08-05 10:16:14
Got the results of the query
{'bib': {'abstract': 'Owing to their connection with generative adversarial '
                     'networks (GANs), saddle-point problems have recently '
                     'attracted considerable interest in machine learning and '
                     'beyond. By necessity, most theoretical guarantees '
                     'revolve around convex-concave (or even linear)',
         'author': ['P Mertikopoulos', 'B Lecouat', 'H Zenati', 'CS Foo'],
         'cites': '53',
         'eprint': 'https://arxiv.org/pdf/1807.02629',
         'gsrank': '1',
         'title': 'Optimistic mirror descent in saddle-point problems: Going '
                  'the extra (gradient) mile',
         'url': 'https://arxiv.org/abs/1807.02629',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=4411409906934175363&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOptimistic%2Bmirror%2Bdescent%2Bin%2Bsaddle-point%2Bproblems:%2BGoing%2Bthe%2Bextra%2B(gradient)%2Bmile%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=g1ZzDfB5OD0J&ei=9YYqX6P9CIbuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:g1ZzDfB5OD0J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqJV-2QJaOO3c82lIb4V5H2MMe6twKT&scisig=AAGBfm0AAAAAXyqJV2IDUYONd0fqXGggobPB7l0k0M_G&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
53
-------------------------------------------------
2020-08-05 10:16:31
Got the results of the query
{'bib': {'abstract': 'Variational autoencoders~(VAEs) have shown a promise in '
                     'data-driven conversation modeling. However, most VAE '
                     'conversation models match the approximate posterior '
                     'distribution over the latent variables to a simple prior '
                     'such as standard normal distribution, thereby '
                     'restricting the generated responses to a relatively '
                     'simple (eg, unimodal) scope. In this paper, we propose '
                     'DialogWAE, a conditional Wasserstein autoencoder~(WAE) '
                     'specially designed for dialogue modeling. Unlike VAEs '
                     'that impose a simple distribution',
         'author': ['X Gu', 'K Cho', 'JW Ha', 'S Kim'],
         'cites': '49',
         'eprint': 'https://arxiv.org/pdf/1805.12352',
         'gsrank': '1',
         'title': 'Dialogwae: Multimodal response generation with conditional '
                  'wasserstein auto-encoder',
         'url': 'https://arxiv.org/abs/1805.12352',
         'venue': 'arXiv preprint arXiv:1805.12352',
         'year': '2018'},
 'citations_link': '/scholar?cites=3591710081490434640&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDialogWAE:%2BMultimodal%2BResponse%2BGeneration%2Bwith%2BConditional%2BWasserstein%2BAuto-Encoder%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=UDptF0pR2DEJ&ei=CIcqX9HzL8mdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:UDptF0pR2DEJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqJaM81u6Wq32ZY0HYkL4Gs49Bp40iZ&scisig=AAGBfm0AAAAAXyqJaO_bAJNlabDeFihXOPjLrqQDDbHI&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
49
-------------------------------------------------
2020-08-05 10:16:48
Got the results of the query
{'bib': {'abstract': 'We explore various methods for computing sentence '
                     'representations from pre-trained word embeddings without '
                     'any training, ie, using nothing but random '
                     'parameterizations. Our aim is to put sentence embeddings '
                     'on more solid footing by 1) looking at how much modern '
                     'sentence embeddings gain over random methods---as it '
                     'turns out, surprisingly little; and by 2) providing the '
                     'field with more appropriate baselines going '
                     'forward---which are, as it turns out, quite strong. We '
                     'also make important observations about proper '
                     'experimental protocol',
         'author': ['J Wieting', 'D Kiela'],
         'cites': '50',
         'eprint': 'https://arxiv.org/pdf/1901.10444',
         'gsrank': '1',
         'title': 'No training required: Exploring random encoders for '
                  'sentence classification',
         'url': 'https://arxiv.org/abs/1901.10444',
         'venue': 'arXiv preprint arXiv:1901.10444',
         'year': '2019'},
 'citations_link': '/scholar?cites=12787240152315433650&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNo%2BTraining%2BRequired:%2BExploring%2BRandom%2BEncoders%2Bfor%2BSentence%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=spJfOCtndbEJ&ei=G4cqX_L8IYuayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:spJfOCtndbEJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqJfNNgq4_EGGUvHmWw6Jgn346DzfI0&scisig=AAGBfm0AAAAAXyqJfCVjez26UEkOjLmRK_yvMy9fw4Sh&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
50
-------------------------------------------------
2020-08-05 10:17:09
Got the results of the query
{'bib': {'abstract': 'Despite the recent successes in robotic locomotion '
                     'control, the design of robot relies heavily on human '
                     'engineering. Automatic robot design has been a long '
                     'studied subject, but the recent progress has been slowed '
                     'due to the large combinatorial search space and the '
                     'difficulty in evaluating the found candidates. To '
                     'address the two challenges, we formulate automatic robot '
                     'design as a graph search problem and perform evolution '
                     'search in graph space. We propose Neural Graph Evolution '
                     '(NGE), which performs selection on current',
         'author': ['T Wang', 'Y Zhou', 'S Fidler', 'J Ba'],
         'cites': '6',
         'eprint': 'https://arxiv.org/pdf/1906.05370',
         'gsrank': '1',
         'title': 'Neural graph evolution: Towards efficient automatic robot '
                  'design',
         'url': 'https://arxiv.org/abs/1906.05370',
         'venue': 'arXiv preprint arXiv:1906.05370',
         'year': '2019'},
 'citations_link': '/scholar?cites=2252025967426248193&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BGraph%2BEvolution:%2BTowards%2BEfficient%2BAutomatic%2BRobot%2BDesign%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=AZ5lha_NQB8J&ei=K4cqX5W-GoS0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:AZ5lha_NQB8J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqJj4nUimyksDYCRgUOZbduS_N6D5f5&scisig=AAGBfm0AAAAAXyqJj9pr3RqhrKh5J_4bjN8XQsMGdWj-&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 10:17:32
Got the results of the query
{'bib': {'abstract': 'While Bayesian neural networks (BNNs) have drawn '
                     'increasing attention, their posterior inference remains '
                     'challenging, due to the high-dimensional and '
                     'over-parameterized nature. To address this issue, '
                     'several highly flexible and scalable variational '
                     'inference procedures based on the idea of particle '
                     'optimization have been proposed. These methods directly '
                     'optimize a set of particles to approximate the target '
                     'posterior. However, their application to BNNs often '
                     'yields sub-optimal performance, as such methods have a '
                     'particular failure mode',
         'author': ['Z Wang', 'T Ren', 'J Zhu', 'B Zhang'],
         'cites': '12',
         'eprint': 'https://arxiv.org/pdf/1902.09754',
         'gsrank': '1',
         'title': 'Function space particle optimization for Bayesian neural '
                  'networks',
         'url': 'https://arxiv.org/abs/1902.09754',
         'venue': 'arXiv preprint arXiv:1902.09754',
         'year': '2019'},
 'citations_link': '/scholar?cites=3265058804151062573&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFunction%2BSpace%2BParticle%2BOptimization%2Bfor%2BBayesian%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=LZAv8K7RTy0J&ei=RIcqX7r9GobuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:LZAv8K7RTy0J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqJpSdWtCjig6f020Kte05obK5DexNN&scisig=AAGBfm0AAAAAXyqJpRw5hXwOpnd1yXahW2_0bXdrJpbZ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 10:17:49
Got the results of the query
{'bib': {'abstract': 'When generating adversarial examples to attack deep '
                     'neural networks (DNNs), Lp norm of the added '
                     'perturbation is usually used to measure the similarity '
                     'between original image and adversarial example. However, '
                     'such adversarial attacks perturbing the raw input spaces '
                     'may fail to capture structural information hidden in the '
                     'input. This work develops a more general attack model, '
                     'ie, the structured attack (StrAttack), which explores '
                     'group sparsity in adversarial perturbations by sliding a '
                     'mask through images aiming for extracting key spatial',
         'author': ['K Xu', 'S Liu', 'P Zhao', 'PY Chen', 'H Zhang', 'Q Fan'],
         'cites': '48',
         'eprint': 'https://arxiv.org/pdf/1808.01664',
         'gsrank': '1',
         'title': 'Structured adversarial attack: Towards general '
                  'implementation and better interpretability',
         'url': 'https://arxiv.org/abs/1808.01664',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=2416957312060244972&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStructured%2BAdversarial%2BAttack:%2BTowards%2BGeneral%2BImplementation%2Band%2BBetter%2BInterpretability%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=7EOipt7BiiEJ&ei=VIcqX-jdN82iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:7EOipt7BiiEJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqJt7IclyCvsTRKDPG3FEYuXE2Y3SNU&scisig=AAGBfm0AAAAAXyqJt3FBNcy1jrDRfi2S4RNUSlCJ58Ik&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
48
-------------------------------------------------
2020-08-05 10:18:07
Got the results of the query
{'bib': {'abstract': 'We present an efficient convolution kernel for '
                     'Convolutional Neural Networks (CNNs) on unstructured '
                     'grids using parameterized differential operators while '
                     'focusing on spherical signals such as panorama images or '
                     'planetary signals. To this end, we replace',
         'author': ['C Jiang', 'J Huang', 'K Kashinath', 'P Marcus'],
         'cites': '39',
         'eprint': 'https://arxiv.org/pdf/1901.02039',
         'gsrank': '1',
         'title': 'Spherical CNNs on unstructured grids',
         'url': 'https://arxiv.org/abs/1901.02039',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=8988090417232263617&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpherical%2BCNNs%2Bon%2BUnstructured%2BGrids%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=wbk4EJ0cvHwJ&ei=Z4cqX-fkM5mG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:wbk4EJ0cvHwJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqJxzNepY7rjP7aCXrwgq0zO9d0p56P&scisig=AAGBfm0AAAAAXyqJx416-SjboFYmxKH7s-Wis3WizNaH&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
39
-------------------------------------------------
2020-08-05 10:18:24
Got the results of the query
{'bib': {'abstract': 'Generative models such as Variational Auto Encoders '
                     '(VAEs) and Generative Adversarial Networks (GANs) are '
                     'typically trained for a fixed prior distribution in the '
                     'latent space, such as uniform or Gaussian. After a '
                     'trained model is obtained, one can sample the Generator '
                     'in various forms for exploration and understanding, such '
                     'as interpolating between two samples, sampling in the '
                     'vicinity of a sample or exploring differences between a '
                     'pair of samples applied to a third sample. In this '
                     'paper, we show that the latent space operations',
         'author': ['E Agustsson', 'A Sage', 'R Timofte', 'L Van Gool'],
         'cites': '9',
         'eprint': 'https://arxiv.org/pdf/1711.01970',
         'gsrank': '1',
         'title': 'Optimal transport maps for distribution preserving '
                  'operations on latent spaces of generative models',
         'url': 'https://arxiv.org/abs/1711.01970',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=11631258824141665090&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOptimal%2BTransport%2BMaps%2BFor%2BDistribution%2BPreserving%2BOperations%2Bon%2BLatent%2BSpaces%2Bof%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=QnOXh0aIaqEJ&ei=eocqX_CUEouayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:QnOXh0aIaqEJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqJ2RfKJ5Qr87kzF2eNoF3OPF6Pifyp&scisig=AAGBfm0AAAAAXyqJ2SEi5XorceaKygiw6VLMYP8oO6t2&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 10:18:41
Got the results of the query
{'bib': {'abstract': 'Deep learning has achieved astonishing results on many '
                     'tasks with large amounts of data and generalization '
                     'within the proximity of training data. For many '
                     'important real-world applications, these requirements '
                     'are unfeasible and additional prior knowledge on the '
                     'task domain is required to overcome the resulting '
                     'problems. In particular, learning physics models for '
                     'model-based control requires robust extrapolation from '
                     'fewer samples-often collected online in real-time-and '
                     'model errors may lead to drastic damages of the system',
         'author': ['M Lutter', 'C Ritter', 'J Peters'],
         'cites': '46',
         'eprint': 'https://arxiv.org/pdf/1907.04490',
         'gsrank': '1',
         'title': 'Deep lagrangian networks: Using physics as model prior for '
                  'deep learning',
         'url': 'https://arxiv.org/abs/1907.04490',
         'venue': 'arXiv preprint arXiv:1907.04490',
         'year': '2019'},
 'citations_link': '/scholar?cites=7347447340504722974&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLagrangian%2BNetworks:%2BUsing%2BPhysics%2Bas%2BModel%2BPrior%2Bfor%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Hi6sgG1g92UJ&ei=i4cqX5W7L4uayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Hi6sgG1g92UJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqJ6b24p9ab9NgS6QBXkuKfL9OGoLaj&scisig=AAGBfm0AAAAAXyqJ6e7Hk1bElsyq23oryvK7UOV8fLK9&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
46
-------------------------------------------------
2020-08-05 10:18:58
Got the results of the query
{'bib': {'abstract': 'Efforts to reduce the numerical precision of '
                     'computations in deep learning training have yielded '
                     'systems that aggressively quantize weights and '
                     'activations, yet employ wide high-precision accumulators '
                     'for partial sums in inner-product operations to preserve '
                     'the quality of convergence. The absence of any framework '
                     'to analyze the precision requirements of partial sum '
                     'accumulations results in conservative design choices. '
                     'This imposes an upper-bound on the reduction of '
                     'complexity of multiply-accumulate units. We present a '
                     'statistical',
         'author': ['C Sakr', 'N Wang', 'CY Chen', 'J Choi', 'A Agrawal'],
         'cites': '6',
         'eprint': 'https://arxiv.org/pdf/1901.06588',
         'gsrank': '1',
         'title': 'Accumulation bit-width scaling for ultra-low precision '
                  'training of deep networks',
         'url': 'https://arxiv.org/abs/1901.06588',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=16539260459869643539&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAccumulation%2BBit-Width%2BScaling%2BFor%2BUltra-Low%2BPrecision%2BTraining%2BOf%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=E6MxbsJBh-UJ&ei=nIcqX_LkAs2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:E6MxbsJBh-UJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqJ_Jio4lcXTbD2OtBQUPb49ga6qZAr&scisig=AAGBfm0AAAAAXyqJ_OB0CGFIqM38VMBAgBUGUCiekvS_&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 10:19:17
Got the results of the query
{'bib': {'abstract': 'We show that the output of a (residual) convolutional '
                     'neural network (CNN) with an appropriate prior over the '
                     'weights and biases is a Gaussian process (GP) in the '
                     'limit of infinitely many convolutional filters, '
                     'extending similar results for dense networks. For a CNN, '
                     'the equivalent kernel can be computed exactly and, '
                     'unlike" deep kernels", has very few parameters: only the '
                     'hyperparameters of the original CNN. Further, we show '
                     'that this kernel has two properties that allow it to be '
                     'computed efficiently; the cost of evaluating the kernel '
                     'for',
         'author': ['A Garriga-Alonso', 'CE Rasmussen'],
         'cites': '67',
         'eprint': 'https://arxiv.org/pdf/1808.05587',
         'gsrank': '1',
         'title': 'Deep convolutional networks as shallow gaussian processes',
         'url': 'https://arxiv.org/abs/1808.05587',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=3896108923568012105&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BConvolutional%2BNetworks%2Bas%2Bshallow%2BGaussian%2BProcesses%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=STuAa2zCETYJ&ei=rocqX9vYL82iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:STuAa2zCETYJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqKEpKx3xRKFrshSOl8XgQWufMKrumf&scisig=AAGBfm0AAAAAXyqKEgE1iq2EfmCkTpjcZFvOonRcA3Lz&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
67
-------------------------------------------------
2020-08-05 10:19:38
Got the results of the query
{'bib': {'abstract': 'Unsupervised domain adaptation is a promising avenue to '
                     'enhance the performance of deep neural networks on a '
                     'target domain, using labels only from a source domain. '
                     'However, the two predominant methods, domain discrepancy '
                     'reduction learning and semi-supervised learning, are not '
                     'readily applicable when source and target domains do not '
                     'share a common label space. This paper addresses the '
                     'above scenario by learning a representation space that '
                     'retains discriminative power on both the (labeled) '
                     'source and (unlabeled) target',
         'author': ['K Sohn', 'W Shang', 'X Yu'],
         'cites': '21',
         'eprint': 'https://openreview.net/pdf?id=BklhAj09K7',
         'gsrank': '1',
         'title': 'Unsupervised domain adaptation for distance metric learning',
         'url': 'https://openreview.net/forum?id=BklhAj09K7',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8422183633615722259&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BDomain%2BAdaptation%2Bfor%2BDistance%2BMetric%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ExfGYmSb4XQJ&ei=wYcqX9yIBMmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ExfGYmSb4XQJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqKJeVFwhWjSskcjbuJkD2k39z726FB&scisig=AAGBfm0AAAAAXyqKJUuMuLPYdscus-X3Pb0wxtYGGev8&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
21
-------------------------------------------------
2020-08-05 10:19:58
Got the results of the query
{'bib': {'abstract': 'We present a large-scale empirical study of catastrophic '
                     'forgetting (CF) in modern Deep Neural Network (DNN) '
                     'models that perform sequential (or: incremental) '
                     'learning. A new experimental protocol is proposed that '
                     'enforces typical constraints encountered in application '
                     'scenarios. As the investigation is empirical, we '
                     'evaluate CF behavior on the hitherto largest number of '
                     'visual classification datasets, from each of which we '
                     'construct a representative number of Sequential Learning '
                     'Tasks (SLTs) in close alignment to previous',
         'author': ['B Pfülb', 'A Gepperth'],
         'cites': '17',
         'eprint': 'https://arxiv.org/pdf/1905.08101',
         'gsrank': '1',
         'title': 'A comprehensive, application-oriented study of catastrophic '
                  'forgetting in dnns',
         'url': 'https://arxiv.org/abs/1905.08101',
         'venue': 'arXiv preprint arXiv:1905.08101',
         'year': '2019'},
 'citations_link': '/scholar?cites=4360661774443932111&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2Bcomprehensive,%2Bapplication-oriented%2Bstudy%2Bof%2Bcatastrophic%2Bforgetting%2Bin%2BDNNs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=z-las8cuhDwJ&ei=2IcqX4KTJ8mdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:z-las8cuhDwJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqKOENM8h3HM33ey4v1TDstY8ryrHLM&scisig=AAGBfm0AAAAAXyqKONl61hpppw4cQCezNkKSq6oJMbvm&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
17
-------------------------------------------------
2020-08-05 10:20:17
Got the results of the query
{'bib': {'abstract': 'Modern neural architectures critically rely on attention '
                     'for mapping structured inputs to sequences. In this '
                     'paper we show that prevalent attention architectures do '
                     'not adequately model the dependence among the attention '
                     'and output tokens across a predicted sequence. We '
                     'present an alternative architecture called Posterior '
                     'Attention Models that after a principled factorization '
                     'of the full joint distribution of the attention and '
                     'output variables, proposes two major changes. First, the '
                     'position where attention is marginalized is changed',
         'author': ['S Shankar', 'S Sarawagi'],
         'cites': '14',
         'eprint': 'https://openreview.net/pdf?id=BkltNhC9FX',
         'gsrank': '1',
         'title': 'Posterior attention models for sequence to sequence '
                  'learning',
         'url': 'https://openreview.net/forum?id=BkltNhC9FX&noteId=BJe8NreK2m',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=6570316980563618012&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPosterior%2BAttention%2BModels%2Bfor%2BSequence%2Bto%2BSequence%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=3Cg2mHt0LlsJ&ei=6YcqX53fD8mdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:3Cg2mHt0LlsJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqKSUWV_gEv1BsaSzbIifbryycJpMM0&scisig=AAGBfm0AAAAAXyqKSah7bqM9ff4chPqCSUjolJgwSJ07&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
14
-------------------------------------------------
2020-08-05 10:20:34
Got the results of the query
{'bib': {'abstract': 'Discriminative question answering models can overfit to '
                     'superficial biases in datasets, because their loss '
                     'function saturates when any clue makes the answer '
                     'likely. We introduce generative models of the joint '
                     'distribution of questions and answers, which are trained '
                     'to explain the whole question, not just to answer it. '
                     'Our question answering (QA) model is implemented by '
                     'learning a prior over answers, and a conditional '
                     'language model to generate the question given the '
                     'answer—allowing scalable and interpretable many-hop',
         'author': ['M Lewis', 'A Fan'],
         'cites': '16',
         'eprint': 'https://openreview.net/pdf?id=Bkx0RjA9tX',
         'gsrank': '1',
         'title': 'Generative question answering: Learning to answer the whole '
                  'question',
         'url': 'https://openreview.net/forum?id=Bkx0RjA9tX&utm_campaign=Data%20Machina&utm_medium=email&utm_source=Revue%20newsletter',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=9070819050812325363&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2BQuestion%2BAnswering:%2BLearning%2Bto%2BAnswer%2Bthe%2BWhole%2BQuestion%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=872DEd4F4n0J&ei=-IcqX_m1HZmG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:872DEd4F4n0J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqKWeRLf-J9T-e_1bs_c2vkkDm5Bkay&scisig=AAGBfm0AAAAAXyqKWQ_5wEvNKJqqhOUTngFbocMqXwwE&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
16
-------------------------------------------------
2020-08-05 10:20:49
Got the results of the query
{'bib': {'abstract': 'Routing models, a form of conditional computation where '
                     'examples are routed through a subset of components in a '
                     'larger network, have shown promising results in recent '
                     'works. Surprisingly, routing models to date have lacked '
                     'important properties, such as architectural diversity '
                     'and large numbers of routing decisions. Both '
                     'architectural diversity and routing depth can increase '
                     'the representational power of a routing network. In this '
                     'work, we address both of these deficiencies. We discuss '
                     'the significance of architectural diversity in routing',
         'author': ['P Ramachandran', 'QV Le'],
         'cites': '7',
         'eprint': 'https://openreview.net/pdf?id=BkxWJnC9tX',
         'gsrank': '1',
         'title': 'Diversity and depth in per-example routing models',
         'url': 'https://openreview.net/forum?id=BkxWJnC9tX',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15364434217579695622&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiversity%2Band%2BDepth%2Bin%2BPer-Example%2BRouting%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Bs6xQYRvOdUJ&ei=CYgqX4GLIoS0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Bs6xQYRvOdUJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqKajGxtl_XPPGHg74AjI3WwXw5-TkM&scisig=AAGBfm0AAAAAXyqKai1y895RlnhBktlAXUoeMoO8oy_3&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
7
-------------------------------------------------
2020-08-05 10:21:07
Got the results of the query
{'bib': {'abstract': 'Sequential learning, also called lifelong learning, '
                     'studies the problem of learning tasks in a sequence with '
                     'access restricted to only the data of the current task. '
                     'In this paper we look at a scenario with fixed model '
                     'capacity, and postulate that the learning process should '
                     'not be',
         'author': ['R Aljundi', 'M Rohrbach', 'T Tuytelaars'],
         'cites': '24',
         'eprint': 'https://arxiv.org/pdf/1806.05421',
         'gsrank': '1',
         'title': 'Selfless sequential learning',
         'url': 'https://arxiv.org/abs/1806.05421',
         'venue': 'arXiv preprint arXiv:1806.05421',
         'year': '2018'},
 'citations_link': '/scholar?cites=11518728044683719539&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSelfless%2BSequential%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=c_OztyC-2p8J&ei=HIgqX4CABYKTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:c_OztyC-2p8J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqKfbbWddbV0gz3Sz69Rcw5wK4G9Fc7&scisig=AAGBfm0AAAAAXyqKfbO3s4_S-1mOSwwEn2as4DMkppaW&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
24
-------------------------------------------------
2020-08-05 10:21:25
Got the results of the query
{'bib': {'abstract': 'Most of the prior work on multi-agent reinforcement '
                     'learning (MARL) achieves optimal collaboration by '
                     'directly controlling the agents to maximize a common '
                     'reward. In this paper, we aim to address this from a '
                     'different angle. In particular, we consider scenarios '
                     'where there are self-interested agents (ie, worker '
                     'agents) which have their own minds (preferences, '
                     'intentions, skills, etc.) and can not be dictated to '
                     'perform tasks they do not wish to do. For achieving '
                     'optimal coordination among these agents, we train a '
                     'super agent (ie',
         'author': ['T Shu', 'Y Tian'],
         'cites': '15',
         'eprint': 'https://arxiv.org/pdf/1810.00147',
         'gsrank': '1',
         'title': 'MRL: Mind-aware Multi-agent Management Reinforcement '
                  'Learning',
         'url': 'https://arxiv.org/abs/1810.00147',
         'venue': 'arXiv preprint arXiv:1810.00147',
         'year': '2018'},
 'citations_link': '/scholar?cites=841802929654227287&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DM%255E3RL:%2BMind-aware%2BMulti-agent%2BManagement%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=V2Hs8E2vrgsJ&ei=LYgqX9zoJ8mdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:V2Hs8E2vrgsJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqKj4c6OhBN2ACe9cJ0XvWB3ky-j5jf&scisig=AAGBfm0AAAAAXyqKjzLtKShBodFRUhqpRsaA-KSgH94c&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 10:21:43
Got the results of the query
{'bib': {'abstract': 'Page 5. The number of weights d in deep networks tends '
                     'to be very large. As such, learning a distribution over '
                     'each weight, ie estimating µ ∈ Rd and Σ ∈ Rd×d, becomes '
                     'intractable. We instead use a layer-wise approach',
         'author': ['D Van Veen', 'A Jalal', 'M Soltanolkotabi', 'E Price'],
         'cites': '54',
         'eprint': 'https://arxiv.org/pdf/1806.06438',
         'gsrank': '1',
         'title': 'Compressed sensing with deep image prior and learned '
                  'regularization',
         'url': 'https://arxiv.org/abs/1806.06438',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=17494647236470015589&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BDeep%2BWeight%2BPrior%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ZYaQkPh4yfIJ&ei=QogqX_L1O8mdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ZYaQkPh4yfIJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqKpExlZ1HVNJofwwWuaU0WSnXed5kc&scisig=AAGBfm0AAAAAXyqKpMxB8xKVVJ6J6mSIov_JG1WtRTv_&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
54
-------------------------------------------------
2020-08-05 10:22:04
Got the results of the query
{'bib': {'abstract': 'Neural Architecture Search aims at automatically finding '
                     'neural architectures that are competitive with '
                     'architectures designed by human experts. While recent '
                     'approaches have achieved state-of-the-art predictive '
                     'performance for image recognition, they are problematic '
                     'under resource constraints for two reasons:(1) the '
                     'neural architectures found are solely optimized for high '
                     'predictive performance, without penalizing excessive '
                     'resource consumption,(2) most architecture search '
                     'methods require vast computational resources',
         'author': ['T Elsken', 'JH Metzen', 'F Hutter'],
         'cites': '108',
         'eprint': 'https://arxiv.org/pdf/1804.09081',
         'gsrank': '1',
         'title': 'Efficient multi-objective neural architecture search via '
                  'lamarckian evolution',
         'url': 'https://arxiv.org/abs/1804.09081',
         'venue': 'arXiv preprint arXiv:1804.09081',
         'year': '2018'},
 'citations_link': '/scholar?cites=7285962348545895359&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BMulti-Objective%2BNeural%2BArchitecture%2BSearch%2Bvia%2BLamarckian%2BEvolution%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=v-vyPyfwHGUJ&ei=VYgqX-LfHobuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:v-vyPyfwHGUJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqKtzha1Ctpnug0pqFwA1fhFm1isft8&scisig=AAGBfm0AAAAAXyqKt9-W9FToHLu8heKhzPlsHGEVeVPV&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
108
-------------------------------------------------
2020-08-05 10:22:23
Got the results of the query
{'bib': {'abstract': 'Recurrent neural networks (RNNs) are powerful '
                     'architectures to model sequential data, due to their '
                     'capability to learn short and long-term dependencies '
                     'between the basic elements of a sequence. Nonetheless, '
                     'popular tasks such as speech or images recognition, '
                     'involve',
         'author': ['T Parcollet', 'M Ravanelli', 'M Morchid', 'G Linarès'],
         'cites': '28',
         'eprint': 'https://arxiv.org/pdf/1806.04418',
         'gsrank': '1',
         'title': 'Quaternion recurrent neural networks',
         'url': 'https://arxiv.org/abs/1806.04418',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=13606271573268587298&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQuaternion%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ImMrIugv07wJ&ei=ZogqX8XSM8yXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ImMrIugv07wJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqKzYEp0J03wQDnLjLFPODvHwA7Gwqe&scisig=AAGBfm0AAAAAXyqKzebdAoq79gDYI5A84Cz1xkj74fh6&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
28
-------------------------------------------------
2020-08-05 10:22:46
Got the results of the query
{'bib': {'abstract': 'Audio signals are sampled at high temporal resolutions, '
                     'and learning to synthesize audio requires capturing '
                     'structure across a range of timescales. Generative '
                     'adversarial networks (GANs) have seen wide success at '
                     'generating images that are both locally and globally '
                     'coherent, but they have seen little application to audio '
                     'generation. In this paper we introduce WaveGAN, a first '
                     'attempt at applying GANs to unsupervised synthesis of '
                     'raw-waveform audio. WaveGAN is capable of synthesizing '
                     'one second slices of audio waveforms with',
         'author': ['C Donahue', 'J McAuley', 'M Puckette'],
         'cites': '106',
         'eprint': 'https://arxiv.org/pdf/1802.04208',
         'gsrank': '1',
         'title': 'Adversarial audio synthesis',
         'url': 'https://arxiv.org/abs/1802.04208',
         'venue': 'arXiv preprint arXiv:1802.04208',
         'year': '2018'},
 'citations_link': '/scholar?cites=5918610073287101746&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BAudio%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Mo1T5oAgI1IJ&ei=f4gqX9e5EIKTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Mo1T5oAgI1IJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqK3g1U_2wbrZrj7qWt-CWXpeNT0P0v&scisig=AAGBfm0AAAAAXyqK3uh8qY1n-eX1oiD4LFw8FosTPVGu&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
106
-------------------------------------------------
2020-08-05 10:23:02
Got the results of the query
{'bib': {'abstract': 'We study two types of preconditioners and preconditioned '
                     'stochastic gradient descent (SGD) methods in a unified '
                     'framework. We call the first one the Newton type due to '
                     'its close relationship to the Newton method, and the '
                     'second one the Fisher type as its preconditioner is '
                     'closely related to the inverse of Fisher information '
                     'matrix. Both preconditioners can be derived from one '
                     'framework, and efficiently estimated on any matrix Lie '
                     'groups designated by the user using natural or relative '
                     'gradient descent minimizing certain preconditioner',
         'author': ['XL Li'],
         'cites': '1',
         'eprint': 'https://arxiv.org/pdf/1809.10232',
         'gsrank': '1',
         'title': 'Preconditioner on Matrix Lie Group for SGD',
         'url': 'https://arxiv.org/abs/1809.10232',
         'venue': 'arXiv preprint arXiv:1809.10232',
         'year': '2018'},
 'citations_link': '/scholar?cites=631643897928454629&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPreconditioner%2Bon%2BMatrix%2BLie%2BGroup%2Bfor%2BSGD%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=5ZGRUMcMxAgJ&ei=jogqX_-RGIuayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:5ZGRUMcMxAgJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqK8dS3Xklgs6m_Tepib1qeVZBogOQX&scisig=AAGBfm0AAAAAXyqK8YcAvfFcCFaTFAkUHLKtJ-t1Dgzb&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
1
-------------------------------------------------
2020-08-05 10:23:22
Got the results of the query
{'bib': {'abstract': 'Neural language models have been widely used in various '
                     'NLP tasks, including machine translation, next word '
                     'prediction and conversational agents. However, it is '
                     'challenging to deploy these models on mobile devices due '
                     'to their slow prediction speed, where the bottleneck is '
                     'to compute top candidates in the softmax layer. In this '
                     'paper, we introduce a novel softmax layer approximation '
                     'algorithm by exploiting the clustering structure of '
                     'context vectors. Our algorithm uses a light-weight '
                     'screening model to predict a much smaller set of',
         'author': ['PH Chen', 'S Si', 'S Kumar', 'Y Li', 'CJ Hsieh'],
         'cites': '1',
         'eprint': 'https://arxiv.org/pdf/1810.12406',
         'gsrank': '1',
         'title': 'Learning to screen for fast Softmax inference on large '
                  'vocabulary neural networks',
         'url': 'https://arxiv.org/abs/1810.12406',
         'venue': 'arXiv preprint arXiv:1810.12406',
         'year': '2018'},
 'citations_link': '/scholar?cites=17886875272425018344&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BScreen%2Bfor%2BFast%2BSoftmax%2BInference%2Bon%2BLarge%2BVocabulary%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=6JOcokryOvgJ&ei=oIgqX9q5Hs2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:6JOcokryOvgJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqK_hRO3CXqja2sd7ZI6MfFycDZyZVs&scisig=AAGBfm0AAAAAXyqK_p1Ela65fNyLSpE20Dkt8wJH6ng2&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
1
-------------------------------------------------
2020-08-05 10:23:34
Got the results of the query
{'bib': {'abstract': 'The ability to generalize quickly from few observations '
                     'is crucial for intelligent systems. In this paper we '
                     'introduce APL, an algorithm that approximates '
                     'probability distributions by remembering the most '
                     'surprising observations it has encountered. These past '
                     'observations are recalled from an external memory module '
                     'and processed by a decoder network that can combine '
                     'information from different memory slots to generalize '
                     'beyond direct recall. We show this algorithm can perform '
                     'as well as state of the art baselines on few-shot '
                     'classification',
         'author': ['T Ramalho', 'M Garnelo'],
         'cites': '11',
         'eprint': 'https://arxiv.org/pdf/1902.02527',
         'gsrank': '1',
         'title': 'Adaptive posterior learning: few-shot learning with a '
                  'surprise-based memory module',
         'url': 'https://arxiv.org/abs/1902.02527',
         'venue': 'arXiv preprint arXiv:1902.02527',
         'year': '2019'},
 'citations_link': '/scholar?cites=3877086335539241291&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaptive%2BPosterior%2BLearning:%2Bfew-shot%2Blearning%2Bwith%2Ba%2Bsurprise-based%2Bmemory%2Bmodule%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=S2FWAXstzjUJ&ei=sYgqX9W-E5mG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:S2FWAXstzjUJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqLEj156Mdc2r3oTxgPFS3g8fCG8s-K&scisig=AAGBfm0AAAAAXyqLElyO9vBZRhFAzfAsKZM6r4MrarH6&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
11
-------------------------------------------------
2020-08-05 10:23:55
Got the results of the query
{'bib': {'abstract': 'In this work, we propose a novel formulation of planning '
                     'which views it as a probabilistic inference problem over '
                     'future optimal trajectories. This enables us to use '
                     'sampling methods, and thus, tackle planning in '
                     'continuous domains using a fixed computational budget. '
                     'We design a new algorithm, Sequential Monte Carlo '
                     'Planning, by leveraging classical methods in Sequential '
                     'Monte Carlo and Bayesian smoothing in the context of '
                     'control as inference. Furthermore, we show that '
                     'Sequential Monte Carlo Planning can capture multimodal',
         'author': ['A Piché', 'V Thomas', 'C Ibrahim', 'Y Bengio'],
         'cites': '8',
         'eprint': 'https://openreview.net/pdf?id=ByetGn0cYX',
         'gsrank': '1',
         'title': 'Probabilistic planning with sequential monte carlo methods',
         'url': 'https://openreview.net/forum?id=ByetGn0cYX',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1860428806595804827&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProbabilistic%2BPlanning%2Bwith%2BSequential%2BMonte%2BCarlo%2Bmethods%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=m_4LgySS0RkJ&ei=wYgqX4G1MsmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:m_4LgySS0RkJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqLJwXn2NPdAf_nIMiSdooApGNX1BCY&scisig=AAGBfm0AAAAAXyqLJ0i4b6LYUaal4otDe7TxmoRs2TL9&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
8
-------------------------------------------------
2020-08-05 10:24:16
Got the results of the query
{'bib': {'abstract': 'We propose a plan online and learn offline (POLO) '
                     'framework for the setting where an agent, with an '
                     'internal model, needs to continually act and learn in '
                     'the world. Our work builds on the synergistic '
                     'relationship between local model-based control, global '
                     'value function learning, and exploration. We study how '
                     'local trajectory optimization can cope with '
                     'approximation errors in the value function, and can '
                     'stabilize and accelerate value function learning. '
                     'Conversely, we also study how approximate value '
                     'functions can help reduce the',
         'author': ['K Lowrey', 'A Rajeswaran', 'S Kakade', 'E Todorov'],
         'cites': '50',
         'eprint': 'https://arxiv.org/pdf/1811.01848',
         'gsrank': '1',
         'title': 'Plan online, learn offline: Efficient learning and '
                  'exploration via model-based control',
         'url': 'https://arxiv.org/abs/1811.01848',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=12747766892860310052&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPlan%2BOnline,%2BLearn%2BOffline:%2BEfficient%2BLearning%2Band%2BExploration%2Bvia%2BModel-Based%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=JIJs0HIq6bAJ&ei=1ogqX-_KIMmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:JIJs0HIq6bAJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqLNVddNB2xExP8RkSqMcy-3Q1atGKw&scisig=AAGBfm0AAAAAXyqLNTkFL-NCQxApSiKOaaB0FqvAcEmW&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
50
-------------------------------------------------
2020-08-05 10:24:30
Got the results of the query
{'bib': {'abstract': 'Dealing with sparse rewards is one of the most important '
                     'challenges in reinforcement learning (RL), especially '
                     'when a goal is dynamic (eg, to grasp a moving object). '
                     'Hindsight experience replay (HER) has been shown an '
                     'effective solution to handling sparse rewards with fixed '
                     'goals. However, it does not account for dynamic goals in '
                     'its vanilla form and, as a result, even degrades the '
                     'performance of existing off-policy RL algorithms when '
                     'the goal is changing over time. In this paper, we '
                     'present Dynamic Hindsight Experience Replay',
         'author': ['M Fang', 'C Zhou', 'B Shi', 'B Gong', 'J Xu'],
         'cites': '9',
         'eprint': 'https://openreview.net/pdf?id=Byf5-30qFX',
         'gsrank': '1',
         'title': 'DHER: Hindsight experience replay for dynamic goals',
         'url': 'https://openreview.net/forum?id=Byf5-30qFX',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=7304888076080019198&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDHER:%2BHindsight%2BExperience%2BReplay%2Bfor%2BDynamic%2BGoals%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=_na8kgAtYGUJ&ei=5IgqX7jAHsyXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:_na8kgAtYGUJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqLSJU9QM7i-HOYE_vn1eyHabb4-N__&scisig=AAGBfm0AAAAAXyqLSPbRnBavEeDSFws97ZKOUYA994bN&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 10:24:48
Got the results of the query
{'bib': {'abstract': 'Conversational machine comprehension requires the '
                     'understanding of the conversation history, such as '
                     'previous question/answer pairs, the document context, '
                     'and the current question. To enable traditional, '
                     'single-turn models to encode the history '
                     'comprehensively, we introduce Flow, a mechanism that can '
                     'incorporate intermediate representations generated '
                     'during the process of answering previous questions, '
                     'through an alternating parallel processing structure. '
                     'Compared to approaches that concatenate previous',
         'author': ['HY Huang', 'E Choi', 'W Yih'],
         'cites': '36',
         'eprint': 'https://arxiv.org/pdf/1810.06683',
         'gsrank': '1',
         'title': 'Flowqa: Grasping flow in history for conversational machine '
                  'comprehension',
         'url': 'https://arxiv.org/abs/1810.06683',
         'venue': 'arXiv preprint arXiv:1810.06683',
         'year': '2018'},
 'citations_link': '/scholar?cites=13021094548556076955&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFlowQA:%2BGrasping%2BFlow%2Bin%2BHistory%2Bfor%2BConversational%2BMachine%2BComprehension%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=m9d10YA4tLQJ&ei=94gqX_ubB8mdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:m9d10YA4tLQJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqLVuHzBjTB3-D8HX7P9jRK_VGTHeP9&scisig=AAGBfm0AAAAAXyqLVvvRrRLbwU9CUk53eKT3-LY8S_3L&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
36
-------------------------------------------------
2020-08-05 10:25:03
Got the results of the query
{'bib': {'abstract': 'Designing RNA molecules has garnered recent interest in '
                     'medicine, synthetic biology, biotechnology and '
                     'bioinformatics since many functional RNA molecules were '
                     'shown to be involved in regulatory processes for '
                     'transcription, epigenetics and translation. Since an',
         'author': ['F Runge', 'D Stoll', 'S Falkner', 'F Hutter'],
         'cites': '8',
         'eprint': 'https://arxiv.org/pdf/1812.11951',
         'gsrank': '1',
         'title': 'Learning to design RNA',
         'url': 'https://arxiv.org/abs/1812.11951',
         'venue': 'arXiv preprint arXiv:1812.11951',
         'year': '2018'},
 'citations_link': '/scholar?cites=17240520904353756155&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BDesign%2BRNA%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=-7OzFmuiQu8J&ei=CIkqX778OIS0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:-7OzFmuiQu8J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqLaC440yQpAbLEI-NKnmm4dMslbmF9&scisig=AAGBfm0AAAAAXyqLaONdT1mUegFF7Rz8IonwomIorxOZ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
8
-------------------------------------------------
2020-08-05 10:25:20
Got the results of the query
{'bib': {'abstract': 'Improving speech system performance in noisy '
                     'environments remains a challenging task, and speech '
                     'enhancement (SE) is one of the effective techniques to '
                     'solve the problem. Motivated by the promising results of '
                     'generative adversarial networks (GANs) in a variety of',
         'author': ['D Michelsanti', 'ZH Tan'],
         'cites': '120',
         'eprint': 'https://arxiv.org/pdf/1709.01703',
         'gsrank': '1',
         'title': 'Conditional generative adversarial networks for speech '
                  'enhancement and noise-robust speaker verification',
         'url': 'https://arxiv.org/abs/1709.01703',
         'venue': 'arXiv preprint arXiv:1709.01703',
         'year': '2017'},
 'citations_link': '/scholar?cites=11550830663320501930&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRobust%2BConditional%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=qn7oE0rLTKAJ&ei=FIkqX4CeGMmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:qn7oE0rLTKAJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqLgPF23828R5zyTLAKsTlpQpxyLMUk&scisig=AAGBfm0AAAAAXyqLgE3I5-zR8YnfRdkt3DtmtV5byDIF&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
120
-------------------------------------------------
2020-08-05 10:25:44
Got the results of the query
{'bib': {'abstract': 'We present a simple neural model that given a formula '
                     'and a property tries to answer the question whether the '
                     'formula has the given property, for example whether a '
                     'propositional formula is always true. The structure of '
                     'the formula is captured by a feedforward neural',
         'author': ['K Chvalovský'],
         'cites': '8',
         'eprint': 'https://openreview.net/pdf?id=Byg5QhR5FQ',
         'gsrank': '1',
         'title': 'Top-down neural model for formulae',
         'url': 'https://openreview.net/forum?id=Byg5QhR5FQ',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=5073436818073510595&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTop-Down%2BNeural%2BModel%2BFor%2BFormulae%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=w-Ko4-d3aEYJ&ei=MYkqX8C0GIbuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:w-Ko4-d3aEYJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqLkWB9h9Fty4lfs4zrrkOiaITVs7JH&scisig=AAGBfm0AAAAAXyqLkQvJHgzhmUd9hbtR9QhzW_CroWc8&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
8
-------------------------------------------------
2020-08-05 10:26:01
Got the results of the query
{'bib': {'abstract': 'Several recent works have developed methods for training '
                     'classifiers that are certifiably robust against '
                     'norm-bounded adversarial perturbations. These methods '
                     'assume that all the adversarial transformations are '
                     'equally important, which is seldom the case in '
                     'real-world applications. We advocate for cost-sensitive '
                     'robustness as the criteria for measuring the '
                     "classifier's performance for tasks where some "
                     'adversarial transformation are more important than '
                     'others. We encode the potential harm of each adversarial '
                     'transformation in a cost',
         'author': ['X Zhang', 'D Evans'],
         'cites': '4',
         'eprint': 'https://arxiv.org/pdf/1810.09225',
         'gsrank': '1',
         'title': 'Cost-sensitive robustness against adversarial examples',
         'url': 'https://arxiv.org/abs/1810.09225',
         'venue': 'arXiv preprint arXiv:1810.09225',
         'year': '2018'},
 'citations_link': '/scholar?cites=16169861265468560490&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCost-Sensitive%2BRobustness%2Bagainst%2BAdversarial%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ahSaOybjZuAJ&ei=P4kqX6-wLpmG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ahSaOybjZuAJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqLolB2sTETQqvLxvLOqvWv3WGcPvxP&scisig=AAGBfm0AAAAAXyqLoqJ3r_nVFeL3uESMwjmaVFJahzCi&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
4
-------------------------------------------------
2020-08-05 10:26:19
Got the results of the query
{'bib': {'abstract': 'Despite existing work on ensuring generalization of '
                     'neural networks in terms of scale sensitive complexity '
                     'measures, such as norms, margin and sharpness, these '
                     'complexity measures do not offer an explanation of why '
                     'neural networks generalize better with '
                     'over-parametrization. In this work we suggest a novel '
                     'complexity measure based on unit-wise capacities '
                     'resulting in a tighter generalization bound for two '
                     'layer ReLU networks. Our capacity bound correlates with '
                     'the behavior of test error with increasing network sizes',
         'author': ['B Neyshabur', 'Z Li', 'S Bhojanapalli'],
         'cites': '53',
         'eprint': 'https://openreview.net/pdf?id=BygfghAcYX',
         'gsrank': '1',
         'title': 'The role of over-parametrization in generalization of '
                  'neural networks',
         'url': 'https://openreview.net/forum?id=BygfghAcYX',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=13985648631478953230&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2Brole%2Bof%2Bover-parametrization%2Bin%2Bgeneralization%2Bof%2Bneural%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=DiW7yFQBF8IJ&ei=VYkqX6i_BIbuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:DiW7yFQBF8IJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqLsaol1mko44lSCw9Ne2Sw4ufEjn8x&scisig=AAGBfm0AAAAAXyqLsbQoutRHDKQtCxn-2RALrp-T2PCu&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
53
-------------------------------------------------
2020-08-05 10:26:34
Got the results of the query
{'bib': {'abstract': 'Stability is a key aspect of data analysis. In many '
                     'applications, the natural notion of stability is '
                     'geometric, as illustrated for example in computer '
                     'vision. Scattering transforms construct deep '
                     'convolutional representations which are certified stable '
                     'to input deformations. This stability to deformations '
                     'can be interpreted as stability with respect to changes '
                     'in the metric structure of the domain. In this work, we '
                     'show that scattering transforms can be generalized to '
                     'non-Euclidean domains using diffusion wavelets, while '
                     'preserving a notion of stability',
         'author': ['F Gama', 'A Ribeiro', 'J Bruna'],
         'cites': '25',
         'eprint': 'https://arxiv.org/pdf/1806.08829',
         'gsrank': '1',
         'title': 'Diffusion scattering transforms on graphs',
         'url': 'https://arxiv.org/abs/1806.08829',
         'venue': 'arXiv preprint arXiv:1806.08829',
         'year': '2018'},
 'citations_link': '/scholar?cites=7856126226724225104&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiffusion%2BScattering%2BTransforms%2Bon%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=UACH_i2RBm0J&ei=YYkqX5bvOcyXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:UACH_i2RBm0J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqLy_c5o7vxctP7w0TkpVgYKosoeGH5&scisig=AAGBfm0AAAAAXyqLy3PYTKtXXgfMQ7Seqd248bMzg79I&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
25
-------------------------------------------------
2020-08-05 10:26:59
Got the results of the query
{'bib': {'abstract': 'The high-quality node embeddings learned from the Graph '
                     'Neural Networks (GNNs) have been applied to a wide range '
                     'of node-based applications and some of them have '
                     'achieved state-of-the-art (SOTA) performance. However, '
                     'when applying node embeddings learned',
         'author': ['Z Xinyi', 'L Chen'],
         'cites': '46',
         'eprint': 'https://openreview.net/pdf?id=Byl8BnRcYm',
         'gsrank': '1',
         'title': 'Capsule graph neural network',
         'url': 'https://openreview.net/forum?id=Byl8BnRcYm&noteId=ByxrZRED3m',
         'venue': 'International conference on learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=4030715970262857024&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCapsule%2BGraph%2BNeural%2BNetwork%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=QPUCvtH67zcJ&ei=e4kqX9HFJ8yXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:QPUCvtH67zcJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqL28i6iD5Np-HOXhAJOCueoppxKdlj&scisig=AAGBfm0AAAAAXyqL26T2225zvFFdlmNaBtpL0sybuhid&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
46
-------------------------------------------------
2020-08-05 10:27:15
Got the results of the query
{'bib': {'abstract': 'Deep Neural Networks (DNNs) are increasingly deployed in '
                     'highly energy-constrained environments such as '
                     'autonomous drones and wearable devices while at the same '
                     'time must operate in real-time. Therefore, reducing the '
                     'energy consumption has become a major design '
                     'consideration in DNN training. This paper proposes the '
                     'first end-to-end DNN training framework that provides '
                     'quantitative energy consumption guarantees via weighted '
                     'sparse projection and input masking. The key idea is to '
                     'formulate the DNN training as an',
         'author': ['H Yang', 'Y Zhu', 'J Liu'],
         'cites': '11',
         'eprint': 'https://arxiv.org/pdf/1806.04321',
         'gsrank': '1',
         'title': 'Energy-constrained compression for deep neural networks via '
                  'weighted sparse projection and layer input masking',
         'url': 'https://arxiv.org/abs/1806.04321',
         'venue': 'arXiv preprint arXiv:1806.04321',
         'year': '2018'},
 'citations_link': '/scholar?cites=6237094978821638350&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnergy-Constrained%2BCompression%2Bfor%2BDeep%2BNeural%2BNetworks%2Bvia%2BWeighted%2BSparse%2BProjection%2Band%2BLayer%2BInput%2BMasking%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=zkgJc9acjlYJ&ei=i4kqX8zKOsmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:zkgJc9acjlYJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqL6_QaYHh34GFELpZyp68vVYmQKLta&scisig=AAGBfm0AAAAAXyqL61WeLc2mJ0Vi8NSdX94Wq9FNLpDZ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
11
-------------------------------------------------
2020-08-05 10:27:31
Got the results of the query
{'bib': {'abstract': 'We study the problem of learning to map, in an '
                     'unsupervised way, between domains A and B, such that the '
                     'samples b in B contain all the information that exists '
                     'in samples a in A and some additional information. For '
                     'example, ignoring occlusions, B can be people with '
                     'glasses, A people without, and the glasses, would be the '
                     'added information. When mapping a sample a from the '
                     'first domain to the other domain, the missing '
                     'information is replicated from an independent reference '
                     'sample b in B. Thus, in the above example, we can create',
         'author': ['O Press', 'T Galanti', 'S Benaim', 'L Wolf'],
         'cites': '7',
         'eprint': 'https://arxiv.org/pdf/2001.05017',
         'gsrank': '1',
         'title': 'Emerging disentanglement in auto-encoder based unsupervised '
                  'image content transfer',
         'url': 'https://arxiv.org/abs/2001.05017',
         'venue': 'arXiv preprint arXiv:2001.05017',
         'year': '2020'},
 'citations_link': '/scholar?cites=1130493139270124070&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmerging%2BDisentanglement%2Bin%2BAuto-Encoder%2BBased%2BUnsupervised%2BImage%2BContent%2BTransfer%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=JkI2Q4VRsA8J&ei=nYkqX4SlBsmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:JkI2Q4VRsA8J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqL_WsHhuDTKyEbNNBR6r5_Me9-FYZQ&scisig=AAGBfm0AAAAAXyqL_Va-oTOr-mOquK-l20M7TNN_ZxGZ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
7
-------------------------------------------------
2020-08-05 10:27:49
Got the results of the query
{'bib': {'abstract': 'Stochastic gradient descent (SGD) has been found to be '
                     'surprisingly effective in training a variety of deep '
                     'neural networks. However, there is still a lack of '
                     'understanding on how and why SGD can train these complex '
                     'networks towards a global minimum. In this study, we '
                     'establish the convergence of SGD to a global minimum for '
                     'nonconvex optimization problems that are commonly '
                     'encountered in neural network training. Our argument '
                     'exploits the following two important properties: 1) the '
                     'training loss can achieve zero value',
         'author': ['Y Zhou', 'J Yang', 'H Zhang', 'Y Liang', 'V Tarokh'],
         'cites': '17',
         'eprint': 'https://arxiv.org/pdf/1901.00451',
         'gsrank': '1',
         'title': 'SGD converges to global minimum in deep learning via '
                  'star-convex path',
         'url': 'https://arxiv.org/abs/1901.00451',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=12174745207770753941&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSGD%2BConverges%2Bto%2BGlobal%2BMinimum%2Bin%2BDeep%2BLearning%2Bvia%2BStar-convex%2BPath%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=lasQDENi9agJ&ei=s4kqX6CxBMmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:lasQDENi9agJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqMFLm-U-9TitqB4GWhxkYkbeXT9rQj&scisig=AAGBfm0AAAAAXyqMFIoT8b4rG_MaonsFyyKZdLFyU_lM&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
17
-------------------------------------------------
2020-08-05 10:28:12
Got the results of the query
{'bib': {'abstract': 'Many distributed machine learning (ML) systems adopt the '
                     'non-synchronous execution in order to alleviate the '
                     'network communication bottleneck, resulting in stale '
                     'parameters that do not reflect the latest updates. '
                     'Despite much development in large-scale ML, the effects '
                     'of staleness on learning are inconclusive as it is '
                     'challenging to directly monitor or control staleness in '
                     'complex distributed environments. In this work, we study '
                     'the convergence behaviors of a wide array of ML models '
                     'and algorithms under delayed updates. Our',
         'author': ['W Dai', 'Y Zhou', 'N Dong', 'H Zhang', 'EP Xing'],
         'cites': '8',
         'eprint': 'https://arxiv.org/pdf/1810.03264',
         'gsrank': '1',
         'title': 'Toward understanding the impact of staleness in distributed '
                  'machine learning',
         'url': 'https://arxiv.org/abs/1810.03264',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=14283113129888033216&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DToward%2BUnderstanding%2Bthe%2BImpact%2Bof%2BStaleness%2Bin%2BDistributed%2BMachine%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=wKmxabfPN8YJ&ei=xIkqX4DBJ4S0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:wKmxabfPN8YJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqMI8A3xYxwf_P1PGo5LQUuPf200CVZ&scisig=AAGBfm0AAAAAXyqMIziR4yYu0KOdBwLahbHmw6Tb3XPX&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
8
-------------------------------------------------
2020-08-05 10:28:27
Got the results of the query
{'bib': {'abstract': 'Transfer learning aims to solve the data sparsity for a '
                     'target domain by applying information of the source '
                     'domain. Given a sequence (eg a natural language '
                     'sentence), the transfer learning, usually enabled by '
                     'recurrent neural network (RNN), represents the '
                     'sequential information transfer. RNN uses a chain of '
                     'repeating cells to model the sequence data. However, '
                     'previous studies of neural network based transfer '
                     'learning simply represents the whole sentence by a '
                     'single vector, which is unfeasible for seq2seq and '
                     'sequence labeling',
         'author': ['W Cui', 'G Zheng', 'Z Shen', 'S Jiang', 'W Wang'],
         'cites': '7',
         'eprint': 'https://arxiv.org/pdf/1902.09092',
         'gsrank': '1',
         'title': 'Transfer learning for sequences via learning to collocate',
         'url': 'https://arxiv.org/abs/1902.09092',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=9924239763188031121&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTransfer%2BLearning%2Bfor%2BSequences%2Bvia%2BLearning%2Bto%2BCollocate%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=kR6h1nv7uYkJ&ei=04kqX4jHBcyXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:kR6h1nv7uYkJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqMM3ZOsr_0wFJraB3pu7djQLuNQoEm&scisig=AAGBfm0AAAAAXyqMM0eYMD1fJq5Ptg6e-zz7dZxOuMTo&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
7
-------------------------------------------------
2020-08-05 10:28:43
Got the results of the query
{'bib': {'abstract': 'Clustering methods and latent variable models are often '
                     'used as tools for pattern mining and discovery of latent '
                     'structure in time-series data. In this work, we consider '
                     'the problem of learning procedural abstractions from '
                     'possibly high-dimensional observational sequences, such '
                     'as video demonstrations. Given a dataset of time-series, '
                     'the goal is to identify the latent sequence of steps '
                     'common to them and label each time-series with the '
                     'temporal extent of these procedural steps. We introduce '
                     'a hierarchical Bayesian model called Prism that',
         'author': ['K Goel', 'E Brunskill'],
         'cites': '0',
         'eprint': 'https://openreview.net/pdf?id=ByleB2CcKm',
         'gsrank': '1',
         'title': 'Learning Procedural Abstractions and Evaluating Discrete '
                  'Latent Temporal Structure',
         'url': 'https://openreview.net/forum?id=ByleB2CcKm',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BProcedural%2BAbstractions%2Band%2BEvaluating%2BDiscrete%2BLatent%2BTemporal%2BStructure%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=c11fSorzhkIJ&ei=4YkqX8S5Np32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:c11fSorzhkIJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqMQ97WdVqEiPONlf1p5rj-VRDZnu3l&scisig=AAGBfm0AAAAAXyqMQ_yFgoEwZdfT37AmtuhizhYqUiYN&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
0
-------------------------------------------------
2020-08-05 10:28:59
Got the results of the query
{'bib': {'abstract': 'We consider the problem of training speech recognition '
                     'systems without using any labeled data, under the '
                     'assumption that the learner can only access to the input '
                     'utterances and a phoneme language model estimated from a '
                     'non-overlapping corpus. We propose a fully unsupervised '
                     'learning algorithm that alternates between solving two '
                     'sub-problems:(i) learn a phoneme classifier for a given '
                     'set of phoneme segmentation boundaries, and (ii) '
                     'refining the phoneme boundaries based on a given '
                     'classifier. To solve the first sub-problem, we',
         'author': ['CK Yeh', 'J Chen', 'C Yu', 'D Yu'],
         'cites': '5',
         'eprint': 'https://arxiv.org/pdf/1812.09323',
         'gsrank': '1',
         'title': 'Unsupervised speech recognition via segmental empirical '
                  'output distribution matching',
         'url': 'https://arxiv.org/abs/1812.09323',
         'venue': 'arXiv preprint arXiv:1812.09323',
         'year': '2018'},
 'citations_link': '/scholar?cites=4232653024484173716&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BSpeech%2BRecognition%2Bvia%2BSegmental%2BEmpirical%2BOutput%2BDistribution%2BMatching%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=lKfcDYBnvToJ&ei=8okqX6efHZ32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:lKfcDYBnvToJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqMT3caGNi2II7edcN32nU-qDyE5Rjc&scisig=AAGBfm0AAAAAXyqMTwFXT5htyKPrETYfrdF5pnmggTsq&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 10:29:11
Got the results of the query
{'bib': {'abstract': 'Deep learning models for graphs have advanced the state '
                     'of the art on many tasks. Despite their recent success, '
                     'little is known about their robustness. We investigate '
                     'training time attacks on graph neural networks for node '
                     'classification that perturb the discrete graph '
                     'structure. Our core principle is to use meta-gradients '
                     'to solve the bilevel problem underlying training-time '
                     'attacks, essentially treating the graph as a '
                     'hyperparameter to optimize. Our experiments show that '
                     'small graph perturbations consistently lead to a strong '
                     'decrease in',
         'author': ['D Zügner', 'S Günnemann'],
         'cites': '61',
         'eprint': 'https://arxiv.org/pdf/1902.08412',
         'gsrank': '1',
         'title': 'Adversarial attacks on graph neural networks via meta '
                  'learning',
         'url': 'https://arxiv.org/abs/1902.08412',
         'venue': 'arXiv preprint arXiv:1902.08412',
         'year': '2019'},
 'citations_link': '/scholar?cites=2520072804439946279&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BAttacks%2Bon%2BGraph%2BNeural%2BNetworks%2Bvia%2BMeta%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=J_TDHN0Y-SIJ&ei=AIoqX-QSgpPKBPfDjPAK',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:J_TDHN0Y-SIJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqMYEfCctQA6lmb1JGYycPhJJBPOFb_&scisig=AAGBfm0AAAAAXyqMYNFlJA6GcwgMRsWkVamuOPh8FpKL&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
61
-------------------------------------------------
2020-08-05 10:29:29
Got the results of the query
{'bib': {'abstract': 'Due to the sharp increase in the severity of the threat '
                     'imposed by software vulnerabilities, the detection of '
                     'vulnerabilities in binary code has become an important '
                     'concern in the software industry, such as the embedded '
                     'systems industry, and in the field of computer security. '
                     'However, most of the work in binary code vulnerability '
                     'detection has relied on handcrafted features which are '
                     'manually chosen by a select few, knowledgeable domain '
                     'experts. In this paper, we attempt to alleviate this '
                     'severe binary vulnerability detection',
         'author': ['T Le', 'T Nguyen', 'T Le', 'D Phung'],
         'cites': '6',
         'eprint': 'https://openreview.net/pdf?id=ByloIiCqYQ',
         'gsrank': '1',
         'title': 'Maximal divergence sequential autoencoder for binary '
                  'software vulnerability detection',
         'url': 'https://openreview.net/forum?id=ByloIiCqYQ',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11843178707743897449&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMaximal%2BDivergence%2BSequential%2BAutoencoder%2Bfor%2BBinary%2BSoftware%2BVulnerability%2BDetection%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=aXeLbklsW6QJ&ei=D4oqX62oPM2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:aXeLbklsW6QJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqMcX_8Q0KoyWYhxvBhLmErb3-DRe-O&scisig=AAGBfm0AAAAAXyqMcXyb4No3PuouiI-18vf0tBfS1YoT&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 10:29:46
Got the results of the query
{'bib': {'abstract': 'Due to its potential to improve programmer productivity '
                     'and software quality, automated program repair has been '
                     'an active topic of research. Newer techniques harness '
                     'neural networks to learn directly from examples of buggy '
                     'programs and their fixes. In this work, we consider a '
                     'recently identified class of bugs called variable-misuse '
                     'bugs. The state-of-the-art solution for variable misuse '
                     'enumerates potential fixes for all possible bug '
                     'locations in a program, before selecting the best '
                     'prediction. We show that it is beneficial to train a '
                     'model',
         'author': ['M Vasic', 'A Kanade', 'P Maniatis', 'D Bieber'],
         'cites': '24',
         'eprint': 'https://arxiv.org/pdf/1904.01720',
         'gsrank': '1',
         'title': 'Neural program repair by jointly learning to localize and '
                  'repair',
         'url': 'https://arxiv.org/abs/1904.01720',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=2606729175407927305&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BProgram%2BRepair%2Bby%2BJointly%2BLearning%2Bto%2BLocalize%2Band%2BRepair%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=CSDWvV_2LCQJ&ei=H4oqX7HzGoKTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:CSDWvV_2LCQJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqMgDM4OxqrVba-mAeRyQDO7ON4lsTj&scisig=AAGBfm0AAAAAXyqMgPxubnv1igFAkl462TxiwufVtCtj&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
24
-------------------------------------------------
2020-08-05 10:30:00
Got the results of the query
{'bib': {'abstract': 'Efficient exploration remains a major challenge for '
                     'reinforcement learning. One reason is that the '
                     'variability of the returns often depends on the current '
                     'state and action, and is therefore heteroscedastic. '
                     'Classical exploration strategies such as upper '
                     'confidence bound algorithms and Thompson sampling fail '
                     'to appropriately account for heteroscedasticity, even in '
                     'the bandit setting. Motivated by recent findings that '
                     'address this issue in bandits, we propose to use '
                     'Information-Directed Sampling (IDS) for exploration in '
                     'reinforcement',
         'author': ['N Nikolov', 'J Kirschner', 'F Berkenkamp'],
         'cites': '12',
         'eprint': 'https://arxiv.org/pdf/1812.07544',
         'gsrank': '1',
         'title': 'Information-directed exploration for deep reinforcement '
                  'learning',
         'url': 'https://arxiv.org/abs/1812.07544',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=12419979613667846761&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInformation-Directed%2BExploration%2Bfor%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=aepDnqehXKwJ&ei=LYoqX8nELMyXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:aepDnqehXKwJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqMjabXPj976ln34aJeLvF2t46sOdBO&scisig=AAGBfm0AAAAAXyqMjc0AyAuTHINGDKm4NSX4q7bmeWku&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 10:30:13
Got the results of the query
{'bib': {'abstract': 'The recently presented idea to learn heuristics for '
                     'combinatorial optimization problems is promising as it '
                     'can save costly development. However, to push this idea '
                     'towards practical implementation, we need better models '
                     'and better ways of training. We contribute in both '
                     'directions: we propose a model based on attention layers '
                     'with benefits over the Pointer Network and we show how '
                     'to train this model using REINFORCE with a simple '
                     'baseline based on a deterministic greedy rollout, which '
                     'we find is more efficient than using a value',
         'author': ['W Kool', 'H Van Hoof', 'M Welling'],
         'cites': '113',
         'eprint': 'https://arxiv.org/pdf/1803.08475.pdf?source=post_page---------------------------',
         'gsrank': '1',
         'title': 'Attention, learn to solve routing problems!',
         'url': 'https://arxiv.org/abs/1803.08475',
         'venue': 'arXiv preprint arXiv:1803.08475',
         'year': '2018'},
 'citations_link': '/scholar?cites=14639976201161443491&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAttention,%2BLearn%2Bto%2BSolve%2BRouting%2BProblems%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=o4hm_smkK8sJ&ei=PYoqX_PBDs2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:o4hm_smkK8sJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqMnACeEjawVRPaRT9-PlMLjOvaVVty&scisig=AAGBfm0AAAAAXyqMnDFUpOAIrcHztVnCY-wTPwJcAedb&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
113
-------------------------------------------------
2020-08-05 10:30:29
Got the results of the query
{'bib': {'abstract': 'This paper proposes a class of well-conditioned neural '
                     'networks in which a unit amount of change in the inputs '
                     'causes at most a unit amount of change in the outputs or '
                     'any of the internal layers. We develop the known '
                     'methodology of controlling Lipschitz constants to',
         'author': ['H Qian', 'MN Wegman'],
         'cites': '30',
         'eprint': 'https://arxiv.org/pdf/1802.07896',
         'gsrank': '1',
         'title': 'L2-nonexpansive neural networks',
         'url': 'https://arxiv.org/abs/1802.07896',
         'venue': 'arXiv preprint arXiv:1802.07896',
         'year': '2018'},
 'citations_link': '/scholar?cites=6491454663849798567&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DL2-Nonexpansive%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=p-tRop9HFloJ&ei=SooqX9v-J82iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:p-tRop9HFloJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqMqftTQge5RxU9EdOBNjGLxdjY7auB&scisig=AAGBfm0AAAAAXyqMqahJMWGEriYY-ErOzkjNbLNXe3--&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
30
-------------------------------------------------
2020-08-05 10:30:42
Got the results of the query
{'bib': {'abstract': 'Generative Adversarial Networks (GANs) are one of the '
                     'most popular tools for learning complex high dimensional '
                     'distributions. However, generalization properties of '
                     'GANs have not been well understood. In this paper, we '
                     'analyze the generalization of GANs in practical '
                     'settings. We show that discriminators trained on '
                     'discrete datasets with the original GAN loss have poor '
                     'generalization capability and do not approximate the '
                     'theoretically optimal discriminator. We propose a '
                     'zero-centered gradient penalty for improving the '
                     'generalization',
         'author': ['H Thanh-Tung', 'T Tran', 'S Venkatesh'],
         'cites': '37',
         'eprint': 'https://arxiv.org/pdf/1902.03984',
         'gsrank': '1',
         'title': 'Improving generalization and stability of generative '
                  'adversarial networks',
         'url': 'https://arxiv.org/abs/1902.03984',
         'venue': 'arXiv preprint arXiv:1902.03984',
         'year': '2019'},
 'citations_link': '/scholar?cites=13499019185526283919&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BGeneralization%2Band%2BStability%2Bof%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=jx56qW0mVrsJ&ei=WYoqX7KsEoS0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:jx56qW0mVrsJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqMurF90_HfR2OsDlUmgT5T6kmD1rj4&scisig=AAGBfm0AAAAAXyqMugmdkC6yeFx13MTGsF9I_oW72ke4&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
37
-------------------------------------------------
2020-08-05 10:30:58
Got the results of the query
{'bib': {'abstract': 'We introduce adaptive input representations for neural '
                     'language modeling which extend the adaptive softmax of '
                     'Grave et al.(2017) to input representations of variable '
                     'capacity. There are several choices on how to factorize '
                     'the input and output layers, and whether to model words, '
                     'characters or sub-word units. We perform a systematic '
                     'comparison of popular choices for a self-attentional '
                     'architecture. Our experiments show that models equipped '
                     'with adaptive embeddings are more than twice as fast to '
                     'train than the popular character input',
         'author': ['A Baevski', 'M Auli'],
         'cites': '81',
         'eprint': 'https://arxiv.org/pdf/1809.10853',
         'gsrank': '1',
         'title': 'Adaptive input representations for neural language modeling',
         'url': 'https://arxiv.org/abs/1809.10853',
         'venue': 'arXiv preprint arXiv:1809.10853',
         'year': '2018'},
 'citations_link': '/scholar?cites=9932684582274973195&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaptive%2BInput%2BRepresentations%2Bfor%2BNeural%2BLanguage%2BModeling%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=C75_gAD814kJ&ei=aooqX4uZOMyXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:C75_gAD814kJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqMzNVd0vscjZLKh5JCfeK0gmd7OJnz&scisig=AAGBfm0AAAAAXyqMzGOIACyjLOTA-gIZiDtXImYCf09w&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
81
-------------------------------------------------
2020-08-05 10:31:17
Got the results of the query
{'bib': {'abstract': 'While many approaches to make neural networks more '
                     'fathomable have been proposed, they are restricted to '
                     'interrogating the network with input data. Measures for '
                     'characterizing and monitoring structural properties, '
                     'however, have not been developed. In this work, we '
                     'propose neural persistence, a complexity measure for '
                     'neural network architectures based on topological data '
                     'analysis on weighted stratified graphs. To demonstrate '
                     'the usefulness of our approach, we show that neural '
                     'persistence reflects best practices developed in the '
                     'deep',
         'author': ['B Rieck', 'M Togninalli', 'C Bock', 'M Moor', 'M Horn'],
         'cites': '19',
         'eprint': 'https://arxiv.org/pdf/1812.09764',
         'gsrank': '1',
         'title': 'Neural persistence: A complexity measure for deep neural '
                  'networks using algebraic topology',
         'url': 'https://arxiv.org/abs/1812.09764',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=12286997751595249495&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BPersistence:%2BA%2BComplexity%2BMeasure%2Bfor%2BDeep%2BNeural%2BNetworks%2BUsing%2BAlgebraic%2BTopology%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=V6_bE1svhKoJ&ei=gYoqX7aOOpmG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:V6_bE1svhKoJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqM6ddOcwLHyepTceOKt0bhCRjp0rma&scisig=AAGBfm0AAAAAXyqM6dyMKPIKQQXxNdXAwo81XdDfD9Dk&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
19
-------------------------------------------------
2020-08-05 10:31:51
Got the results of the query
{'bib': {'abstract': 'Data augmentation is commonly used to encode invariances '
                     'in learning methods. However, this process is often '
                     'performed in an inefficient manner, as artificial '
                     'examples are created by applying a number of '
                     'transformations to all points in the training set. The '
                     'resulting explosion of the dataset size can be an issue '
                     'in terms of storage and training costs, as well as in '
                     'selecting and tuning the optimal set of transformations '
                     'to apply. In this work, we demonstrate that it is '
                     'possible to significantly reduce the number of data '
                     'points included in data',
         'author': ['M Kuchnik', 'V Smith'],
         'cites': '8',
         'eprint': 'https://arxiv.org/pdf/1810.05222',
         'gsrank': '1',
         'title': 'Efficient augmentation via data subsampling',
         'url': 'https://arxiv.org/abs/1810.05222',
         'venue': 'arXiv preprint arXiv:1810.05222',
         'year': '2018'},
 'citations_link': '/scholar?cites=12087290703747116451&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BAugmentation%2Bvia%2BData%2BSubsampling%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=o226ldquvqcJ&ei=n4oqX67fKJ32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:o226ldquvqcJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqNANbXJ47Y9dl0iczClGs5ZVAu2rOs&scisig=AAGBfm0AAAAAXyqNAN14jmvlpjwVd4A-w7ShvX-1BIqw&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
8
-------------------------------------------------
2020-08-05 10:32:08
Got the results of the query
{'bib': {'abstract': 'The modeling of style when synthesizing natural human '
                     'speech from text has been the focus of significant '
                     'attention. Some state-of-the-art approaches train an '
                     'encoder-decoder network on paired text and audio samples '
                     '(x_txt, x_aud) by encouraging its output to reconstruct '
                     'x_aud. The synthesized audio waveform is expected to '
                     'contain the verbal content of x_txt and the auditory '
                     'style of x_aud. Unfortunately, modeling style in TTS is '
                     'somewhat under-determined and training models with a '
                     'reconstruction loss alone is insufficient to disentangle',
         'author': ['S Ma', 'D Mcduff', 'Y Song'],
         'cites': '3',
         'eprint': 'https://openreview.net/pdf?id=ByzcS3AcYX',
         'gsrank': '1',
         'title': 'Neural TTS stylization with adversarial and collaborative '
                  'games',
         'url': 'https://openreview.net/forum?id=ByzcS3AcYX',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=14283708523200938731&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BTTS%2BStylization%2Bwith%2BAdversarial%2Band%2BCollaborative%2BGames%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=64KcODntOcYJ&ei=sIoqX__ECobuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:64KcODntOcYJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqNEJWohE47lvLFcHLF31uNNYvrWxj1&scisig=AAGBfm0AAAAAXyqNEP-jrpfifV_AYKIWptTBC9Y7eVb_&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
3
-------------------------------------------------
2020-08-05 10:32:25
Got the results of the query
{'bib': {'abstract': 'Control of complex systems involves both system '
                     'identification and controller design. Deep neural '
                     'networks have proven to be successful in many '
                     'identification tasks, however, from model-based control '
                     'perspective, these networks are difficult to work with '
                     'because they are typically nonlinear and nonconvex. '
                     'Therefore many systems are still identified and '
                     'controlled based on simple linear models despite their '
                     'poor representation capability. In this paper we bridge '
                     'the gap between model accuracy and control tractability '
                     'faced by neural',
         'author': ['Y Chen', 'Y Shi', 'B Zhang'],
         'cites': '18',
         'eprint': 'https://arxiv.org/pdf/1805.11835',
         'gsrank': '1',
         'title': 'Optimal control via neural networks: A convex approach',
         'url': 'https://arxiv.org/abs/1805.11835',
         'venue': 'arXiv preprint arXiv:1805.11835',
         'year': '2018'},
 'citations_link': '/scholar?cites=1075593248050773588&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOptimal%2BControl%2BVia%2BNeural%2BNetworks:%2BA%2BConvex%2BApproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=VP7fQFxG7Q4J&ei=wIoqX9HTEZ32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:VP7fQFxG7Q4J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqNJeVKNbfvw6DhPZoc4SzBsuInIu-K&scisig=AAGBfm0AAAAAXyqNJfYTky4rOLTqm0Znof0jp7TbQ1hC&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
18
-------------------------------------------------
2020-08-05 10:32:46
Got the results of the query
{'bib': {'abstract': 'Continuous Bag of Words (CBOW) is a powerful text '
                     'embedding method. Due to its strong capabilities to '
                     'encode word content, CBOW embeddings perform well on a '
                     'wide range of downstream tasks while being efficient to '
                     'compute. However, CBOW is not capable of capturing the '
                     "word order. The reason is that the computation of CBOW's "
                     'word embeddings is commutative, ie, embeddings of XYZ '
                     'and ZYX are the same. In order to address this '
                     'shortcoming, we propose a learning algorithm for the '
                     'Continuous Matrix Space Model',
         'author': ['F Mai', 'L Galke', 'A Scherp'],
         'cites': '0',
         'eprint': 'https://arxiv.org/pdf/1902.06423',
         'gsrank': '1',
         'title': 'CBOW Is Not All You Need: Combining CBOW with the '
                  'Compositional Matrix Space Model',
         'url': 'https://arxiv.org/abs/1902.06423',
         'venue': 'arXiv preprint arXiv:1902.06423',
         'year': '2019'},
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCBOW%2BIs%2BNot%2BAll%2BYou%2BNeed:%2BCombining%2BCBOW%2Bwith%2Bthe%2BCompositional%2BMatrix%2BSpace%2BModel%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=DsKBt7MRzVMJ&ei=0ooqX5X1LIKTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:DsKBt7MRzVMJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqNM5SuS-riYiN6heL6HAMcG42reitW&scisig=AAGBfm0AAAAAXyqNM3mTQULEtaOvkUmA7FSwSaT792bs&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
0
-------------------------------------------------
2020-08-05 10:32:59
Got the results of the query
{'bib': {'abstract': 'Sorting input objects is an important step in many '
                     'machine learning pipelines. However, the sorting '
                     'operator is non-differentiable with respect to its '
                     'inputs, which prohibits end-to-end gradient-based '
                     'optimization. In this work, we propose NeuralSort, a '
                     'general-purpose continuous relaxation of the output of '
                     'the sorting operator from permutation matrices to the '
                     'set of unimodal row-stochastic matrices, where every row '
                     'sums to one and has a distinct arg max. This relaxation '
                     'permits straight-through optimization of any '
                     'computational graph',
         'author': ['A Grover', 'E Wang', 'A Zweig', 'S Ermon'],
         'cites': '20',
         'eprint': 'https://arxiv.org/pdf/1903.08850',
         'gsrank': '1',
         'title': 'Stochastic optimization of sorting networks via continuous '
                  'relaxations',
         'url': 'https://arxiv.org/abs/1903.08850',
         'venue': 'arXiv preprint arXiv:1903.08850',
         'year': '2019'},
 'citations_link': '/scholar?cites=10619362619006891050&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStochastic%2BOptimization%2Bof%2BSorting%2BNetworks%2Bvia%2BContinuous%2BRelaxations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=KrT3xgmOX5MJ&ei=44oqX8_oO4S0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:KrT3xgmOX5MJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqNQjlkUXRHyNYocK_Xjf7hL6zAqdE_&scisig=AAGBfm0AAAAAXyqNQiPiNE-SMHwS1W42HQpRh4lJPdTe&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
20
-------------------------------------------------
2020-08-05 10:33:15
Got the results of the query
{'bib': {'abstract': 'Deep learning has shown high performances in various '
                     'types of tasks from visual recognition to natural '
                     'language processing, which indicates superior '
                     'flexibility and adaptivity of deep learning. To '
                     'understand this phenomenon theoretically, we develop a '
                     'new approximation and estimation error analysis of deep '
                     'learning with the ReLU activation for functions in a '
                     'Besov space and its variant with mixed smoothness. The '
                     'Besov space is a considerably general function space '
                     'including the Holder space and Sobolev space, and',
         'author': ['T Suzuki'],
         'cites': '40',
         'eprint': 'https://arxiv.org/pdf/1810.08033',
         'gsrank': '1',
         'title': 'Adaptivity of deep relu network for learning in besov and '
                  'mixed smooth besov spaces: optimal rate and curse of '
                  'dimensionality',
         'url': 'https://arxiv.org/abs/1810.08033',
         'venue': 'arXiv preprint arXiv:1810.08033',
         'year': '2018'},
 'citations_link': '/scholar?cites=5691160361865711721&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaptivity%2Bof%2Bdeep%2BReLU%2Bnetwork%2Bfor%2Blearning%2Bin%2BBesov%2Band%2Bmixed%2Bsmooth%2BBesov%2Bspaces:%2Boptimal%2Brate%2Band%2Bcurse%2Bof%2Bdimensionality%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=aRDKMDIQ-04J&ei=8ooqX8OZK8mdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:aRDKMDIQ-04J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqNVPcizLi17YC2VwkB1BNDQ-RYmTAp&scisig=AAGBfm0AAAAAXyqNVKeaUshsy3RfgHFfXz2qeoarfPu1&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
40
-------------------------------------------------
2020-08-05 10:33:33
Got the results of the query
{'bib': {'abstract': 'Recent improvements to Generative Adversarial Networks '
                     '(GANs) have made it possible to generate realistic '
                     'images in high resolution based on natural language '
                     'descriptions such as image captions. Furthermore, '
                     'conditional GANs allow us to control the image '
                     'generation process through labels or even natural '
                     'language descriptions. However, fine-grained control of '
                     'the image layout, ie where in the image specific objects '
                     'should be located, is still difficult to achieve. This '
                     'is especially true for images that should contain '
                     'multiple distinct objects at',
         'author': ['T Hinz', 'S Heinrich', 'S Wermter'],
         'cites': '26',
         'eprint': 'https://arxiv.org/pdf/1901.00686',
         'gsrank': '1',
         'title': 'Generating multiple objects at spatially distinct locations',
         'url': 'https://arxiv.org/abs/1901.00686',
         'venue': 'arXiv preprint arXiv:1901.00686',
         'year': '2019'},
 'citations_link': '/scholar?cites=13574885695794039292&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerating%2BMultiple%2BObjects%2Bat%2BSpatially%2BDistinct%2BLocations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=_MlVFJ6uY7wJ&ei=BosqX8KBAYuayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:_MlVFJ6uY7wJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqNZXwDyGfmoUfNbC_pa38ByOymlxKz&scisig=AAGBfm0AAAAAXyqNZSWEnS8GFHlBeQ5TQsQV9ryguVZj&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
26
-------------------------------------------------
2020-08-05 10:33:50
Got the results of the query
{'bib': {'abstract': 'We study the problem of representation learning in '
                     'goal-conditioned hierarchical reinforcement learning. In '
                     'such hierarchical structures, a higher-level controller '
                     'solves tasks by iteratively communicating goals which a '
                     'lower-level policy is trained to reach. Accordingly, the '
                     'choice of representation--the mapping of observation '
                     'space to goal space--is crucial. To study this problem, '
                     'we develop a notion of sub-optimality of a '
                     'representation, defined in terms of expected reward of '
                     'the optimal hierarchical policy using this',
         'author': ['O Nachum', 'S Gu', 'H Lee', 'S Levine'],
         'cites': '43',
         'eprint': 'https://arxiv.org/pdf/1810.01257',
         'gsrank': '1',
         'title': 'Near-optimal representation learning for hierarchical '
                  'reinforcement learning',
         'url': 'https://arxiv.org/abs/1810.01257',
         'venue': 'arXiv preprint arXiv:1810.01257',
         'year': '2018'},
 'citations_link': '/scholar?cites=17682749665983906973&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNear-Optimal%2BRepresentation%2BLearning%2Bfor%2BHierarchical%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ncQgTSK_ZfUJ&ei=FosqX76JN82iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ncQgTSK_ZfUJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqNd49k_JouXHzyCFnnzg9EMhSoACjT&scisig=AAGBfm0AAAAAXyqNd4sGhoHCu9abJ6HK1CZ2X-UudCwv&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
43
-------------------------------------------------
2020-08-05 10:34:08
Got the results of the query
{'bib': {'abstract': 'Word embedding is a powerful tool in natural language '
                     'processing. In this paper we consider the problem of '
                     'word embedding composition\\---given vector '
                     'representations of two words, compute a vector for the '
                     'entire phrase. We give a generative model that can '
                     'capture specific syntactic relations between words. '
                     'Under our model, we prove that the correlations between '
                     'three words (measured by their PMI) form a tensor that '
                     'has an approximate low rank Tucker decomposition. The '
                     'result of the Tucker decomposition gives the word',
         'author': ['A Frandsen', 'R Ge'],
         'cites': '3',
         'eprint': 'https://arxiv.org/pdf/1902.00613',
         'gsrank': '1',
         'title': 'Understanding composition of word embeddings via tensor '
                  'decomposition',
         'url': 'https://arxiv.org/abs/1902.00613',
         'venue': 'arXiv preprint arXiv:1902.00613',
         'year': '2019'},
 'citations_link': '/scholar?cites=9072436238425463642&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BComposition%2Bof%2BWord%2BEmbeddings%2Bvia%2BTensor%2BDecomposition%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Whu75rDE530J&ei=KIsqX4CYFZmG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Whu75rDE530J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqNi5Yq4FUujVCxE2ffXR4AG9t75m5f&scisig=AAGBfm0AAAAAXyqNi8JkWfGVE-8aBWs8-S3Nz4BhQqeF&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
3
-------------------------------------------------
2020-08-05 10:34:27
Got the results of the query
{'bib': {'abstract': 'Summarization of long sequences into a concise statement '
                     'is a core problem in natural language processing, '
                     'requiring non-trivial understanding of the input. Based '
                     'on the promising results of graph neural networks on '
                     'highly structured data, we develop a',
         'author': ['P Fernandes', 'M Allamanis', 'M Brockschmidt'],
         'cites': '28',
         'eprint': 'https://arxiv.org/pdf/1811.01824',
         'gsrank': '1',
         'title': 'Structured neural summarization',
         'url': 'https://arxiv.org/abs/1811.01824',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=5961913139611201410&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStructured%2BNeural%2BSummarization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=gm9CtGn4vFIJ&ei=QIsqX6agOouayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:gm9CtGn4vFIJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqNpGX3AYKD7q_q5mK9jGnwmhNYTtfM&scisig=AAGBfm0AAAAAXyqNpKOsHqR6WRKMj3W919DlcSf50mCi&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
28
-------------------------------------------------
2020-08-05 10:34:52
Got the results of the query
{'bib': {'abstract': 'We present graph wavelet neural network (GWNN), a novel '
                     'graph convolutional neural network (CNN), leveraging '
                     'graph wavelet transform to address the shortcomings of '
                     'previous spectral graph CNN methods that depend on graph '
                     'Fourier transform. Different from graph Fourier '
                     'transform, graph wavelet transform can be obtained via a '
                     'fast algorithm without requiring matrix '
                     'eigendecomposition with high computational cost. '
                     'Moreover, graph wavelets are sparse and localized in '
                     'vertex domain, offering high efficiency and good',
         'author': ['B Xu', 'H Shen', 'Q Cao', 'Y Qiu', 'X Cheng'],
         'cites': '36',
         'eprint': 'https://arxiv.org/pdf/1904.07785',
         'gsrank': '1',
         'title': 'Graph wavelet neural network',
         'url': 'https://arxiv.org/abs/1904.07785',
         'venue': 'arXiv preprint arXiv:1904.07785',
         'year': '2019'},
 'citations_link': '/scholar?cites=10385380643777669724&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGraph%2BWavelet%2BNeural%2BNetwork%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=XOqV4atIIJAJ&ei=VosqX5nSB4S0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:XOqV4atIIJAJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqNs6_eMM_rNCSRx76PjCdFszRNJUeQ&scisig=AAGBfm0AAAAAXyqNs_VUFOxYEr7JgRMYbYNSliVlFTVY&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
36
-------------------------------------------------
2020-08-05 10:35:08
Got the results of the query
{'bib': {'abstract': 'Classical models describe primary visual cortex (V1) as '
                     'a filter bank of orientation-selective linear-nonlinear '
                     '(LN) or energy models, but these models fail to predict '
                     'neural responses to natural stimuli accurately. Recent '
                     'work shows that models based on convolutional neural '
                     'networks (CNNs) lead to much more accurate predictions, '
                     'but it remains unclear which features are extracted by '
                     'V1 neurons beyond orientation selectivity and phase '
                     'invariance. Here we work towards systematically studying '
                     'V1 computations by categorizing neurons into',
         'author': ['AS Ecker', 'FH Sinz', 'E Froudarakis', 'PG Fahey'],
         'cites': '12',
         'eprint': 'https://arxiv.org/pdf/1809.10504',
         'gsrank': '1',
         'title': 'A rotation-equivariant convolutional neural network model '
                  'of primary visual cortex',
         'url': 'https://arxiv.org/abs/1809.10504',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=16775727253632927156&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2Brotation-equivariant%2Bconvolutional%2Bneural%2Bnetwork%2Bmodel%2Bof%2Bprimary%2Bvisual%2Bcortex%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=tEWhOQ5bz-gJ&ei=Y4sqX6yVE4S0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:tEWhOQ5bz-gJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqNxMEEH2C63X-SiZ3DRyPwBihnTplh&scisig=AAGBfm0AAAAAXyqNxGwepnNKxGFYyl97p81pvGmbYEq3&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 10:35:25
Got the results of the query
{'bib': {'abstract': 'We study data-driven methods for community detection on '
                     'graphs, an inverse problem that is typically solved in '
                     'terms of the spectrum of certain operators or via '
                     'posterior inference under certain probabilistic '
                     'graphical models. Focusing on random graph families such '
                     'as the stochastic block model, recent research has '
                     'unified both approaches and identified both statistical '
                     'and computational signal-to-noise detection thresholds. '
                     'This graph inference task can be recast as a node-wise '
                     'graph classification problem, and, as such, '
                     'computational',
         'author': ['Z Chen', 'X Li', 'J Bruna'],
         'cites': '53',
         'eprint': 'https://arxiv.org/pdf/1705.08415',
         'gsrank': '1',
         'title': 'Supervised community detection with line graph neural '
                  'networks',
         'url': 'https://arxiv.org/abs/1705.08415',
         'venue': 'arXiv preprint arXiv:1705.08415',
         'year': '2017'},
 'citations_link': '/scholar?cites=5008209229610559765&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSupervised%2BCommunity%2BDetection%2Bwith%2BLine%2BGraph%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=FSnFe8K7gEUJ&ei=dosqX5SID4uayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:FSnFe8K7gEUJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqN1PLfwTqEZHRDWy01UIrPd7gXsb_O&scisig=AAGBfm0AAAAAXyqN1Br0D4QpoztnlZEgd2OBouIg7-T_&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
53
-------------------------------------------------
2020-08-05 10:35:40
Got the results of the query
{'bib': {'abstract': 'The dominant approach to unsupervised" style '
                     "transfer''in text is based on the idea of learning a "
                     'latent representation, which is independent of the '
                     'attributes specifying its" style\'\'. In this paper, we '
                     'show that this condition is not necessary and is not '
                     'always met in practice',
         'author': ['G Lample', 'S Subramanian', 'E Smith'],
         'cites': '49',
         'eprint': 'https://openreview.net/pdf?id=H1g2NhC5KQ',
         'gsrank': '1',
         'title': 'Multiple-attribute text rewriting',
         'url': 'https://openreview.net/forum?id=H1g2NhC5KQ',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=9470659499240433770&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMultiple-Attribute%2BText%2BRewriting%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=aqjnCKOKboMJ&ei=gYsqX-SjMcmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:aqjnCKOKboMJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqN4UM9GuM2FeEUqPaJzecU4FulF3FL&scisig=AAGBfm0AAAAAXyqN4bOoMuaiOlG-KT7S_xJlwqbIYanD&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
49
-------------------------------------------------
2020-08-05 10:35:53
Got the results of the query
{'bib': {'abstract': 'In this paper we propose to perform model ensembling in '
                     'a multiclass or a multilabel learning setting using '
                     'Wasserstein (W.) barycenters. Optimal transport metrics, '
                     'such as the Wasserstein distance, allow incorporating '
                     'semantic side information such as word',
         'author': ['P Dognin', 'I Melnyk', 'Y Mroueh', 'J Ross'],
         'cites': '4',
         'eprint': 'https://arxiv.org/pdf/1902.04999',
         'gsrank': '1',
         'title': 'Wasserstein barycenter model ensembling',
         'url': 'https://arxiv.org/abs/1902.04999',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=9651886521360061542&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWasserstein%2BBarycenter%2BModel%2BEnsembling%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ZhQMPqdj8oUJ&ei=kIsqX76UOp32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ZhQMPqdj8oUJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqN89tjtqP1Fd5k9rHbiSPxo_KZ0kuX&scisig=AAGBfm0AAAAAXyqN8y1kCZkeaDomgiyDiobbqcFzg1Vy&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
4
-------------------------------------------------
2020-08-05 10:36:11
Got the results of the query
{'bib': {'abstract': 'Computer simulation provides an automatic and safe way '
                     'for training robotic control policies to achieve complex '
                     'tasks such as locomotion. However, a policy trained in '
                     'simulation usually does not transfer directly to the '
                     'real hardware due to the differences between the two '
                     'environments. Transfer learning using domain '
                     'randomization is a promising approach, but it usually '
                     'assumes that the target environment is close to the '
                     'distribution of the training environments, thus relying '
                     'heavily on accurate system identification. In this '
                     'paper, we',
         'author': ['W Yu', 'CK Liu', 'G Turk'],
         'cites': '19',
         'eprint': 'https://arxiv.org/pdf/1810.05751',
         'gsrank': '1',
         'title': 'Policy transfer with strategy optimization',
         'url': 'https://arxiv.org/abs/1810.05751',
         'venue': 'arXiv preprint arXiv:1810.05751',
         'year': '2018'},
 'citations_link': '/scholar?cites=3099719291478959869&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPolicy%2BTransfer%2Bwith%2BStrategy%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=_WYHyEVqBCsJ&ei=o4sqX5unNobuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:_WYHyEVqBCsJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqOB-UU2yLdXEnB4k0xDu10CAq8FZ_H&scisig=AAGBfm0AAAAAXyqOB2QhuUzYY5zLQa4P1GVW1H9Jnl9G&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
19
-------------------------------------------------
2020-08-05 10:36:33
Got the results of the query
{'bib': {'abstract': 'The ability to generate natural language sequences from '
                     'source code snippets has a variety of applications such '
                     'as code summarization, documentation, and retrieval. '
                     'Sequence-to-sequence (seq2seq) models, adopted from '
                     'neural machine translation (NMT), have achieved '
                     'state-of-the-art performance on these tasks by treating '
                     'source code as a sequence of tokens. We present ${\\rm '
                     '{\\scriptsize CODE2SEQ}} $: an alternative approach that '
                     'leverages the syntactic structure of programming '
                     'languages to better encode source code',
         'author': ['U Alon', 'S Brody', 'O Levy', 'E Yahav'],
         'cites': '81',
         'eprint': 'https://arxiv.org/pdf/1808.01400',
         'gsrank': '1',
         'title': 'code2seq: Generating sequences from structured '
                  'representations of code',
         'url': 'https://arxiv.org/abs/1808.01400',
         'venue': 'arXiv preprint arXiv:1808.01400',
         'year': '2018'},
 'citations_link': '/scholar?cites=14844338714783082531&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3Dcode2seq:%2BGenerating%2BSequences%2Bfrom%2BStructured%2BRepresentations%2Bof%2BCode%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=I2g5lGmvAc4J&ei=t4sqX9eUL532ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:I2g5lGmvAc4J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqOGEyrrw5hmQoC7ecvqO1_HhRl4ug1&scisig=AAGBfm0AAAAAXyqOGGTTj1hQNY4VbGby_Iq195UP_942&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
81
-------------------------------------------------
2020-08-05 10:36:49
Got the results of the query
{'bib': {'abstract': 'Neural message passing algorithms for semi-supervised '
                     'classification on graphs have recently achieved great '
                     'success. However, for classifying a node these methods '
                     'only consider nodes that are a few propagation steps '
                     'away and the size of this utilized neighborhood is hard '
                     'to extend. In this paper, we use the relationship '
                     'between graph convolutional networks (GCN) and PageRank '
                     'to derive an improved propagation scheme based on '
                     'personalized PageRank. We utilize this propagation '
                     'procedure to construct a',
         'author': ['J Klicpera', 'A Bojchevski', 'S Günnemann'],
         'cites': '58',
         'eprint': 'https://arxiv.org/pdf/1810.05997',
         'gsrank': '1',
         'title': 'Predict then propagate: Graph neural networks meet '
                  'personalized pagerank',
         'url': 'https://arxiv.org/abs/1810.05997',
         'venue': 'arXiv preprint arXiv:1810.05997',
         'year': '2018'},
 'citations_link': '/scholar?cites=8962659856676213454&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPredict%2Bthen%2BPropagate:%2BGraph%2BNeural%2BNetworks%2Bmeet%2BPersonalized%2BPageRank%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ztLWR6fDYXwJ&ei=xosqX__FHcyXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ztLWR6fDYXwJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqOJdN7AbURITmFmpa2Z0T4uiHcflJc&scisig=AAGBfm0AAAAAXyqOJdzbHDgF75TdU_IheVV23OrXzAHS&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
58
-------------------------------------------------
2020-08-05 10:37:02
Got the results of the query
{'bib': {'abstract': 'We present a simple and general method to train a single '
                     'neural network executable at different widths (number of '
                     'channels in a layer), permitting instant and adaptive '
                     'accuracy-efficiency trade-offs at runtime. Instead of '
                     'training individual networks with different width',
         'author': ['J Yu', 'L Yang', 'N Xu', 'J Yang', 'T Huang'],
         'cites': '282',
         'eprint': 'https://arxiv.org/pdf/1812.08928',
         'gsrank': '1',
         'title': 'Slimmable neural networks',
         'url': 'https://arxiv.org/abs/1812.08928',
         'venue': 'arXiv preprint arXiv:1812.08928',
         'year': '2018'},
 'citations_link': '/scholar?cites=15212173000600372424&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSlimmable%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=yMSSGr9-HNMJ&ei=1osqX5LTMZ32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:yMSSGr9-HNMJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqOOkh_KHvJgEvc_Ii-mhYsvsZjrVry&scisig=AAGBfm0AAAAAXyqOOhGw8sxh0hL_J62exxRxn0LRdTdH&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
282
-------------------------------------------------
2020-08-05 10:37:23
Got the results of the query
{'bib': {'abstract': 'Mathematical reasoning---a core ability within human '
                     'intelligence---presents some unique challenges as a '
                     'domain: we do not come to understand and solve '
                     'mathematical problems primarily on the back of '
                     'experience and evidence, but on the basis of inferring, '
                     'learning, and exploiting laws, axioms, and symbol '
                     'manipulation rules. In this paper, we present a new '
                     'challenge for the evaluation (and eventually the design) '
                     'of neural architectures and similar system, developing a '
                     'task suite of mathematics problems involving sequential '
                     'questions and',
         'author': ['D Saxton', 'E Grefenstette', 'F Hill', 'P Kohli'],
         'cites': '46',
         'eprint': 'https://arxiv.org/pdf/1904.01557',
         'gsrank': '1',
         'title': 'Analysing mathematical reasoning abilities of neural models',
         'url': 'https://arxiv.org/abs/1904.01557',
         'venue': 'arXiv preprint arXiv:1904.01557',
         'year': '2019'},
 'citations_link': '/scholar?cites=5177820928273150256&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAnalysing%2BMathematical%2BReasoning%2BAbilities%2Bof%2BNeural%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=MPFNdLNQ20cJ&ei=6YsqX-GyI8yXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:MPFNdLNQ20cJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqORpIsqx69vqJvVrjmRtGL4uYXnCiH&scisig=AAGBfm0AAAAAXyqORly8aLW3vl0akF61l6HKhKDyxdmL&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
46
-------------------------------------------------
2020-08-05 10:37:35
Got the results of the query
{'bib': {'abstract': 'Explicit encoding of group actions in deep features '
                     'makes it possible for convolutional neural networks '
                     '(CNNs) to handle global deformations of images, which is '
                     'critical to success in many vision tasks. This paper '
                     'proposes to decompose the convolutional filters over '
                     'joint steerable bases across the space and the group '
                     'geometry simultaneously, namely a rotation-equivariant '
                     'CNN with decomposed convolutional filters (RotDCF). This '
                     'decomposition facilitates computing the joint '
                     'convolution, which is proved to be necessary for the '
                     'group',
         'author': ['X Cheng', 'Q Qiu', 'R Calderbank', 'G Sapiro'],
         'cites': '13',
         'eprint': 'https://arxiv.org/pdf/1805.06846',
         'gsrank': '1',
         'title': 'RotDCF: Decomposition of convolutional filters for '
                  'rotation-equivariant deep networks',
         'url': 'https://arxiv.org/abs/1805.06846',
         'venue': 'arXiv preprint arXiv:1805.06846',
         'year': '2018'},
 'citations_link': '/scholar?cites=6799055083001221032&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRotDCF:%2BDecomposition%2Bof%2BConvolutional%2BFilters%2Bfor%2BRotation-Equivariant%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=qA-aNpMYW14J&ei=-IsqX6ihBMyXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:qA-aNpMYW14J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqOXb2y6Ox88lyys_8mXRCM0d5TgNvJ&scisig=AAGBfm0AAAAAXyqOXYa6ATZuTXku00h7AlCNN7Yn5rpv&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
13
-------------------------------------------------
2020-08-05 10:37:58
Got the results of the query
{'bib': {'abstract': 'Neural program synthesis from input-output examples has '
                     'attracted an increasing interest from both the machine '
                     'learning and the programming language community. Most '
                     'existing neural program synthesis approaches employ an '
                     'encoder-decoder architecture, which uses',
         'author': ['X Chen', 'C Liu', 'D Song'],
         'cites': '22',
         'eprint': 'https://openreview.net/pdf?id=H1gfOiAqYm',
         'gsrank': '1',
         'title': 'Execution-guided neural program synthesis',
         'url': 'https://openreview.net/forum?id=H1gfOiAqYm',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=17780553618296819799&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExecution-Guided%2BNeural%2BProgram%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=V1BeeE83wfYJ&ei=D4wqX83TBouayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:V1BeeE83wfYJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqObJooP0SlathuQ3TPPH5vcdO1kvSQ&scisig=AAGBfm0AAAAAXyqObH4eInNXvYEhT-wMe5Fs7-YgU8Cq&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
22
-------------------------------------------------
2020-08-05 10:38:13
Got the results of the query
{'bib': {'abstract': 'We propose to execute deep neural networks (DNNs) with '
                     'dynamic and sparse graph (DSG) structure for compressive '
                     'memory and accelerative execution during both training '
                     'and inference. The great success of DNNs motivates the '
                     'pursuing of lightweight models for the deployment onto '
                     'embedded devices. However, most of the previous studies '
                     'optimize for inference while neglect training or even '
                     'complicate it. Training is far more intractable, since '
                     '(i) the neurons dominate the memory cost rather than the '
                     'weights in inference;(ii) the',
         'author': ['L Liu', 'L Deng', 'X Hu', 'M Zhu', 'G Li', 'Y Ding'],
         'cites': '14',
         'eprint': 'https://arxiv.org/pdf/1810.00859',
         'gsrank': '1',
         'title': 'Dynamic sparse graph for efficient deep learning',
         'url': 'https://arxiv.org/abs/1810.00859',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=961887975812995994&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamic%2BSparse%2BGraph%2Bfor%2BEfficient%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=mpsNiARQWQ0J&ei=G4wqX_ThKMmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:mpsNiARQWQ0J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqOeN8wILfouy2EbaQXOB_BXNilI-le&scisig=AAGBfm0AAAAAXyqOeGEsaho0mMoCDerjHklQM-QCQsCD&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
14
-------------------------------------------------
2020-08-05 10:38:25
Got the results of the query
{'bib': {'abstract': 'Normalization layers are a staple in state-of-the-art '
                     'deep neural network architectures. They are widely '
                     'believed to stabilize training, enable higher learning '
                     'rate, accelerate convergence and improve generalization, '
                     'though the reason for their effectiveness is still an '
                     'active research topic. In this work, we challenge the '
                     'commonly-held beliefs by showing that none of the '
                     'perceived benefits is unique to normalization. '
                     'Specifically, we propose fixed-update initialization '
                     '(Fixup), an initialization motivated by solving the '
                     'exploding and',
         'author': ['H Zhang', 'YN Dauphin', 'T Ma'],
         'cites': '87',
         'eprint': 'https://arxiv.org/pdf/1901.09321.pdf)',
         'gsrank': '1',
         'title': 'Fixup initialization: Residual learning without '
                  'normalization',
         'url': 'https://arxiv.org/abs/1901.09321',
         'venue': 'arXiv preprint arXiv:1901.09321',
         'year': '2019'},
 'citations_link': '/scholar?cites=10342250007176178945&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFixup%2BInitialization:%2BResidual%2BLearning%2BWithout%2BNormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=AW0EAJYNh48J&ei=LYwqX_bWN4S0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:AW0EAJYNh48J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqOj8LT2TGGeOyeg2slxw2nkW7W8Ez6&scisig=AAGBfm0AAAAAXyqOj0uUGSrx-PEQvieMA4vZIEsjtnV-&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
87
-------------------------------------------------
2020-08-05 10:38:48
Got the results of the query
{'bib': {'abstract': 'Probabilistic modelling is a principled framework to '
                     'perform model aggregation, which has been a primary '
                     'mechanism to combat mode collapse in the context of '
                     'Generative Adversarial Networks (GAN). In this paper, we '
                     'propose a novel probabilistic framework for GANs, '
                     'ProbGAN, which iteratively learns a distribution over '
                     'generators with a carefully crafted prior. Learning is '
                     'efficiently triggered by a tailored stochastic gradient '
                     'Hamiltonian Monte Carlo with a novel gradient '
                     'approximation to perform Bayesian inference. Our',
         'author': ['H He', 'H Wang', 'GH Lee', 'Y Tian'],
         'cites': '6',
         'eprint': 'https://pdfs.semanticscholar.org/99c8/70a6d94d1295a6ed1d004235cb4f8f676fbe.pdf',
         'gsrank': '1',
         'title': 'ProbGAN: Towards Probabilistic GAN with Theoretical '
                  'Guarantees.',
         'url': 'https://pdfs.semanticscholar.org/99c8/70a6d94d1295a6ed1d004235cb4f8f676fbe.pdf',
         'venue': 'ICLR (Poster)',
         'year': '2019'},
 'citations_link': '/scholar?cites=14122982623202387017&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProbGAN:%2BTowards%2BProbabilistic%2BGAN%2Bwith%2BTheoretical%2BGuarantees%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=SST-eN7p_sMJ&ei=QIwqX7bYLsyXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:SST-eN7p_sMJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqOn1UURIuP-JvXIURi5ul1d3kJ1arf&scisig=AAGBfm0AAAAAXyqOnyB2rcvBr7mBGkU6zPH3XCsN0lN7&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 10:39:06
Got the results of the query
{'bib': {'abstract': 'We introduce an exploration bonus for deep reinforcement '
                     'learning methods that is easy to implement and adds '
                     'minimal overhead to the computation performed. The bonus '
                     'is the error of a neural network predicting features of '
                     'the observations given by a fixed randomly',
         'author': ['Y Burda', 'H Edwards', 'A Storkey', 'O Klimov'],
         'cites': '215',
         'eprint': 'https://arxiv.org/pdf/1810.12894.pdf%20http://arxiv.org/abs/1810.12894',
         'gsrank': '1',
         'title': 'Exploration by random network distillation',
         'url': 'https://arxiv.org/abs/1810.12894',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=126098205768710278&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExploration%2Bby%2Brandom%2Bnetwork%2Bdistillation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=hnioaab9vwEJ&ei=VYwqX4r_JcmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:hnioaab9vwEJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqOtpO5ceVlN7tTOm3_dEILK9TgoQKc&scisig=AAGBfm0AAAAAXyqOtgRcDyx30GtUD-Wet6Bw6cIf6gbO&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
215
-------------------------------------------------
2020-08-05 10:39:26
Got the results of the query
{'bib': {'abstract': 'This paper describes a new form of unsupervised '
                     'learning, whose input is a set of unlabeled points that '
                     'are assumed to be local maxima of an unknown value '
                     'function v in an unknown subset of the vector space. Two '
                     'functions are learned:(i) a set indicator c, which is a '
                     'binary classifier, and (ii) a comparator function h that '
                     'given two nearby samples, predicts which sample has the '
                     'higher value of the unknown function v. Loss terms are '
                     'used to ensure that all training samples x are a local '
                     'maxima of v, according to h and satisfy c (x)= 1. '
                     'Therefore',
         'author': ['L Wolf', 'S Benaim', 'T Galanti'],
         'cites': '2',
         'eprint': 'https://arxiv.org/pdf/2001.05026',
         'gsrank': '1',
         'title': 'Unsupervised learning of the set of local maxima',
         'url': 'https://arxiv.org/abs/2001.05026',
         'venue': 'arXiv preprint arXiv:2001.05026',
         'year': '2020'},
 'citations_link': '/scholar?cites=4956525565743484630&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BLearning%2Bof%2Bthe%2BSet%2Bof%2BLocal%2BMaxima%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=1k5Iwr0dyUQJ&ei=ZowqX6v4HM2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:1k5Iwr0dyUQJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqOxgBFR9s5gf3wNAL4kwVl7pYp6x2p&scisig=AAGBfm0AAAAAXyqOxhMt2ZGX8Z_2bBn8YkRzS1Ee0EDL&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
2
-------------------------------------------------
2020-08-05 10:39:42
Got the results of the query
{'bib': {'abstract': 'This paper studies a class of adaptive gradient based '
                     'momentum algorithms that update the search directions '
                     'and learning rates simultaneously using past gradients. '
                     'This class, which we refer to as the" Adam-type", '
                     'includes the popular algorithms such as the Adam, '
                     'AMSGrad and AdaGrad. Despite their popularity in '
                     'training deep neural networks, the convergence of these '
                     'algorithms for solving nonconvex problems remains an '
                     'open question. This paper provides a set of mild '
                     'sufficient conditions that guarantee the convergence for '
                     'the Adam-type',
         'author': ['X Chen', 'S Liu', 'R Sun', 'M Hong'],
         'cites': '88',
         'eprint': 'https://arxiv.org/pdf/1808.02941',
         'gsrank': '1',
         'title': 'On the convergence of a class of adam-type algorithms for '
                  'non-convex optimization',
         'url': 'https://arxiv.org/abs/1808.02941',
         'venue': 'arXiv preprint arXiv:1808.02941',
         'year': '2018'},
 'citations_link': '/scholar?cites=16342443701076005816&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BConvergence%2Bof%2BA%2BClass%2Bof%2BAdam-Type%2BAlgorithms%2Bfor%2BNon-Convex%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=uJN0zPUFzOIJ&ei=dowqX9nbHIS0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:uJN0zPUFzOIJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqO1wgAKgaKbqg1DKxj5cjcJYsihY9d&scisig=AAGBfm0AAAAAXyqO106XALsHohkHqgUkUKtRqdpcQAq7&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
88
-------------------------------------------------
2020-08-05 10:39:59
Got the results of the query
{'bib': {'abstract': 'Sequence to sequence (seq2seq) models have become a '
                     'popular framework for neural sequence prediction. While '
                     'traditional seq2seq models are trained by Maximum '
                     'Likelihood Estimation (MLE), much recent work has made '
                     'various attempts to optimize evaluation scores directly '
                     'to solve the mismatch between training and evaluation, '
                     'since model predictions are usually evaluated by a task '
                     'specific evaluation metric like BLEU or ROUGE scores '
                     'instead of perplexity. This paper puts this existing '
                     'work into two categories, a)',
         'author': ['H Zhang', 'H Zhao'],
         'cites': '6',
         'eprint': 'https://openreview.net/pdf?id=H1xD9sR5Fm',
         'gsrank': '1',
         'title': 'Minimum divergence vs. maximum margin: an empirical '
                  'comparison on seq2seq models',
         'url': 'https://openreview.net/forum?id=H1xD9sR5Fm',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1741946152253085868&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMinimum%2BDivergence%2Bvs.%2BMaximum%2BMargin:%2Ban%2BEmpirical%2BComparison%2Bon%2BSeq2Seq%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=rFjn1suiLBgJ&ei=howqX-3MLM2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:rFjn1suiLBgJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqO6aswWekLlRoJurf7ridtolI7Hqg5&scisig=AAGBfm0AAAAAXyqO6S8miCN50HGUPnBQU1XxMMnxaPmR&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 10:40:17
Got the results of the query
{'bib': {'abstract': 'Efficient audio synthesis is an inherently difficult '
                     'machine learning task, as human perception is sensitive '
                     'to both global structure and fine-scale waveform '
                     'coherence. Autoregressive models, such as WaveNet, model '
                     'local structure at the expense of global',
         'author': ['J Engel', 'KK Agrawal', 'S Chen', 'I Gulrajani'],
         'cites': '84',
         'eprint': 'https://arxiv.org/pdf/1902.08710',
         'gsrank': '1',
         'title': 'Gansynth: Adversarial neural audio synthesis',
         'url': 'https://arxiv.org/abs/1902.08710',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=1141907515038951552&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGANSynth:%2BAdversarial%2BNeural%2BAudio%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=gOg-uNXe2A8J&ei=mIwqX7uWHYKTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:gOg-uNXe2A8J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqO-Muzusyd8LGWG12PyfLRYbTpXRKY&scisig=AAGBfm0AAAAAXyqO-IvTLo7wleKgI9kZsDjOh8pqUvMm&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
84
-------------------------------------------------
2020-08-05 10:40:32
Got the results of the query
{'bib': {'abstract': 'In this paper we use the geometric properties of the '
                     'optimal transport (OT) problem and the Wasserstein '
                     'distances to define a prior distribution for the latent '
                     'space of an auto-encoder. We introduce '
                     'Sliced-Wasserstein Auto-Encoders (SWAE), that enable one '
                     'to shape the',
         'author': ['S Kolouri', 'PE Pope', 'CE Martin'],
         'cites': '24',
         'gsrank': '1',
         'title': 'Sliced Wasserstein auto-encoders',
         'url': 'https://openreview.net/forum?id=H1xaJn05FQ',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=5090616683770354534&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSliced%2BWasserstein%2BAuto-Encoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ZhvPE-eApUYJ&ei=powqX6LwCZ32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ZhvPE-eApUYJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqPB41cUzx0JOIWKluk4R5YTUoHT3zq&scisig=AAGBfm0AAAAAXyqPB64eOLdsLv8w_BYCuzeUIMnU9sA6&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
24
-------------------------------------------------
2020-08-05 10:40:47
Got the results of the query
{'bib': {'abstract': 'We give a new algorithm for learning a two-layer neural '
                     'network under a general class of input distributions. '
                     'Assuming there is a ground-truth two-layer network $$ y= '
                     'A\\sigma (Wx)+\\xi, $$ where $ A, W $ are weight '
                     'matrices, $\\xi $ represents noise, and the number of '
                     'neurons in the hidden layer is no larger than the input '
                     'or output, our algorithm is guaranteed to recover the '
                     'parameters $ A, W $ of the ground-truth network. The '
                     'only requirement on the input $ x $ is that it is '
                     'symmetric, which still allows highly complicated and '
                     'structured input',
         'author': ['R Ge', 'R Kuditipudi', 'Z Li', 'X Wang'],
         'cites': '22',
         'eprint': 'https://arxiv.org/pdf/1810.06793',
         'gsrank': '1',
         'title': 'Learning two-layer neural networks with symmetric inputs',
         'url': 'https://arxiv.org/abs/1810.06793',
         'venue': 'arXiv preprint arXiv:1810.06793',
         'year': '2018'},
 'citations_link': '/scholar?cites=205914550108929480&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTwo-layer%2BNeural%2BNetworks%2Bwith%2BSymmetric%2BInputs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=yL1TPTGO2wIJ&ei=s4wqX_XzNIbuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:yL1TPTGO2wIJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqPFmo1NGcvjyOqsuQneHsZHmlz--JM&scisig=AAGBfm0AAAAAXyqPFluYOhlMJsy99HLeYMk847qTuMEu&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
22
-------------------------------------------------
2020-08-05 10:41:02
Got the results of the query
{'bib': {'abstract': 'Recent work has shown that deep reinforcement-learning '
                     'agents can learn to follow language-like instructions '
                     'from infrequent environment rewards. However, this '
                     'places on environment designers the onus of designing '
                     'language-conditional reward functions which may not be '
                     'easily or tractably implemented as the complexity of the '
                     'environment and the language scales. To overcome this '
                     'limitation, we present a framework within which '
                     'instruction-conditional RL agents are trained using '
                     'rewards obtained not from the',
         'author': ['D Bahdanau', 'F Hill', 'J Leike', 'E Hughes'],
         'cites': '33',
         'eprint': 'https://arxiv.org/pdf/1806.01946',
         'gsrank': '1',
         'title': 'Learning to understand goal specifications by modelling '
                  'reward',
         'url': 'https://arxiv.org/abs/1806.01946',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=12539431874776998736&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BUnderstand%2BGoal%2BSpecifications%2Bby%2BModelling%2BReward%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=UCNLddoCBa4J&ei=x4wqX4-FOcyXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:UCNLddoCBa4J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqPJ1N6SGXuBp_uA2RW6hcufnZz3aCC&scisig=AAGBfm0AAAAAXyqPJ3KI49M9rlnulEFmUgy1K_5b65cp&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
33
-------------------------------------------------
2020-08-05 10:41:20
Got the results of the query
{'bib': {'abstract': 'A neural network deployed in the wild may be asked to '
                     'make predictions for inputs that were drawn from a '
                     'different distribution than that of the training data. A '
                     'plethora of work has demonstrated that it is easy to '
                     'find or synthesize inputs for which a neural network is '
                     'highly confident yet wrong. Generative models are widely '
                     'viewed to be robust to such mistaken confidence as '
                     'modeling the density of the input features can be used '
                     'to detect novel, out-of-distribution inputs. In this '
                     'paper we challenge this assumption. We find that the '
                     'density',
         'author': ['E Nalisnick', 'A Matsukawa', 'YW Teh', 'D Gorur'],
         'cites': '132',
         'eprint': 'https://arxiv.org/pdf/1810.09136',
         'gsrank': '1',
         'title': "Do deep generative models know what they don't know?",
         'url': 'https://arxiv.org/abs/1810.09136',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8498584058191576508&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDo%2BDeep%2BGenerative%2BModels%2BKnow%2BWhat%2BThey%2BDon%255C%2527t%2BKnow%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=vH1OaiwJ8XUJ&ei=2IwqX8WhJM2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:vH1OaiwJ8XUJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqPNl7AjndXHFl2PC5476VTnOzIfnEk&scisig=AAGBfm0AAAAAXyqPNtxQ4WihPktIEcHW3sxwAWcEXFq2&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
132
-------------------------------------------------
2020-08-05 10:41:35
Got the results of the query
{'bib': {'abstract': 'Neural machine translation (NMT) models learn '
                     'representations containing substantial linguistic '
                     'information. However, it is not clear if such '
                     'information is fully distributed or if some of it can be '
                     'attributed to individual neurons. We develop '
                     'unsupervised methods for discovering important neurons '
                     'in NMT models. Our methods rely on the intuition that '
                     'different models learn similar properties, and do not '
                     'require any costly external supervision. We show '
                     'experimentally that translation quality depends on the '
                     'discovered neurons, and',
         'author': ['A Bau', 'Y Belinkov', 'H Sajjad', 'N Durrani', 'F Dalvi'],
         'cites': '37',
         'eprint': 'https://arxiv.org/pdf/1811.01157',
         'gsrank': '1',
         'title': 'Identifying and controlling important neurons in neural '
                  'machine translation',
         'url': 'https://arxiv.org/abs/1811.01157',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=10670221460130643181&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DIdentifying%2Band%2BControlling%2BImportant%2BNeurons%2Bin%2BNeural%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=7TzWfeI9FJQJ&ei=54wqX_fDBs2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:7TzWfeI9FJQJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqPSsKtrEReCklcXu-g_37_6DXdpNIR&scisig=AAGBfm0AAAAAXyqPSn2U3r6eHImBwxgLlPzbXQJP2BkD&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
37
-------------------------------------------------
2020-08-05 10:41:54
Got the results of the query
{'bib': {'abstract': 'We investigate the internal representations that a '
                     'recurrent neural network (RNN) uses while learning to '
                     'recognize a regular formal language. Specifically, we '
                     'train a RNN on positive and negative examples from a '
                     'regular language, and ask if there is a simple decoding '
                     'function that maps states of this RNN to states of the '
                     'minimal deterministic finite automaton (MDFA) for the '
                     'language. Our experiments show that such a decoding '
                     'function indeed exists, and that it maps states of the '
                     'RNN not to MDFA states, but to states of an {\\em',
         'author': ['JJ Michalenko', 'A Shah', 'A Verma', 'RG Baraniuk'],
         'cites': '3',
         'eprint': 'https://arxiv.org/pdf/1902.10297',
         'gsrank': '1',
         'title': 'Representing formal languages: A comparison between finite '
                  'automata and recurrent neural networks',
         'url': 'https://arxiv.org/abs/1902.10297',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=1743679466435781138&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRepresenting%2BFormal%2BLanguages:%2BA%2BComparison%2BBetween%2BFinite%2BAutomata%2Band%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=EiIMfzzLMhgJ&ei=-YwqX8fHCp32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:EiIMfzzLMhgJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqPWhMmNr7DS44GT8fmcYC8pPAlTxlF&scisig=AAGBfm0AAAAAXyqPWkfcW3OI2vPTRyOUgjejvIj-pnl4&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
3
-------------------------------------------------
2020-08-05 10:42:11
Got the results of the query
{'bib': {'abstract': 'Interpretation and explanation of deep models is '
                     'critical towards wide adoption of systems that rely on '
                     'them. In this paper, we propose a novel scheme for both '
                     'interpretation as well as explanation in which, given a '
                     'pretrained model, we automatically identify internal '
                     'features',
         'author': ['J Oramas', 'K Wang', 'T Tuytelaars'],
         'cites': '13',
         'eprint': 'https://arxiv.org/pdf/1712.06302',
         'gsrank': '1',
         'title': 'Visual explanation by interpretation: Improving visual '
                  'feedback capabilities of deep neural networks',
         'url': 'https://arxiv.org/abs/1712.06302',
         'venue': 'arXiv preprint arXiv:1712.06302',
         'year': '2017'},
 'citations_link': '/scholar?cites=5644310437973500116&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVisual%2BExplanation%2Bby%2BInterpretation:%2BImproving%2BVisual%2BFeedback%2BCapabilities%2Bof%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=1KBwQXCeVE4J&ei=Co0qX8mAMp32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:1KBwQXCeVE4J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqPaVIW4v9CyUsKoLCBz3r1dY2pmtAN&scisig=AAGBfm0AAAAAXyqPaZlYvB4LWPOQN6quKlXGc1DBslDW&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
13
-------------------------------------------------
2020-08-05 10:42:26
Got the results of the query
{'bib': {'abstract': 'Generative Adversarial Networks are one of the leading '
                     'tools in generative modeling, image editing and content '
                     'creation. However, they are hard to train as they '
                     'require a delicate balancing act between two deep '
                     'networks fighting a never ending duel. Some of the most '
                     'promising adversarial models today minimize a '
                     'Wasserstein objective. It is smoother and more stable to '
                     'optimize. In this paper, we show that the Wasserstein '
                     'distance is just one out of a large family of objective '
                     'functions that yield these properties. By making the',
         'author': ['B Zhou', 'P Krähenbühl'],
         'cites': '4',
         'eprint': 'https://openreview.net/pdf?id=HJE6X305Fm',
         'gsrank': '1',
         'title': "Don't let your Discriminator be fooled",
         'url': 'https://openreview.net/forum?id=HJE6X305Fm',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8029169282646543117&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDon%255C%2527t%2Blet%2Byour%2BDiscriminator%2Bbe%2Bfooled%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=DQugFexWbW8J&ei=GY0qX6nIC4uayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:DQugFexWbW8J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqPe_raAZEpT07NUZX3S_o-LpiOTEUu&scisig=AAGBfm0AAAAAXyqPexjU15rL660g_1zxwaBafPKF8-GL&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
4
-------------------------------------------------
2020-08-05 10:42:43
Got the results of the query
{'bib': {'abstract': 'In this paper, we propose a new latent semantic model '
                     'that incorporates a convolutional-pooling structure over '
                     'word sequences to learn low-dimensional, semantic vector '
                     'representations for search queries and Web documents. In '
                     'order to capture the rich',
         'author': ['Y Shen', 'X He', 'J Gao', 'L Deng', 'G Mesnil'],
         'cites': '462',
         'eprint': 'https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2014_cdssm_final.pdf',
         'gsrank': '1',
         'title': 'A latent semantic model with convolutional-pooling '
                  'structure for information retrieval',
         'url': 'https://dl.acm.org/doi/abs/10.1145/2661829.2661935',
         'venue': 'Proceedings of the 23rd ACM …',
         'year': '2014'},
 'citations_link': '/scholar?cites=15105908672529426618&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLatent%2BConvolutional%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=uuibM-f3otEJ&ei=Ko0qX8GBPc2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:uuibM-f3otEJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqPipo5WDN9dGEcnzlSSRpN0Hln--sO&scisig=AAGBfm0AAAAAXyqPiiXIsOKC25TyNdwB3x2hkGdtPT0e&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
462
-------------------------------------------------
2020-08-05 10:42:58
Got the results of the query
{'bib': {'abstract': 'We present a method for translating music across musical '
                     'instruments, genres, and styles. This method is based on '
                     'a multi-domain wavenet autoencoder, with a shared '
                     'encoder and a disentangled latent space that is trained '
                     'end-to-end on waveforms. Employing a diverse',
         'author': ['N Mor', 'L Wolf', 'A Polyak', 'Y Taigman'],
         'cites': '59',
         'eprint': 'https://arxiv.org/pdf/1805.07848',
         'gsrank': '1',
         'title': 'A universal music translation network',
         'url': 'https://arxiv.org/abs/1805.07848',
         'venue': 'arXiv preprint arXiv:1805.07848',
         'year': '2018'},
 'citations_link': '/scholar?cites=6168332349111008894&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BUniversal%2BMusic%2BTranslation%2BNetwork%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=fv5PHZdRmlUJ&ei=PY0qX6rrBZmG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:fv5PHZdRmlUJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqPn_KEHHnuD321PQkuaVtbtT-yKuVf&scisig=AAGBfm0AAAAAXyqPn6DlIe-5HZ4wHswTwtkd9gD4kSI-&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
59
-------------------------------------------------
2020-08-05 10:43:19
Got the results of the query
{'bib': {'abstract': 'The field of few-shot learning has recently seen '
                     'substantial advancements. Most of these advancements '
                     'came from casting few-shot learning as a meta-learning '
                     'problem. Model Agnostic Meta Learning or MAML is '
                     'currently one of the best approaches for few-shot',
         'author': ['A Antoniou', 'H Edwards', 'A Storkey'],
         'cites': '98',
         'eprint': 'https://arxiv.org/pdf/1810.09502',
         'gsrank': '1',
         'title': 'How to train your MAML',
         'url': 'https://arxiv.org/abs/1810.09502',
         'venue': 'arXiv preprint arXiv:1810.09502',
         'year': '2018'},
 'citations_link': '/scholar?cites=12854985256703612425&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHow%2Bto%2Btrain%2Byour%2BMAML%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=CdazevsUZrIJ&ei=TY0qX46BHoKTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:CdazevsUZrIJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqPqxQoQSEdoM_AL6I4FXAMCiNzKD1Z&scisig=AAGBfm0AAAAAXyqPq0Z1mJMF9r7GX8G6ssdFx57kTbyh&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
98
-------------------------------------------------
2020-08-05 10:43:31
Got the results of the query
{'bib': {'abstract': 'We present NeuroSAT, a message passing neural network '
                     'that learns to solve SAT problems after only being '
                     'trained as a classifier to predict satisfiability. '
                     'Although it is not competitive with state-of-the-art SAT '
                     'solvers, NeuroSAT can solve problems that are '
                     'substantially larger and more difficult than it ever saw '
                     'during training by simply running for more iterations. '
                     'Moreover, NeuroSAT generalizes to novel distributions; '
                     'after training only on random SAT problems, at test time '
                     'it can solve SAT problems encoding graph coloring',
         'author': ['D Selsam', 'M Lamm', 'B Bünz', 'P Liang'],
         'cites': '123',
         'eprint': 'https://arxiv.org/pdf/1802.03685',
         'gsrank': '1',
         'title': 'Learning a SAT solver from single-bit supervision',
         'url': 'https://arxiv.org/abs/1802.03685',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=6266294675244210264&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Ba%2BSAT%2BSolver%2Bfrom%2BSingle-Bit%2BSupervision%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=WFjaj85Z9lYJ&ei=WY0qX-3ENsmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:WFjaj85Z9lYJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqPuXlGcBCmmt4evEat2Mg-YFHNQA64&scisig=AAGBfm0AAAAAXyqPuQLmvrNBG6IyWLeEbuJn6EVRPJi1&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
123
-------------------------------------------------
2020-08-05 10:43:46
Got the results of the query
{'bib': {'abstract': 'Representations of sets are challenging to learn because '
                     'operations on sets should be permutation-invariant. To '
                     'this end, we propose a Permutation-Optimisation module '
                     'that learns how to permute a set end-to-end. The '
                     'permuted set can be further processed to learn a '
                     'permutation-invariant representation of that set, '
                     'avoiding a bottleneck in traditional set models. We '
                     "demonstrate our model's ability to learn permutations "
                     'and set representations with either explicit or implicit '
                     'supervision on four datasets, on which we achieve '
                     'state-of-the',
         'author': ['Y Zhang', 'J Hare', 'A Prügel-Bennett'],
         'cites': '7',
         'eprint': 'https://arxiv.org/pdf/1812.03928',
         'gsrank': '1',
         'title': 'Learning representations of sets through optimized '
                  'permutations',
         'url': 'https://arxiv.org/abs/1812.03928',
         'venue': 'arXiv preprint arXiv:1812.03928',
         'year': '2018'},
 'citations_link': '/scholar?cites=18380743779170392260&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BRepresentations%2Bof%2BSets%2Bthrough%2BOptimized%2BPermutations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=xBjs8BSFFf8J&ei=a40qX5exFIbuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:xBjs8BSFFf8J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqPyUf-9onjGBLE8q7MZemGUxxrQF27&scisig=AAGBfm0AAAAAXyqPyUF7EobqGiVEnNy32ppwLjyhP2XS&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
7
-------------------------------------------------
2020-08-05 10:44:01
Got the results of the query
{'bib': {'abstract': 'In this paper, we propose a novel Convolutional Neural '
                     'Network (CNN) architecture for learning multi-scale '
                     'feature representations with good tradeoffs between '
                     'speed and accuracy. This is achieved by using a '
                     'multi-branch network, which has different computational '
                     'complexity at different branches. Through frequent '
                     'merging of features from branches at distinct scales, '
                     'our model obtains multi-scale features while using less '
                     'computation. The proposed approach demonstrates '
                     'improvement of model efficiency and',
         'author': ['CF Chen', 'Q Fan', 'N Mallinar', 'T Sercu'],
         'cites': '21',
         'eprint': 'https://arxiv.org/pdf/1807.03848',
         'gsrank': '1',
         'title': 'Big-little net: An efficient multi-scale feature '
                  'representation for visual and speech recognition',
         'url': 'https://arxiv.org/abs/1807.03848',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=555905086227832192&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBig-Little%2BNet:%2BAn%2BEfficient%2BMulti-Scale%2BFeature%2BRepresentation%2Bfor%2BVisual%2Band%2BSpeech%2BRecognition%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=gF1xCbv4tgcJ&ei=eo0qX8y_GoKTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:gF1xCbv4tgcJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqP3UOY_jqMhj_JF1RGFIRMrrjLocXU&scisig=AAGBfm0AAAAAXyqP3Z0PN42BhwLquSaEMQT8sSGVD7pm&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
21
-------------------------------------------------
2020-08-05 10:44:21
Got the results of the query
{'bib': {'abstract': 'We consider the problem of aligning continuous word '
                     'representations, learned in multiple languages, to a '
                     'common space. It was recently shown that, in the case of '
                     'two languages, it is possible to learn such a mapping '
                     'without supervision. This paper extends this line of '
                     'work to the problem of aligning multiple languages to a '
                     'common space. A solution is to independently map all '
                     'languages to a pivot language. Unfortunately, this '
                     'degrades the quality of indirect word translation. We '
                     'thus propose a novel formulation that ensures',
         'author': ['J Alaux', 'E Grave', 'M Cuturi', 'A Joulin'],
         'cites': '29',
         'eprint': 'https://arxiv.org/pdf/1811.01124',
         'gsrank': '1',
         'title': 'Unsupervised hyperalignment for multilingual word '
                  'embeddings',
         'url': 'https://arxiv.org/abs/1811.01124',
         'venue': 'arXiv preprint arXiv:1811.01124',
         'year': '2018'},
 'citations_link': '/scholar?cites=9547479920673238095&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BHyper-alignment%2Bfor%2BMultilingual%2BWord%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=TyznM2d2f4QJ&ei=j40qX93xA4uayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:TyznM2d2f4QJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqP8Poj7u-p0H8IlpvEW2LECWEdjWo8&scisig=AAGBfm0AAAAAXyqP8MteD32rF_AAzW_-aaheaN2DkXgS&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
29
-------------------------------------------------
2020-08-05 10:44:41
Got the results of the query
{'bib': {'abstract': 'How do humans navigate to target objects in novel '
                     'scenes? Do we use the semantic/functional priors we have '
                     'built over years to efficiently search and navigate? For '
                     'example, to search for mugs, we search cabinets near the '
                     'coffee machine and for fruits we try the fridge. In this '
                     'work, we focus on incorporating semantic priors in the '
                     'task of semantic navigation. We propose to use Graph '
                     'Convolutional Networks for incorporating the prior '
                     'knowledge into a deep reinforcement learning framework. '
                     'The agent uses the features from',
         'author': ['W Yang', 'X Wang', 'A Farhadi', 'A Gupta'],
         'cites': '42',
         'eprint': 'https://arxiv.org/pdf/1810.06543.pdf).',
         'gsrank': '1',
         'title': 'Visual semantic navigation using scene priors',
         'url': 'https://arxiv.org/abs/1810.06543',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=10385662033870004027&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVisual%2BSemantic%2BNavigation%2Busing%2BScene%2BPriors%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Ox8VHphIIZAJ&ei=no0qX7HBOouayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Ox8VHphIIZAJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqQAFQi2ieHV2mgjmgehbSdiS32Go4G&scisig=AAGBfm0AAAAAXyqQAGgoIdGAn3Dm-vrBAGonrnw1QMKp&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
42
-------------------------------------------------
2020-08-05 10:44:56
Got the results of the query
{'bib': {'abstract': 'We consider the dictionary learning problem, where the '
                     'aim is to model the given data as a linear combination '
                     'of a few columns of a matrix known as a dictionary, '
                     'where the sparse weights forming the linear combination '
                     'are known as coefficients. Since the dictionary and '
                     'coefficients, parameterizing the linear model are '
                     'unknown, the corresponding optimization is inherently '
                     'non-convex. This was a major challenge until recently, '
                     'when provable algorithms for dictionary learning were '
                     'proposed. Yet, these provide guarantees only on the '
                     'recovery of',
         'author': ['S Rambhatla', 'X Li', 'J Haupt'],
         'cites': '4',
         'eprint': 'https://arxiv.org/pdf/1902.11261',
         'gsrank': '1',
         'title': 'NOODL: Provable online dictionary learning and sparse '
                  'coding',
         'url': 'https://arxiv.org/abs/1902.11261',
         'venue': 'arXiv preprint arXiv:1902.11261',
         'year': '2019'},
 'citations_link': '/scholar?cites=12647512351246426060&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNOODL:%2BProvable%2BOnline%2BDictionary%2BLearning%2Band%2BSparse%2BCoding%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=zJ9oXHn9hK8J&ei=ro0qX5iiOZ32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:zJ9oXHn9hK8J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqQDnhIMAv_-ZYOmP40gdkvkx8ImSGH&scisig=AAGBfm0AAAAAXyqQDmzeDbixrTfcyy2P_jF5VVLYldTU&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
4
-------------------------------------------------
2020-08-05 10:45:11
Got the results of the query
{'bib': {'abstract': 'Stochastic descent methods (of the gradient and mirror '
                     'varieties) have become increasingly popular in '
                     'optimization. In fact, it is now widely recognized that '
                     'the success of deep learning is not only due to the '
                     'special deep architecture of the models, but also due to '
                     'the behavior of the stochastic descent methods used, '
                     'which play a key role in reaching" good" solutions that '
                     'generalize well to unseen data. In an attempt to shed '
                     'some light on why this is the case, we revisit some '
                     'minimax properties of stochastic gradient descent (SGD) '
                     'for the square loss of',
         'author': ['N Azizan', 'B Hassibi'],
         'cites': '18',
         'eprint': 'https://arxiv.org/pdf/1806.00952',
         'gsrank': '1',
         'title': 'Stochastic gradient/mirror descent: Minimax optimality and '
                  'implicit regularization',
         'url': 'https://arxiv.org/abs/1806.00952',
         'venue': 'arXiv preprint arXiv:1806.00952',
         'year': '2018'},
 'citations_link': '/scholar?cites=11983430360306499226&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStochastic%2BGradient/Mirror%2BDescent:%2BMinimax%2BOptimality%2Band%2BImplicit%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=mj6z-GuyTaYJ&ei=vo0qX5iIBZmG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:mj6z-GuyTaYJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqQIOZcgx-wFmH9-TgemX01NG2gpaa0&scisig=AAGBfm0AAAAAXyqQIFhw_XfPX5e1reRIwfNkcnMAvoTl&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
18
-------------------------------------------------
2020-08-05 10:45:28
Got the results of the query
{'bib': {'abstract': 'While many active learning papers assume that the '
                     'learner can simply ask for a label and receive it, real '
                     'annotation often presents a mismatch between the form of '
                     'a label (say, one among many classes), and the form of '
                     'an annotation (typically yes/no binary feedback). To '
                     'annotate examples corpora for multiclass classification, '
                     'we might need to ask multiple yes/no questions, '
                     'exploiting a label hierarchy if one is available. To '
                     'address this more realistic setting, we propose active '
                     'learning with partial feedback (ALPF), where the learner',
         'author': ['P Hu', 'ZC Lipton', 'A Anandkumar'],
         'cites': '6',
         'eprint': 'https://arxiv.org/pdf/1802.07427',
         'gsrank': '1',
         'title': 'Active learning with partial feedback',
         'url': 'https://arxiv.org/abs/1802.07427',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=2828167692054854631&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DActive%2BLearning%2Bwith%2BPartial%2BFeedback%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=53sqGoirPycJ&ei=040qX83WIIbuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:53sqGoirPycJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqQNJGtv8K8xImNmtVpFr5_SYQ3g1Yd&scisig=AAGBfm0AAAAAXyqQNNqhEsbbAY99V0lO8OMeI6pYF7RC&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 10:45:49
Got the results of the query
{'bib': {'abstract': 'This paper establishes risk convergence and asymptotic '
                     'weight matrix alignment---a form of implicit '
                     'regularization---of gradient flow and gradient descent '
                     'when applied to deep linear networks on linearly '
                     'separable data. In more detail, for gradient flow '
                     'applied to strictly decreasing loss functions (with '
                     'similar results for gradient descent with particular '
                     'decreasing step sizes):(i) the risk converges to 0;(ii) '
                     'the normalized i-th weight matrix asymptotically equals '
                     'its rank-1 approximation $ u_iv_i^{\\top} $;(iii) these '
                     'rank-1 matrices are aligned',
         'author': ['Z Ji', 'M Telgarsky'],
         'cites': '49',
         'eprint': 'https://arxiv.org/pdf/1810.02032',
         'gsrank': '1',
         'title': 'Gradient descent aligns the layers of deep linear networks',
         'url': 'https://arxiv.org/abs/1810.02032',
         'venue': 'arXiv preprint arXiv:1810.02032',
         'year': '2018'},
 'citations_link': '/scholar?cites=6734207111249111403&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGradient%2Bdescent%2Baligns%2Bthe%2Blayers%2Bof%2Bdeep%2Blinear%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=a6npNbC1dF0J&ei=7Y0qX7mbCZmG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:a6npNbC1dF0J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqQTbyZb9k9TPX3Ya96FjGFQCUrOvyO&scisig=AAGBfm0AAAAAXyqQTb_o38aI8kyYW18uM7Ka9FXNRkpG&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
49
-------------------------------------------------
2020-08-05 10:46:14
Got the results of the query
{'bib': {'abstract': 'We present an efficient coresets-based neural network '
                     'compression algorithm that sparsifies the parameters of '
                     'a trained fully-connected neural network in a manner '
                     "that provably approximates the network's output. Our "
                     'approach is based on an importance sampling scheme that '
                     'judiciously defines a sampling distribution over the '
                     'neural network parameters, and as a result, retains '
                     'parameters of high importance while discarding redundant '
                     'ones. We leverage a novel, empirical notion of '
                     'sensitivity and extend traditional coreset constructions',
         'author': ['C Baykal', 'L Liebenwein', 'I Gilitschenski'],
         'cites': '17',
         'eprint': 'https://arxiv.org/pdf/1804.05345',
         'gsrank': '1',
         'title': 'Data-dependent coresets for compressing neural networks '
                  'with applications to generalization bounds',
         'url': 'https://arxiv.org/abs/1804.05345',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15061912731430801795&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DData-Dependent%2BCoresets%2Bfor%2BCompressing%2BNeural%2BNetworks%2Bwith%2BApplications%2Bto%2BGeneralization%2BBounds%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=g9Hd6tOpBtEJ&ei=_40qX5WCFM2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:g9Hd6tOpBtEJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqQX_JTAxANxkBQGBbMRgvUG-uxpJd1&scisig=AAGBfm0AAAAAXyqQX2PYavWr0n-NnX-rFOt1aLgGTijo&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
17
-------------------------------------------------
2020-08-05 10:46:32
Got the results of the query
{'bib': {'abstract': 'We identify a class of over-parameterized deep neural '
                     'networks with standard activation functions and '
                     'cross-entropy loss which provably have no bad local '
                     'valley, in the sense that from any point in parameter '
                     'space there exists a continuous path on which the '
                     'cross-entropy loss is non-increasing and gets '
                     'arbitrarily close to zero. This implies that these '
                     'networks have no sub-optimal strict local minima.',
         'author': ['Q Nguyen', 'MC Mukkamala', 'M Hein'],
         'cites': '25',
         'eprint': 'https://arxiv.org/pdf/1809.10749',
         'gsrank': '1',
         'title': 'On the loss landscape of a class of deep neural networks '
                  'with no bad local valleys',
         'url': 'https://arxiv.org/abs/1809.10749',
         'venue': 'arXiv preprint arXiv:1809.10749',
         'year': '2018'},
 'citations_link': '/scholar?cites=11262228996628138485&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2Bloss%2Blandscape%2Bof%2Ba%2Bclass%2Bof%2Bdeep%2Bneural%2Bnetworks%2Bwith%2Bno%2Bbad%2Blocal%2Bvalleys%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=9ckfQ5p5S5wJ&ei=EY4qX-nRBcmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:9ckfQ5p5S5wJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqQcnWh5uaNOU--544mpJ21JrWIj6Cx&scisig=AAGBfm0AAAAAXyqQciMxTEuMGUpNR-gb3-qh8MQzhquO&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
25
-------------------------------------------------
2020-08-05 10:46:51
Got the results of the query
{'bib': {'abstract': 'Building agents to interact with the web would allow for '
                     'significant improvements in knowledge understanding and '
                     'representation learning. However, web navigation tasks '
                     'are difficult for current deep reinforcement learning '
                     '(RL) models due to the large discrete action space and '
                     'the varying number of actions between the states. In '
                     'this work, we introduce DOM-Q-NET, a novel architecture '
                     'for RL-based web navigation to address both of these '
                     'problems. It parametrizes Q functions with separate '
                     'networks for different action categories: clicking a',
         'author': ['S Jia', 'J Kiros', 'J Ba'],
         'cites': '1',
         'eprint': 'https://arxiv.org/pdf/1902.07257',
         'gsrank': '1',
         'title': 'DOM-Q-NET: Grounded RL on Structured Language',
         'url': 'https://arxiv.org/abs/1902.07257',
         'venue': 'arXiv preprint arXiv:1902.07257',
         'year': '2019'},
 'citations_link': '/scholar?cites=10126688324952353090&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDOM-Q-NET:%2BGrounded%2BRL%2Bon%2BStructured%2BLanguage%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Qmnhx2A5iYwJ&ei=Io4qX6v-PIS0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Qmnhx2A5iYwJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqQg5c0OKYlx4e8shU3PDO8aW5vEM21&scisig=AAGBfm0AAAAAXyqQg0fzxMp8zzFAHJwo6qCiQXCNP8ME&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
1
-------------------------------------------------
2020-08-05 10:47:08
Got the results of the query
{'bib': {'abstract': 'We present a novel approach for the certification of '
                     'neural networks against adversarial perturbations which '
                     'combines scalable overapproximation methods with precise '
                     '(mixed integer) linear programming. This results in '
                     'significantly better precision than state-of-the-art '
                     'verifiers on challenging feedforward and convolutional '
                     'neural networks with piecewise linear activation '
                     'functions.',
         'author': ['G Singh', 'T Gehr', 'M Püschel'],
         'cites': '35',
         'eprint': 'https://openreview.net/pdf?id=HJgeEh09KQ',
         'gsrank': '1',
         'title': 'Boosting robustness certification of neural networks',
         'url': 'https://openreview.net/forum?id=HJgeEh09KQ',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=9553026242055019849&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBoosting%2BRobustness%2BCertification%2Bof%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=SZ2sx8Aqk4QJ&ei=M44qX_y1M8mdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:SZ2sx8Aqk4QJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqQlqv2yaw8pQmQXp485Di8cw5ASnRS&scisig=AAGBfm0AAAAAXyqQlk85iKawiLX9LYx_Hc2ejR0sm8Bt&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
35
-------------------------------------------------
2020-08-05 10:47:27
Got the results of the query
{'bib': {'abstract': 'A fundamental challenge in social cognition is how '
                     "humans learn another person's values to predict their "
                     'decision-making behavior. This form of learning is often '
                     'assumed to require simulation of the other by direct '
                     "recruitment of one's own valuation process to model the",
         'author': ['S Suzuki', 'N Harasawa', 'K Ueno', 'JL Gardner'],
         'cites': '144',
         'eprint': 'https://www.sciencedirect.com/science/article/pii/S0896627312004278',
         'gsrank': '1',
         'title': "Learning to simulate others' decisions",
         'url': 'https://www.sciencedirect.com/science/article/pii/S0896627312004278',
         'venue': 'Neuron',
         'year': '2012'},
 'citations_link': '/scholar?cites=12466667622111919908&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BTo%2BSimulate%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=JD_YrCaAAq0J&ei=R44qX4_2OoS0ywTJwKPwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:JD_YrCaAAq0J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqQs3DSNhEC7lIXHblRbsmqGB-JDG8R&scisig=AAGBfm0AAAAAXyqQs9ebmJvkU6GfBwqC7dEZTiVjKRhf&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
144
-------------------------------------------------
2020-08-05 10:47:55
Got the results of the query
{'bib': {'abstract': 'Batch Normalization (BN) improves both convergence and '
                     'generalization in training neural networks. This work '
                     'understands these phenomena theoretically. We analyze BN '
                     'by using a basic block of neural networks, consisting of '
                     'a kernel layer, a BN layer, and a nonlinear activation '
                     'function. This basic network helps us understand the '
                     'impacts of BN in three aspects. First, by viewing BN as '
                     'an implicit regularizer, BN can be decomposed into '
                     'population normalization (PN) and gamma decay as an '
                     'explicit regularization. Second',
         'author': ['P Luo', 'X Wang', 'W Shao', 'Z Peng'],
         'cites': '40',
         'eprint': 'https://arxiv.org/pdf/1809.00846',
         'gsrank': '1',
         'title': 'Towards understanding regularization in batch normalization',
         'url': 'https://arxiv.org/abs/1809.00846',
         'venue': 'arXiv preprint arXiv:1809.00846',
         'year': '2018'},
 'citations_link': '/scholar?cites=7627324369776488890&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BUnderstanding%2BRegularization%2Bin%2BBatch%2BNormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ujE1Xxqz2WkJ&ei=YY4qX536NIKTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ujE1Xxqz2WkJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqQwYjt3-arMgn5dAX-hSVbnfRdWjNA&scisig=AAGBfm0AAAAAXyqQweVIyEgF7zjJmsWQwkTKkcGx37j4&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
40
-------------------------------------------------
2020-08-05 10:48:10
Got the results of the query
{'bib': {'abstract': 'The smallest eigenvectors of the graph Laplacian are '
                     'well-known to provide a succinct representation of the '
                     'geometry of a weighted graph. In reinforcement learning '
                     '(RL), where the weighted graph may be interpreted as the '
                     'state transition process induced by a behavior policy '
                     'acting on the environment, approximating the '
                     'eigenvectors of the Laplacian provides a promising '
                     'approach to state representation learning. However, '
                     'existing methods for performing this approximation are '
                     'ill-suited in general RL settings for two main reasons',
         'author': ['Y Wu', 'G Tucker', 'O Nachum'],
         'cites': '9',
         'eprint': 'https://arxiv.org/pdf/1810.04586',
         'gsrank': '1',
         'title': 'The Laplacian in RL: Learning representations with '
                  'efficient approximations',
         'url': 'https://arxiv.org/abs/1810.04586',
         'venue': 'arXiv preprint arXiv:1810.04586',
         'year': '2018'},
 'citations_link': '/scholar?cites=5981331586225750792&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BLaplacian%2Bin%2BRL:%2BLearning%2BRepresentations%2Bwith%2BEfficient%2BApproximations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=CEPPIGP1AVMJ&ei=do4qX6CqGM2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:CEPPIGP1AVMJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqQ1tFpqK-JEstaFb4QA19swbc1PBwm&scisig=AAGBfm0AAAAAXyqQ1hzCV3zDosFup4oitiWu5mIJfU0k&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 10:48:30
Got the results of the query
{'bib': {'abstract': 'As shown in recent research, deep neural networks can '
                     'perfectly fit randomly labeled data, but with very poor '
                     'accuracy on held out data. This phenomenon indicates '
                     'that loss functions such as cross-entropy are not a '
                     'reliable indicator of generalization. This leads to the '
                     'crucial question of how generalization gap should be '
                     'predicted from the training data and network parameters. '
                     'In this paper, we propose such a measure, and conduct '
                     'extensive empirical studies on how well it can predict '
                     'the generalization gap. Our measure is based on the',
         'author': ['Y Jiang', 'D Krishnan', 'H Mobahi', 'S Bengio'],
         'cites': '34',
         'eprint': 'https://arxiv.org/pdf/1810.00113',
         'gsrank': '1',
         'title': 'Predicting the generalization gap in deep networks with '
                  'margin distributions',
         'url': 'https://arxiv.org/abs/1810.00113',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=13633337648471293543&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPredicting%2Bthe%2BGeneralization%2BGap%2Bin%2BDeep%2BNetworks%2Bwith%2BMargin%2BDistributions%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Zz4X2FtYM70J&ei=iI4qX7SnDs2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Zz4X2FtYM70J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqQ6k7j7eLJaEd5u4xu5pdMh0A9mKMN&scisig=AAGBfm0AAAAAXyqQ6pSErlc8sv_ZbYnEs9J03PprEm3A&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
34
-------------------------------------------------
2020-08-05 10:48:50
Got the results of the query
{'bib': {'abstract': 'We consider a problem of learning the reward and policy '
                     'from expert examples under unknown dynamics. Our '
                     'proposed method builds on the framework of generative '
                     'adversarial networks and introduces the '
                     'empowerment-regularized maximum-entropy inverse '
                     'reinforcement learning to learn near-optimal rewards and '
                     'policies. Empowerment-based regularization prevents the '
                     'policy from overfitting to expert demonstrations, which '
                     'advantageously leads to more generalized behaviors that '
                     'result in learning near-optimal',
         'author': ['AH Qureshi', 'B Boots', 'MC Yip'],
         'cites': '19',
         'eprint': 'https://arxiv.org/pdf/1809.06404',
         'gsrank': '1',
         'title': 'Adversarial imitation via variational inverse reinforcement '
                  'learning',
         'url': 'https://arxiv.org/abs/1809.06404',
         'venue': 'arXiv preprint arXiv:1809.06404',
         'year': '2018'},
 'citations_link': '/scholar?cites=17015599061555307750&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BImitation%2Bvia%2BVariational%2BInverse%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=5mR5qDGNI-wJ&ei=mo4qX-6wG4KTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:5mR5qDGNI-wJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqQ-vycHrszse4DtgfDKn-04HKHND5J&scisig=AAGBfm0AAAAAXyqQ-q6etGYLoHfvIunGkgpDsXruRxuX&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
19
-------------------------------------------------
2020-08-05 10:49:06
Got the results of the query
{'bib': {'abstract': 'Object-based factorizations provide a useful level of '
                     'abstraction for interacting with the world. Building '
                     'explicit object representations, however, often requires '
                     'supervisory signals that are difficult to obtain in '
                     'practice. We present a paradigm for learning '
                     'object-centric representations for physical scene '
                     'understanding without direct supervision of object '
                     'properties. Our model, Object-Oriented Prediction and '
                     'Planning (O2P2), jointly learns a perception function to '
                     'map from image observations to object representations, a '
                     'pairwise',
         'author': ['M Janner', 'S Levine', 'WT Freeman'],
         'cites': '38',
         'eprint': 'https://arxiv.org/pdf/1812.10972',
         'gsrank': '1',
         'title': 'Reasoning about physical interactions with object-oriented '
                  'prediction and planning',
         'url': 'https://arxiv.org/abs/1812.10972',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15432976583597993232&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReasoning%2BAbout%2BPhysical%2BInteractions%2Bwith%2BObject-Oriented%2BPrediction%2Band%2BPlanning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=EK1HdG_yLNYJ&ei=q44qX96TAp32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:EK1HdG_yLNYJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqRDJejc_LDOU3CWdjZ0UfUFNnsfKyW&scisig=AAGBfm0AAAAAXyqRDGReup2F1zOyz7j6uwie2raL6lce&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
38
-------------------------------------------------
2020-08-05 10:49:25
Got the results of the query
{'bib': {'abstract': 'Layout is important for graphic design and scene '
                     'generation. We propose a novel Generative Adversarial '
                     'Network, called LayoutGAN, that synthesizes layouts by '
                     'modeling geometric relations of different types of 2D '
                     'elements. The generator of LayoutGAN takes as input a '
                     'set of randomly-placed 2D graphic elements and uses '
                     'self-attention modules to refine their labels and '
                     'geometric parameters jointly to produce a realistic '
                     'layout. Accurate alignment is critical for good layouts. '
                     'We thus propose a novel differentiable wireframe',
         'author': ['J Li', 'J Yang', 'A Hertzmann', 'J Zhang', 'T Xu'],
         'cites': '26',
         'eprint': 'https://arxiv.org/pdf/1901.06767',
         'gsrank': '1',
         'title': 'Layoutgan: Generating graphic layouts with wireframe '
                  'discriminators',
         'url': 'https://arxiv.org/abs/1901.06767',
         'venue': 'arXiv preprint arXiv:1901.06767',
         'year': '2019'},
 'citations_link': '/scholar?cites=79761969946922777&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLayoutGAN:%2BGenerating%2BGraphic%2BLayouts%2Bwith%2BWireframe%2BDiscriminators%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=GY9FzRZfGwEJ&ei=vI4qX-KAG8yXygThnKGgCw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:GY9FzRZfGwEJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqRHazXlSxptqelhfsDODCjM8alwO8R&scisig=AAGBfm0AAAAAXyqRHY5WTQa1NdEAnfsajxxhsPmL1rIK&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
26
-------------------------------------------------
2020-08-05 10:49:42
Got the results of the query
{'bib': {'abstract': 'The quality of the representations achieved by '
                     'embeddings is determined by how well the geometry of the '
                     'embedding space matches the structure of the data. '
                     'Euclidean space has been the workhorse for embeddings; '
                     'recently hyperbolic and spherical spaces have gained '
                     'popularity due to their ability to better embed new '
                     'types of structured data---such as hierarchical '
                     'data---but most data is not structured so uniformly. We '
                     'address this problem by proposing learning embeddings in '
                     'a product manifold combining multiple copies of these',
         'author': ['A Gu', 'F Sala', 'B Gunel', 'C Ré'],
         'cites': '29',
         'eprint': 'https://openreview.net/pdf?id=HJxeWnCcF7',
         'gsrank': '1',
         'title': 'Learning mixed-curvature representations in product spaces',
         'url': 'https://openreview.net/forum?id=HJxeWnCcF7',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1341296966114816513&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BMixed-Curvature%2BRepresentations%2Bin%2BProduct%2BSpaces%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ATKV83s-nRIJ&ei=zY4qX4DSAc2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ATKV83s-nRIJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqRMDgIhIO8NWyy_45JrcPkb1ZW-kLH&scisig=AAGBfm0AAAAAXyqRMBionZZzcVuD3S2xl6ZRmwZkB3kI&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
29
-------------------------------------------------
2020-08-05 10:50:00
Got the results of the query
{'bib': {'abstract': "We've seen tremendous success of image generating models "
                     'these years. Generating images through a neural network '
                     'is usually pixel-based, which is fundamentally different '
                     'from how humans create artwork using brushes. To imitate '
                     'human drawing, interactions between the environment and '
                     'the agent is required to allow trials. However, the '
                     'environment is usually non-differentiable, leading to '
                     'slow convergence and massive computation. In this paper '
                     'we try to address the discrete nature of software '
                     'environment with an intermediate',
         'author': ['N Zheng', 'Y Jiang', 'D Huang'],
         'cites': '13',
         'eprint': 'https://openreview.net/pdf?id=HJxwDiActX',
         'gsrank': '1',
         'title': 'Strokenet: A neural painting environment',
         'url': 'https://openreview.net/forum?id=HJxwDiActX&noteId=H1x_SWqKn7',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=16441956279595179741&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStrokeNet:%2BA%2BNeural%2BPainting%2BEnvironment%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=3R7GgR-QLeQJ&ei=544qX6W9EYbuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:3R7GgR-QLeQJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqRRs1U7L1UkD9RwfC4KQ3JuProxGyH&scisig=AAGBfm0AAAAAXyqRRjmbR5Ep2xuxtHu7jyUARtFPmcqe&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
13
-------------------------------------------------
2020-08-05 10:50:22
Got the results of the query
{'bib': {'abstract': 'Recent advances in conditional image generation tasks, '
                     'such as image-to-image translation and image inpainting, '
                     'are largely accounted to the success of conditional GAN '
                     'models, which are often optimized by the joint use of '
                     'the GAN loss with the reconstruction loss. However, we '
                     'reveal that this training recipe shared by almost all '
                     'existing methods causes one critical side effect: lack '
                     'of diversity in output samples. In order to accomplish '
                     'both training stability and multimodal output '
                     'generation, we propose novel training schemes with',
         'author': ['S Lee', 'J Ha', 'G Kim'],
         'cites': '6',
         'eprint': 'https://arxiv.org/pdf/1902.09225',
         'gsrank': '1',
         'title': 'Harmonizing maximum likelihood with GANs for multimodal '
                  'conditional generation',
         'url': 'https://arxiv.org/abs/1902.09225',
         'venue': 'arXiv preprint arXiv:1902.09225',
         'year': '2019'},
 'citations_link': '/scholar?cites=905119799608670927&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHarmonizing%2BMaximum%2BLikelihood%2Bwith%2BGANs%2Bfor%2BMultimodal%2BConditional%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=z_rYhqmhjwwJ&ei=-I4qX9WXNMmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:z_rYhqmhjwwJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqRWz7IV0seFT7INdsegtbFR6nSJvDr&scisig=AAGBfm0AAAAAXyqRW-1S3dF9HUmF5TdP6Moh-C_rhOLP&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 10:50:43
Got the results of the query
{'bib': {'abstract': 'Many machine learning algorithms represent input data '
                     'with vector embeddings or discrete codes. When inputs '
                     'exhibit compositional structure (eg objects built from '
                     'parts or procedures from subroutines), it is natural to '
                     'ask whether this compositional structure is reflected in '
                     "the the inputs' learned representations. While the "
                     'assessment of compositionality in languages has received '
                     'significant attention in linguistics and adjacent '
                     'fields, the machine learning literature lacks '
                     'general-purpose tools for producing graded measurements '
                     'of compositional',
         'author': ['J Andreas'],
         'cites': '29',
         'eprint': 'https://arxiv.org/pdf/1902.07181',
         'gsrank': '1',
         'title': 'Measuring compositionality in representation learning',
         'url': 'https://arxiv.org/abs/1902.07181',
         'venue': 'arXiv preprint arXiv:1902.07181',
         'year': '2019'},
 'citations_link': '/scholar?cites=36884338001216785&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeasuring%2BCompositionality%2Bin%2BRepresentation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=EYVWKBwKgwAJ&ei=Do8qX6GGDs2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:EYVWKBwKgwAJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqRcRHz4TPjd4lgbf-2U6xXkfRBcV3V&scisig=AAGBfm0AAAAAXyqRcYKeEGnARhqWLuBKu2EAi6j_9xDX&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
29
-------------------------------------------------
2020-08-05 10:51:06
Got the results of the query
{'bib': {'abstract': 'In this paper we establish rigorous benchmarks for image '
                     'classifier robustness. Our first benchmark, ImageNet-C, '
                     'standardizes and expands the corruption robustness '
                     'topic, while showing which classifiers are preferable in '
                     'safety-critical applications. Then we propose a new '
                     'dataset called ImageNet-P which enables researchers to '
                     "benchmark a classifier's robustness to common "
                     'perturbations. Unlike recent robustness research, this '
                     'benchmark evaluates performance on common corruptions '
                     'and perturbations not worst-case',
         'author': ['D Hendrycks', 'T Dietterich'],
         'cites': '239',
         'eprint': 'https://arxiv.org/pdf/1903.12261',
         'gsrank': '1',
         'title': 'Benchmarking neural network robustness to common '
                  'corruptions and perturbations',
         'url': 'https://arxiv.org/abs/1903.12261',
         'venue': 'arXiv preprint arXiv:1903.12261',
         'year': '2019'},
 'citations_link': '/scholar?cites=4440880036617273374&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBenchmarking%2BNeural%2BNetwork%2BRobustness%2Bto%2BCommon%2BCorruptions%2Band%2BPerturbations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=HkCyUN0soT0J&ei=IY8qX5L6CJmG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:HkCyUN0soT0J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqRgEssGvXpvWrDtARw3_Uy3Vn6igc5&scisig=AAGBfm0AAAAAXyqRgNZkGlRqzPw_JPEO-KrGCcFPyxM0&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
239
-------------------------------------------------
2020-08-05 10:51:21
Got the results of the query
{'bib': {'abstract': 'While deep neural networks have proven to be a powerful '
                     'tool for many recognition and classification tasks, '
                     'their stability properties are still not well '
                     'understood. In the past, image classifiers have been '
                     'shown to be vulnerable to so-called adversarial attacks, '
                     'which are created by additively perturbing the correctly '
                     'classified image. In this paper, we propose the ADef '
                     'algorithm to construct a different kind of adversarial '
                     'attack created by iteratively applying small '
                     'deformations to the image, found through a gradient '
                     'descent step. We',
         'author': ['R Alaifari', 'GS Alberti', 'T Gauksson'],
         'cites': '22',
         'eprint': 'https://arxiv.org/pdf/1804.07729',
         'gsrank': '1',
         'title': 'ADef: an iterative algorithm to construct adversarial '
                  'deformations',
         'url': 'https://arxiv.org/abs/1804.07729',
         'venue': 'arXiv preprint arXiv:1804.07729',
         'year': '2018'},
 'citations_link': '/scholar?cites=4601042374122210571&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DADef:%2Ban%2BIterative%2BAlgorithm%2Bto%2BConstruct%2BAdversarial%2BDeformations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=CwUTcqkv2j8J&ei=MI8qX57fL532ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:CwUTcqkv2j8J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqRnDQcR9LC2vIS9yWWaof603FuXPF6&scisig=AAGBfm0AAAAAXyqRnIRFFdQ8v0cZE2D-tM-acRDCoJUl&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
22
-------------------------------------------------
2020-08-05 10:51:48
Got the results of the query
{'bib': {'abstract': 'We identify two issues with the family of algorithms '
                     'based on the Adversarial Imitation Learning framework. '
                     'The first problem is implicit bias present in the reward '
                     'functions used in these algorithms. While these biases '
                     'might work well for some environments, they can also '
                     'lead to sub-optimal behavior in others. Secondly, even '
                     'though these algorithms can learn from few expert '
                     'demonstrations, they require a prohibitively large '
                     'number of interactions with the environment in order to '
                     'imitate the expert for many real-world applications. In '
                     'order to',
         'author': ['I Kostrikov', 'KK Agrawal', 'D Dwibedi', 'S Levine'],
         'cites': '32',
         'eprint': 'https://arxiv.org/pdf/1809.02925',
         'gsrank': '1',
         'title': 'Discriminator-actor-critic: Addressing sample inefficiency '
                  'and reward bias in adversarial imitation learning',
         'url': 'https://arxiv.org/abs/1809.02925',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=10939703062864014386&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiscriminator-Actor-Critic:%2BAddressing%2BSample%2BInefficiency%2Band%2BReward%2BBias%2Bin%2BAdversarial%2BImitation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=MixDgvmh0ZcJ&ei=TY8qX42tBYbuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:MixDgvmh0ZcJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqRrmypZgtT-iq2rgiE-9AGbBQrFybg&scisig=AAGBfm0AAAAAXyqRrsdc9Smb1_yzyRlNBd7aIUgJE6ht&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
32
-------------------------------------------------
2020-08-05 10:52:06
Got the results of the query
{'bib': {'abstract': 'Deep latent variable models have become a popular model '
                     'choice due to the scalable learning algorithms '
                     'introduced by (Kingma & Welling, 2013; Rezende et al., '
                     '2014). These approaches maximize a variational lower '
                     'bound on the intractable log likelihood of the observed '
                     'data. Burda et al.(2015) introduced a multi-sample '
                     'variational bound, IWAE, that is at least as tight as '
                     'the standard variational lower bound and becomes '
                     'increasingly tight as the number of samples increases. '
                     'Counterintuitively, the typical inference network '
                     'gradient',
         'author': ['G Tucker', 'D Lawson', 'S Gu', 'CJ Maddison'],
         'cites': '35',
         'eprint': 'https://arxiv.org/pdf/1810.04152',
         'gsrank': '1',
         'title': 'Doubly reparameterized gradient estimators for monte carlo '
                  'objectives',
         'url': 'https://arxiv.org/abs/1810.04152',
         'venue': 'arXiv preprint arXiv:1810.04152',
         'year': '2018'},
 'citations_link': '/scholar?cites=15749904107210589457&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDoubly%2BReparameterized%2BGradient%2BEstimators%2Bfor%2BMonte%2BCarlo%2BObjectives%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=EcXAxFbmktoJ&ei=XI8qX9HWBcmdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:EcXAxFbmktoJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqRys7_ieTLzPNTkzzPwo5x-GNE36aC&scisig=AAGBfm0AAAAAXyqRykE8lxMzF2JYOIqUV6kcxpfY4E2D&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
35
-------------------------------------------------
2020-08-05 10:52:34
Got the results of the query
{'bib': {'abstract': 'Recurrent neural networks (RNNs) have shown excellent '
                     'performance in processing sequence data. However, they '
                     'are both complex and memory intensive due to their '
                     'recursive nature. These limitations make RNNs difficult '
                     'to embed on mobile devices requiring real-time processes '
                     'with limited hardware resources. To address the above '
                     'issues, we introduce a method that can learn binary and '
                     'ternary weights during the training phase to facilitate '
                     'hardware implementations of RNNs. As a result, using '
                     'this approach replaces all',
         'author': ['A Ardakani', 'Z Ji', 'SC Smithson', 'BH Meyer'],
         'cites': '10',
         'eprint': 'https://arxiv.org/pdf/1809.11086',
         'gsrank': '1',
         'title': 'Learning recurrent binary/ternary weights',
         'url': 'https://arxiv.org/abs/1809.11086',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=14324986620118227094&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BRecurrent%2BBinary/Ternary%2BWeights%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ltheGG-TzMYJ&ei=eY8qX8LUJIbuygTggJDwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ltheGG-TzMYJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqR20a9r-_SJGC0f56xtae0Vsr7QTfr&scisig=AAGBfm0AAAAAXyqR23EHoHb4lmD1b4uEqxw94vzloV0A&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
10
-------------------------------------------------
2020-08-05 10:52:52
Got the results of the query
{'bib': {'abstract': 'We propose and study a method for learning interpretable '
                     'representations for the task of regression. Features are '
                     'represented as networks of multi-type expression trees '
                     'comprised of activation functions common in neural '
                     'networks in addition to other elementary functions. '
                     'Differentiable features are trained via gradient '
                     'descent, and the performance of features in a linear '
                     'model is used to weight the rate of change among '
                     'subcomponents of each representation. The search process '
                     'maintains an archive of representations with accuracy',
         'author': ['W La Cava', 'TR Singh', 'J Taggart', 'S Suri'],
         'cites': '4',
         'eprint': 'https://arxiv.org/pdf/1807.00981',
         'gsrank': '1',
         'title': 'Learning concise representations for regression by evolving '
                  'networks of trees',
         'url': 'https://arxiv.org/abs/1807.00981',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8620889637656985656&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bconcise%2Brepresentations%2Bfor%2Bregression%2Bby%2Bevolving%2Bnetworks%2Bof%2Btrees%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=OABkNHONo3cJ&ei=i48qX43zMoKTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:OABkNHONo3cJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqR60YXnAZm4_ibHHkYccYIqngYmcnU&scisig=AAGBfm0AAAAAXyqR6yZmRoP69EyhvpZDEpeGnAjFTjMW&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
4
-------------------------------------------------
2020-08-05 10:53:08
Got the results of the query
{'bib': {'abstract': 'We study the problem of learning similarity functions '
                     'over very large corpora using neural network embedding '
                     'models. These models are typically trained using SGD '
                     'with sampling of random observed and unobserved pairs, '
                     'with a number of samples that grows quadratically with '
                     'the corpus size, making it expensive to scale to very '
                     'large corpora. We propose new efficient methods to train '
                     'these models without having to sample unobserved pairs. '
                     'Inspired by matrix factorization, our approach relies on '
                     'adding a global quadratic penalty to all pairs',
         'author': ['W Krichene', 'N Mayoraz', 'S Rendle', 'L Zhang'],
         'cites': '11',
         'eprint': 'https://arxiv.org/pdf/1807.07187',
         'gsrank': '1',
         'title': 'Efficient training on very large corpora via gramian '
                  'estimation',
         'url': 'https://arxiv.org/abs/1807.07187',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=14969655596866173703&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BTraining%2Bon%2BVery%2BLarge%2BCorpora%2Bvia%2BGramian%2BEstimation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=B3sl5XPmvs8J&ei=nI8qX53ZKM2iygSfppLwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:B3sl5XPmvs8J:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqR--Ird-qAOjS0suDGhXa2kZDgmnHD&scisig=AAGBfm0AAAAAXyqR-wdQjsXqpME3l2ftDG3383lTQC5Y&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
11
-------------------------------------------------
2020-08-05 10:53:24
Got the results of the query
{'bib': {'abstract': 'Variational Autoencoder (VAE), a simple and effective '
                     'deep generative model, has led to a number of impressive '
                     'empirical successes and spawned many advanced variants '
                     'and theoretical investigations. However, recent studies '
                     'demonstrate that, when equipped with expressive '
                     'generative distributions (aka. decoders), VAE suffers '
                     'from learning uninformative latent representations with '
                     'the observation called KL Varnishing, in which case VAE '
                     'collapses into an unconditional generative model. In '
                     'this work, we introduce mutual',
         'author': ['X Ma', 'C Zhou', 'E Hovy'],
         'cites': '11',
         'eprint': 'https://arxiv.org/pdf/1901.01498',
         'gsrank': '1',
         'title': 'Mae: Mutual posterior-divergence regularization for '
                  'variational autoencoders',
         'url': 'https://arxiv.org/abs/1901.01498',
         'venue': 'arXiv preprint arXiv:1901.01498',
         'year': '2019'},
 'citations_link': '/scholar?cites=1538532826408236978&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMAE:%2BMutual%2BPosterior-Divergence%2BRegularization%2Bfor%2BVariational%2BAutoEncoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=sjdXQXT3WRUJ&ei=qo8qX-bJKZ32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:sjdXQXT3WRUJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqSCi6VpSCxO0wU8rkDody1h9Fn31rv&scisig=AAGBfm0AAAAAXyqSCn8UwrpKeClXpuxByLI6rggREV__&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
11
-------------------------------------------------
2020-08-05 10:53:38
Got the results of the query
{'bib': {'abstract': 'In this paper, we propose a residual non-local attention '
                     'network for high-quality image restoration. Without '
                     'considering the uneven distribution of information in '
                     'the corrupted images, previous methods are restricted by '
                     'local convolutional operation and equal treatment of '
                     'spatial-and channel-wise features. To address this '
                     'issue, we design local and non-local attention blocks to '
                     'extract features that capture the long-range '
                     'dependencies between pixels and pay more attention to '
                     'the challenging parts. Specifically, we design trunk',
         'author': ['Y Zhang', 'K Li', 'K Li', 'B Zhong', 'Y Fu'],
         'cites': '74',
         'eprint': 'https://arxiv.org/pdf/1903.10082',
         'gsrank': '1',
         'title': 'Residual non-local attention networks for image restoration',
         'url': 'https://arxiv.org/abs/1903.10082',
         'venue': 'arXiv preprint arXiv:1903.10082',
         'year': '2019'},
 'citations_link': '/scholar?cites=5425381515618577679&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DResidual%2BNon-local%2BAttention%2BNetworks%2Bfor%2BImage%2BRestoration%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=D21qWb7TSksJ&ei=yI8qX8TbDouayAT2uYHwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:D21qWb7TSksJ:scholar.google.com/&output=citation&scisdr=CgX8qrcrGAA:AAGBfm0AAAAAXyqSJ3_eZ-kEdnJO4w2ttrzbCNGDLXvQ&scisig=AAGBfm0AAAAAXyqSJ8lYD4fypSmUTBQnkBdH6KzCOsYn&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
74
-------------------------------------------------
2020-08-05 10:54:08
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Got the results of the query
{'bib': {'abstract': 'Stochastic gradient Markov chain Monte Carlo (SG-MCMC) '
                     'has become increasingly popular for simulating posterior '
                     'samples in large-scale Bayesian modeling. However, '
                     'existing SG-MCMC schemes are not tailored to any '
                     'specific probabilistic model, even a simple modification '
                     'of the underlying dynamical system requires significant '
                     'physical intuition. This paper presents the first '
                     'meta-learning algorithm that allows automated design for '
                     'the underlying continuous dynamics of an SG-MCMC '
                     'sampler. The learned sampler',
         'author': ['W Gong', 'Y Li', 'JM Hernández-Lobato'],
         'cites': '13',
         'eprint': 'https://arxiv.org/pdf/1806.04522',
         'gsrank': '1',
         'title': 'Meta-learning for stochastic gradient MCMC',
         'url': 'https://arxiv.org/abs/1806.04522',
         'venue': 'arXiv preprint arXiv:1806.04522',
         'year': '2018'},
 'citations_link': '/scholar?cites=5266885862075190072&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-Learning%2BFor%2BStochastic%2BGradient%2BMCMC%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=OMuEUcm8F0kJ&ei=ZpAqX5rKB4vrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:OMuEUcm8F0kJ:scholar.google.com/&output=citation&scisdr=CgU4ognyGAA:AAGBfm0AAAAAXyqSxDVxJGlfyBMzGNHT_Yspm2_37f3o&scisig=AAGBfm0AAAAAXyqSxDLpTTiVBrKgRpCuhClLoiWiFK80&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
13
-------------------------------------------------
2020-08-05 10:56:44
Got the results of the query
{'bib': {'abstract': 'Numerous models for grounded language understanding have '
                     'been recently proposed, including (i) generic models '
                     'that can be easily adapted to any given task and (ii) '
                     'intuitively appealing modular models that require '
                     'background knowledge to be instantiated. We compare both '
                     'types of models in how much they lend themselves to a '
                     'particular form of systematic generalization. Using a '
                     'synthetic VQA test, we evaluate which models are capable '
                     'of reasoning about all possible object pairs after '
                     'training on only a small subset of',
         'author': ['D Bahdanau', 'S Murty', 'M Noukhovitch'],
         'cites': '21',
         'eprint': 'https://arxiv.org/pdf/1811.12889',
         'gsrank': '1',
         'title': 'Systematic generalization: what is required and can it be '
                  'learned?',
         'url': 'https://arxiv.org/abs/1811.12889',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=376953749686735892&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSystematic%2BGeneralization:%2BWhat%2BIs%2BRequired%2Band%2BCan%2BIt%2BBe%2BLearned%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=FFTUKnA1OwUJ&ei=b5AqX6LzAqiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:FFTUKnA1OwUJ:scholar.google.com/&output=citation&scisdr=CgUX_2VGGAA:AAGBfm0AAAAAXyqSyi9wcYkkxsWcSs2sc1SkjM4RMiNm&scisig=AAGBfm0AAAAAXyqSygf8VW89ujn4iY_PkkDrk7JDn6mW&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
21
-------------------------------------------------
2020-08-05 10:56:50
Got the results of the query
{'bib': {'abstract': 'In lifelong learning, the learner is presented with a '
                     'sequence of tasks, incrementally building a data-driven '
                     'prior which may be leveraged to speed up learning of a '
                     'new task. In this work, we investigate the efficiency of '
                     'current lifelong approaches, in terms of sample '
                     'complexity',
         'author': ['A Chaudhry', 'MA Ranzato', 'M Rohrbach'],
         'cites': '108',
         'eprint': 'https://arxiv.org/pdf/1812.00420',
         'gsrank': '1',
         'title': 'Efficient lifelong learning with a-gem',
         'url': 'https://arxiv.org/abs/1812.00420',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=14191909055509326948&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BLifelong%2BLearning%2Bwith%2BA-GEM%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ZGxIEhjK88QJ&ei=d5AqX7S2KZqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ZGxIEhjK88QJ:scholar.google.com/&output=citation&scisdr=CgU4ogn3GAA:AAGBfm0AAAAAXyqS1lIWZZdsR569OVthDlod0t66tRbz&scisig=AAGBfm0AAAAAXyqS1uyQywxUitmds6UFdHtibnDAhgk5&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
108
-------------------------------------------------
2020-08-05 10:57:02
Got the results of the query
-------------------------------------------------
2020-08-05 10:57:20
Got the results of the query
{'bib': {'abstract': 'Weight pruning has been introduced as an efficient model '
                     'compression technique. Even though pruning removes '
                     'significant amount of weights in a network, memory '
                     'requirement reduction was limited since conventional '
                     'sparse matrix formats require significant amount of '
                     'memory to store index-related information. Moreover, '
                     'computations associated with such sparse matrix formats '
                     'are slow because sequential sparse matrix decoding '
                     'process does not utilize highly parallel computing '
                     'systems efficiently. As an attempt to compress index',
         'author': ['D Ahn', 'D Lee', 'T Kim', 'JJ Kim'],
         'cites': '5',
         'eprint': 'https://openreview.net/pdf?id=HkfYOoCcYX',
         'gsrank': '1',
         'title': 'Double Viterbi: Weight encoding for high compression ratio '
                  'and fast on-chip reconstruction for deep neural network',
         'url': 'https://openreview.net/forum?id=HkfYOoCcYX',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=16044429059282632198&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDouble%2BViterbi:%2BWeight%2BEncoding%2Bfor%2BHigh%2BCompression%2BRatio%2Band%2BFast%2BOn-Chip%2BReconstruction%2Bfor%2BDeep%2BNeural%2BNetwork%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Bv5-7jhDqd4J&ei=opAqX93TF6iBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Bv5-7jhDqd4J:scholar.google.com/&output=citation&scisdr=CgU4ogv-GAA:AAGBfm0AAAAAXyqTAyte5A-kZ-7tTf_x7bT9FxId9b-i&scisig=AAGBfm0AAAAAXyqTA7u7orbNCYpNnqeX93z0G9_tMJpV&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 10:57:47
Got the results of the query
{'bib': {'abstract': '• Short latent codes facilitate disentanglement, at the '
                     'cost of blurry reconstructions. Conversely, if the '
                     'latent dimension is too big the decoder can just ignore '
                     'the disentangled components [3].• To obtain both good '
                     'reconstruction and good disentangling, we employ a '
                     'teacher-student paradigm where the Jacobian of a teacher '
                     'autoencoder with a short latent code is transferred to a '
                     'student with a long latent code.',
         'author': ['J Lezama'],
         'cites': '5',
         'eprint': 'http://postersession.ai.s3.amazonaws.com/1667efa3-fd77-4b93-86d3-10cf56ad673b.pdf',
         'gsrank': '1',
         'title': 'Overcoming the Disentanglement vs Reconstruction Trade-off '
                  'via Jacobian Supervision.',
         'url': 'http://postersession.ai.s3.amazonaws.com/1667efa3-fd77-4b93-86d3-10cf56ad673b.pdf',
         'venue': 'ICLR (Poster)',
         'year': '2019'},
 'citations_link': '/scholar?cites=72617481773116679&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOvercoming%2Bthe%2BDisentanglement%2Bvs%2BReconstruction%2BTrade-off%2Bvia%2BJacobian%2BSupervision%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=B-2RBTf9AQEJ&ei=rpAqX6OUO6iBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:B-2RBTf9AQEJ:scholar.google.com/&output=citation&scisdr=CgU4ognyGAA:AAGBfm0AAAAAXyqTDFCZA_ZEEmvVW1N9-gulyeQh8sKJ&scisig=AAGBfm0AAAAAXyqTDKoO6QDSwWBZClDmm68MNs3gVZRF&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 10:57:56
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://185.141.227.144:5566
Got the results of the query
{'bib': {'abstract': 'We study the problem of learning representations of '
                     'entities and relations in knowledge graphs for '
                     'predicting missing links. The success of such a task '
                     'heavily relies on the ability of modeling and inferring '
                     'the patterns of (or between) the relations. In this '
                     'paper, we present a new approach for knowledge graph '
                     'embedding called RotatE, which is able to model and '
                     'infer various relation patterns including: '
                     'symmetry/antisymmetry, inversion, and composition. '
                     'Specifically, the RotatE model defines each relation as '
                     'a rotation from the source entity to',
         'author': ['Z Sun', 'ZH Deng', 'JY Nie', 'J Tang'],
         'cites': '131',
         'eprint': 'https://arxiv.org/pdf/1902.10197',
         'gsrank': '1',
         'title': 'Rotate: Knowledge graph embedding by relational rotation in '
                  'complex space',
         'url': 'https://arxiv.org/abs/1902.10197',
         'venue': 'arXiv preprint arXiv:1902.10197',
         'year': '2019'},
 'citations_link': '/scholar?cites=9820389801132772086&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRotatE:%2BKnowledge%2BGraph%2BEmbedding%2Bby%2BRelational%2BRotation%2Bin%2BComplex%2BSpace%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=9kpjVH4ISYgJ&ei=AZEqX7eHB8mdywSUkqn4Cg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:9kpjVH4ISYgJ:scholar.google.com/&output=citation&scisdr=CgU4ogjLGAA:AAGBfm0AAAAAXyqTZ5qgnPJznnV-AHu7eRcLaZV2X-2H&scisig=AAGBfm0AAAAAXyqTZ-fyTYVMah-1wvR8Nob2tNEsp34a&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
131
-------------------------------------------------
2020-08-05 10:59:27
Got the results of the query
{'bib': {'abstract': 'Behavioral skills or policies for autonomous agents are '
                     'conventionally learned from reward functions, via '
                     'reinforcement learning, or from demonstrations, via '
                     'imitation learning. However, both modes of task '
                     'specification have their disadvantages: reward functions '
                     'require manual engineering, while demonstrations require '
                     'a human expert to be able to actually perform the task '
                     'in order to generate the demonstration. Instruction '
                     'following from natural language instructions provides an '
                     'appealing alternative: in the same way that we',
         'author': ['JD Co-Reyes', 'A Gupta', 'S Sanjeev', 'N Altieri'],
         'cites': '12',
         'eprint': 'https://arxiv.org/pdf/1811.07882',
         'gsrank': '1',
         'title': 'Guiding policies with language via meta-learning',
         'url': 'https://arxiv.org/abs/1811.07882',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=18060553357406887446&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGuiding%2BPolicies%2Bwith%2BLanguage%2Bvia%2BMeta-Learning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Ft4585b5o_oJ&ei=GZEqX4W4NJ32ygSJ04PwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Ft4585b5o_oJ:scholar.google.com/&output=citation&scisdr=CgWW_y3PGAA:AAGBfm0AAAAAXyqTemwphgAng-rYONka67gH3_m_ADH3&scisig=AAGBfm0AAAAAXyqTeiMN63qsoxaaE4i6QUSRHfcZ5GaQ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 10:59:46
Got the results of the query
{'bib': {'abstract': 'Adam is shown not being able to converge to the optimal '
                     'solution in certain cases. Researchers recently propose '
                     'several algorithms to avoid the issue of non-convergence '
                     'of Adam, but their efficiency turns out to be '
                     'unsatisfactory in practice. In this paper, we provide '
                     'new insight into the non-convergence issue of Adam as '
                     'well as other adaptive learning rate methods. We argue '
                     'that there exists an inappropriate correlation between '
                     'gradient $ g_t $ and the second-moment term $ v_t $ in '
                     'Adam ($ t $ is the timestep), which results in that a',
         'author': ['Z Zhou', 'Q Zhang', 'G Lu', 'H Wang', 'W Zhang'],
         'cites': '17',
         'eprint': 'https://arxiv.org/pdf/1810.00143',
         'gsrank': '1',
         'title': 'AdaShift: Decorrelation and convergence of adaptive '
                  'learning rate methods',
         'url': 'https://arxiv.org/abs/1810.00143',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=9276200117186011487&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaShift:%2BDecorrelation%2Band%2BConvergence%2Bof%2BAdaptive%2BLearning%2BRate%2BMethods%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=XyFP8tuuu4AJ&ei=K5EqX_vyIZqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:XyFP8tuuu4AJ:scholar.google.com/&output=citation&scisdr=CgU4ogsHGAA:AAGBfm0AAAAAXyqTiNBFQTVH6IQ8hXp_nAi5U0U6qhJc&scisig=AAGBfm0AAAAAXyqTiFVRgGixK38klTj4dbqZ5WKdojDc&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
17
-------------------------------------------------
2020-08-05 11:00:00
Got the results of the query
{'bib': {'abstract': 'Visual Active Tracking (VAT) aims at following a target '
                     'object by autonomously controlling the motion system of '
                     'a tracker given visual observations. Previous work has '
                     'shown that the tracker can be trained in a simulator via '
                     'reinforcement learning and deployed in real-world '
                     'scenarios. However, during training, such a method '
                     'requires manually specifying the moving path of the '
                     'target object to be tracked, which cannot ensure the '
                     "tracker's generalization on the unseen object moving "
                     'patterns. To learn a robust tracker for VAT, in',
         'author': ['F Zhong', 'P Sun', 'W Luo', 'T Yan'],
         'cites': '4',
         'eprint': 'https://openreview.net/pdf?id=HkgYmhR9KX',
         'gsrank': '1',
         'title': 'AD-VAT: An asymmetric dueling mechanism for learning visual '
                  'active tracking',
         'url': 'https://openreview.net/forum?id=HkgYmhR9KX',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=10828916814543014886&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAD-VAT:%2BAn%2BAsymmetric%2BDueling%2Bmechanism%2Bfor%2Blearning%2BVisual%2BActive%2BTracking%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=5ksXAXgKSJYJ&ei=OpEqX97ZFZmG6rQPsfys8Ac',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:5ksXAXgKSJYJ:scholar.google.com/&output=citation&scisdr=CgWW_y25GAA:AAGBfm0AAAAAXyqTqiOh4Oct0Ut2A8p1BgTsyEGWDeyb&scisig=AAGBfm0AAAAAXyqTqrAIiSxb63SZvjJWaf6Zy6X9sZ38&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
4
-------------------------------------------------
2020-08-05 11:00:34
Got the results of the query
-------------------------------------------------
2020-08-05 11:01:52
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://134.122.71.12:3128
Got the results of the query
{'bib': {'abstract': 'Training Generative Adversarial Networks (GANs) is '
                     'notoriously challenging. We propose and study an '
                     'architectural modification, self-modulation, which '
                     'improves GAN performance across different data sets, '
                     'architectures, losses, regularizers, and hyperparameter '
                     'settings. Intuitively, self-modulation allows the '
                     'intermediate feature maps of a generator to change as a '
                     'function of the input noise vector. While reminiscent of '
                     'other conditioning techniques, it requires no labeled '
                     'data. In a large-scale empirical study we observe a '
                     'relative decrease of',
         'author': ['T Chen', 'M Lucic', 'N Houlsby', 'S Gelly'],
         'cites': '32',
         'eprint': 'https://arxiv.org/pdf/1810.01365',
         'gsrank': '1',
         'title': 'On self modulation for generative adversarial networks',
         'url': 'https://arxiv.org/abs/1810.01365',
         'venue': 'arXiv preprint arXiv:1810.01365',
         'year': '2018'},
 'citations_link': '/scholar?cites=14481067201346722037&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BSelf%2BModulation%2Bfor%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=9djUSOUV98gJ&ei=GpIqX8DWJIvrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:9djUSOUV98gJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqUdjp0cjA7GoBXZF0oPSKWOUWrhrxd&scisig=AAGBfm0AAAAAXyqUdm5K8gf2vdaMRjgdVY4_TvhFV6Ts&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
32
-------------------------------------------------
2020-08-05 11:03:58
Got the results of the query
{'bib': {'abstract': 'When learning from a batch of logged bandit feedback, '
                     'the discrepancy between the policy to be learned and the '
                     'off-policy training data imposes statistical and '
                     'computational challenges. Unlike classical supervised '
                     'learning and online learning settings, in batch '
                     'contextual bandit learning, one only has access to a '
                     'collection of logged feedback from the actions taken by '
                     'a historical policy, and expect to learn a policy that '
                     'takes good actions in possibly unseen contexts. Such a '
                     'batch learning setting is ubiquitous in online and',
         'author': ['Y Xie', 'B Liu', 'Q Liu', 'Z Wang', 'Y Zhou', 'J Peng'],
         'cites': '4',
         'eprint': 'https://arxiv.org/pdf/1808.00232',
         'gsrank': '1',
         'title': 'Off-policy evaluation and learning from logged bandit '
                  'feedback: Error reduction via surrogate policy',
         'url': 'https://arxiv.org/abs/1808.00232',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11720089646814691493&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOff-Policy%2BEvaluation%2Band%2BLearning%2Bfrom%2BLogged%2BBandit%2BFeedback:%2BError%2BReduction%2Bvia%2BSurrogate%2BPolicy%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=pYAfJnAfpqIJ&ei=IZIqX7aENI-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:pYAfJnAfpqIJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqUfKjBwUIzioV5gB93HMWii93E3Kb6&scisig=AAGBfm0AAAAAXyqUfFUxdPl5KS2nNfWmJ2GXFNIG-Jr0&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
4
-------------------------------------------------
2020-08-05 11:04:04
Got the results of the query
{'bib': {'abstract': 'This paper concerns dictionary learning, ie, sparse '
                     'coding, a fundamental representation learning problem. '
                     'We show that a subgradient descent algorithm, with '
                     'random initialization, can provably recover orthogonal '
                     'dictionaries on a natural nonsmooth, nonconvex $\\ell_1 '
                     '$ minimization formulation of the problem, under mild '
                     'statistical assumptions on the data. This is in contrast '
                     'to previous provable methods that require either '
                     'expensive computation or delicate initialization '
                     'schemes. Our analysis develops several tools for '
                     'characterizing',
         'author': ['Y Bai', 'Q Jiang', 'J Sun'],
         'cites': '20',
         'eprint': 'https://arxiv.org/pdf/1810.10702',
         'gsrank': '1',
         'title': 'Subgradient descent learns orthogonal dictionaries',
         'url': 'https://arxiv.org/abs/1810.10702',
         'venue': 'arXiv preprint arXiv:1810.10702',
         'year': '2018'},
 'citations_link': '/scholar?cites=3757427846147866582&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSubgradient%2BDescent%2BLearns%2BOrthogonal%2BDictionaries%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=1v_N5rcQJTQJ&ei=J5IqX_GYNY-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:1v_N5rcQJTQJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqUhfQpY1XE01kyOCeMAcpe57ATGIGI&scisig=AAGBfm0AAAAAXyqUhZgX4aULA396irMWUQgRWuygv4yy&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
20
-------------------------------------------------
2020-08-05 11:04:13
Got the results of the query
{'bib': {'abstract': 'In this work, we propose a new solution for parallel '
                     'wave generation by WaveNet. In contrast to parallel '
                     'WaveNet (van den Oord et al., 2018), we distill a '
                     'Gaussian inverse autoregressive flow from the '
                     'autoregressive WaveNet by minimizing a regularized KL '
                     'divergence between their highly-peaked output '
                     'distributions. Our method computes the KL divergence in '
                     'closed-form, which simplifies the training algorithm and '
                     'provides very efficient distillation. In addition, we '
                     'introduce the first text-to-wave neural architecture for '
                     'speech synthesis, which is',
         'author': ['W Ping', 'K Peng', 'J Chen'],
         'cites': '125',
         'eprint': 'https://arxiv.org/pdf/1807.07281',
         'gsrank': '1',
         'title': 'Clarinet: Parallel wave generation in end-to-end '
                  'text-to-speech',
         'url': 'https://arxiv.org/abs/1807.07281',
         'venue': 'arXiv preprint arXiv:1807.07281',
         'year': '2018'},
 'citations_link': '/scholar?cites=1675505652651694755&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DClariNet:%2BParallel%2BWave%2BGeneration%2Bin%2BEnd-to-End%2BText-to-Speech%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=o5p2jYOXQBcJ&ei=MZIqX7nLHKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:o5p2jYOXQBcJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqUj6SE8L9KFTg1C-uGqdmc6tE4xhie&scisig=AAGBfm0AAAAAXyqUj57GEY8qROGzJHsvKl1O6b7HfiwJ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
125
-------------------------------------------------
2020-08-05 11:04:23
Got the results of the query
{'bib': {'abstract': 'In weakly-supervised temporal action localization, '
                     'previous works have failed to locate dense and integral '
                     'regions for each entire action due to the overestimation '
                     'of the most salient regions. To alleviate this issue, we '
                     'propose a marginalized average attentional network '
                     '(MAAN) to suppress the dominant response of the most '
                     'salient regions in a principled manner. The MAAN employs '
                     'a novel marginalized average aggregation (MAA) module '
                     'and learns a set of latent discriminative probabilities '
                     'in an end-to-end fashion. MAA',
         'author': ['Y Yuan', 'Y Lyu', 'X Shen', 'IW Tsang', 'DY Yeung'],
         'cites': '15',
         'eprint': 'https://arxiv.org/pdf/1905.08586',
         'gsrank': '1',
         'title': 'Marginalized average attentional network for '
                  'weakly-supervised learning',
         'url': 'https://arxiv.org/abs/1905.08586',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=3372820484309967388&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMarginalized%2BAverage%2BAttentional%2BNetwork%2Bfor%2BWeakly-Supervised%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=HOJ0Slyqzi4J&ei=OZIqX_GrPKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:HOJ0Slyqzi4J:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqUlO_eeZXEqmFBtdeSqdhbAPPHmqkZ&scisig=AAGBfm0AAAAAXyqUlBSclcAuM_JVf3s8Hx10vly7H65s&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 11:04:28
Got the results of the query
{'bib': {'abstract': 'For many evaluation metrics commonly used as benchmarks '
                     'for unconditional image generation, trivially memorizing '
                     'the training set attains a better score than models '
                     'which are considered state-of-the-art; we consider this '
                     'problematic. We clarify a necessary condition for an '
                     'evaluation metric not to behave this way: estimating the '
                     'function must require a large sample from the model. In '
                     'search of such a metric, we turn to neural network '
                     'divergences (NNDs), which are defined in terms of a '
                     'neural network trained to distinguish between',
         'author': ['I Gulrajani', 'C Raffel', 'L Metz'],
         'cites': '12',
         'eprint': 'https://arxiv.org/pdf/2001.03653',
         'gsrank': '1',
         'title': 'Towards GAN benchmarks which require generalization',
         'url': 'https://arxiv.org/abs/2001.03653',
         'venue': 'arXiv preprint arXiv:2001.03653',
         'year': '2020'},
 'citations_link': '/scholar?cites=9003774771707079711&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BGAN%2BBenchmarks%2BWhich%2BRequire%2BGeneralization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=H2w5aXPV83wJ&ei=QZIqX8XUBJqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:H2w5aXPV83wJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqUntBgZnoF_yq8ghg6hjXNudpte718&scisig=AAGBfm0AAAAAXyqUntZVviIJIIcRftDFF9aEU-Bcv3OV&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 11:04:38
Got the results of the query
{'bib': {'abstract': 'Few-shot classification aims to learn a classifier to '
                     'recognize unseen classes during training with limited '
                     'labeled examples. While significant progress has been '
                     'made, the growing complexity of network designs, '
                     'meta-learning algorithms, and differences in '
                     'implementation details make a fair comparison difficult. '
                     'In this paper, we present 1) a consistent comparative '
                     'analysis of several representative few-shot '
                     'classification algorithms, with results showing that '
                     'deeper backbones significantly reduce the performance '
                     'differences among methods on',
         'author': ['WY Chen', 'YC Liu', 'Z Kira', 'YCF Wang'],
         'cites': '203',
         'eprint': 'https://arxiv.org/pdf/1904.04232',
         'gsrank': '1',
         'title': 'A closer look at few-shot classification',
         'url': 'https://arxiv.org/abs/1904.04232',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=10436738309048088927&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BCloser%2BLook%2Bat%2BFew-shot%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=X7mFJDK-1pAJ&ei=S5IqX5i6C4vrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:X7mFJDK-1pAJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqUpaovh_ale0p_m5yzWPtr2Bcd6wDR&scisig=AAGBfm0AAAAAXyqUpcG41nPsaqJDI2YjsFqUm0uZGQds&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
203
-------------------------------------------------
2020-08-05 11:04:45
Got the results of the query
{'bib': {'abstract': 'This paper introduces a new framework for data efficient '
                     'and versatile learning. Specifically: 1) We develop '
                     'ML-PIP, a general framework for Meta-Learning '
                     'approximate Probabilistic Inference for Prediction. '
                     'ML-PIP extends existing probabilistic interpretations of '
                     'meta-learning to cover a broad class of methods. 2) We '
                     'introduce VERSA, an instance of the framework employing '
                     'a flexible and versatile amortization network that takes '
                     'few-shot learning datasets as inputs, with arbitrary '
                     'numbers of shots, and outputs a distribution over',
         'author': ['J Gordon', 'J Bronskill', 'M Bauer', 'S Nowozin'],
         'cites': '51',
         'eprint': 'https://arxiv.org/pdf/1805.09921',
         'gsrank': '1',
         'title': 'Meta-learning probabilistic inference for prediction',
         'url': 'https://arxiv.org/abs/1805.09921',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=18291407046711557858&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-Learning%2BProbabilistic%2BInference%2Bfor%2BPrediction%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=4r5RBcwh2P0J&ei=UpIqX72jKYyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:4r5RBcwh2P0J:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqUrikKWsm_DxqSxZHPiHMjhxEb6Clf&scisig=AAGBfm0AAAAAXyqUrqz1u71PgZM4ZdUA64JNtxnDSRk7&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
51
-------------------------------------------------
2020-08-05 11:04:54
Got the results of the query
{'bib': {'abstract': 'We introduce an approach for augmenting model-free deep '
                     'reinforcement learning agents with a mechanism for '
                     'relational reasoning over structured representations, '
                     'which improves performance, learning efficiency, '
                     'generalization, and interpretability. Our architecture '
                     'encodes an image as a set of vectors, and applies an '
                     'iterative message-passing procedure to discover and '
                     'reason about relevant entities and relations in a scene. '
                     'In six of seven StarCraft II Learning Environment '
                     'mini-games, our agent achieved state-of-the-art',
         'author': ['V Zambaldi', 'D Raposo', 'A Santoro', 'V Bapst'],
         'cites': '46',
         'gsrank': '1',
         'title': 'Deep reinforcement learning with relational inductive '
                  'biases',
         'url': 'https://openreview.net/forum?id=HkxaFoC9KQ&utm_campaign=piqcy&utm_medium=email&utm_source=Revue%20newsletter',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=16530148958946396046&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2Breinforcement%2Blearning%2Bwith%2Brelational%2Binductive%2Bbiases%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=jgdLzuXiZuUJ&ei=WZIqX5WlIqOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:jgdLzuXiZuUJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqUtVuYFEhnvcR-ZhURydTP_dkkNGf-&scisig=AAGBfm0AAAAAXyqUtQSWFJ6-d60GAFQ483-2gOb-sFZx&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
46
-------------------------------------------------
2020-08-05 11:05:01
Got the results of the query
{'bib': {'abstract': 'Neural network quantization has become an important '
                     'research area due to its great impact on deployment of '
                     'large models on resource constrained devices. In order '
                     'to train networks that can be effectively discretized '
                     'without loss of performance, we introduce a '
                     'differentiable quantization procedure. Differentiability '
                     'can be achieved by transforming continuous distributions '
                     'over the weights and activations of the network to '
                     'categorical distributions over the quantization grid. '
                     'These are subsequently relaxed to continuous surrogates '
                     'that can',
         'author': ['C Louizos', 'M Reisser', 'T Blankevoort', 'E Gavves'],
         'cites': '46',
         'eprint': 'https://arxiv.org/pdf/1810.01875',
         'gsrank': '1',
         'title': 'Relaxed quantization for discretized neural networks',
         'url': 'https://arxiv.org/abs/1810.01875',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=3712050618324227952&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRelaxed%2BQuantization%2Bfor%2BDiscretized%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=cBd8vV7agzMJ&ei=YZIqX4S9AbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:cBd8vV7agzMJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqUvWK6cRmgXiXq2EU2YfW2CwFC5bOC&scisig=AAGBfm0AAAAAXyqUvV3Vld6S_ji4q2AWqe1IR650wYI_&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
46
-------------------------------------------------
2020-08-05 11:05:09
Got the results of the query
{'bib': {'abstract': 'Many real-world systems studied are governed by complex, '
                     'nonlinear dynamics. By modeling these dynamics, we can '
                     'gain insight into how these systems work, make '
                     'predictions about how they will behave, and develop '
                     'strategies for controlling them. While there are many '
                     'methods for modeling nonlinear dynamical systems, '
                     'existing techniques face a trade off between offering '
                     'interpretable descriptions and making accurate '
                     'predictions. Here, we develop a class of models that '
                     'aims to achieve both simultaneously, smoothly',
         'author': ['J Nassar', 'SW Linderman', 'M Bugallo'],
         'cites': '14',
         'eprint': 'https://arxiv.org/pdf/1811.12386',
         'gsrank': '1',
         'title': 'Tree-structured recurrent switching linear dynamical '
                  'systems for multi-scale modeling',
         'url': 'https://arxiv.org/abs/1811.12386',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=10945679458649765039&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTree-Structured%2BRecurrent%2BSwitching%2BLinear%2BDynamical%2BSystems%2Bfor%2BMulti-Scale%2BModeling%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=r_Qblnnd5pcJ&ei=aZIqX_f2K6OGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:r_Qblnnd5pcJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqUxRV9UHltF9s5fbtYp6Olk4nenqlU&scisig=AAGBfm0AAAAAXyqUxdnl8muwMCWEuaDtrIU9LvXYrKgc&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
14
-------------------------------------------------
2020-08-05 11:05:17
Got the results of the query
{'bib': {'abstract': 'Convolutional architectures have recently been shown to '
                     'be competitive on many sequence modelling tasks when '
                     'compared to the de-facto standard of recurrent neural '
                     'networks (RNNs), while providing computational and '
                     'modeling advantages due to inherent parallelism. '
                     'However, currently there remains a performance gap to '
                     'more expressive stochastic RNN variants, especially '
                     'those with several layers of dependent random variables. '
                     'In this work, we propose stochastic temporal '
                     'convolutional networks (STCNs), a',
         'author': ['E Aksan', 'O Hilliges'],
         'cites': '11',
         'eprint': 'https://arxiv.org/pdf/1902.06568',
         'gsrank': '1',
         'title': 'Stcn: Stochastic temporal convolutional networks',
         'url': 'https://arxiv.org/abs/1902.06568',
         'venue': 'arXiv preprint arXiv:1902.06568',
         'year': '2019'},
 'citations_link': '/scholar?cites=15721878191744318436&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSTCN:%2BStochastic%2BTemporal%2BConvolutional%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=5Hvh1OpUL9oJ&ei=cZIqX9qLM4jHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:5Hvh1OpUL9oJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqUztaV_JdTPCZ3krbMv06V5u_W0PqD&scisig=AAGBfm0AAAAAXyqUzg7w2J7PoyHq0mIQBllmZtXod56j&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
11
-------------------------------------------------
2020-08-05 11:05:26
Got the results of the query
{'bib': {'abstract': 'We propose a reinforcement learning (RL) algorithm that '
                     'uses mutual-information regularization to optimize a '
                     'prior action distribution for better performance and '
                     'exploration. Entropy-based regularization has previously '
                     'been shown to improve both exploration and robustness in '
                     'challenging sequential decision-making tasks. It does so '
                     'by encouraging policies to put probability mass on all '
                     'actions. However, entropy regularization might be '
                     'undesirable when actions have significantly different '
                     'importance. In this paper, we propose',
         'author': ['J Grau-Moya', 'F Leibfried', 'P Vrancx'],
         'cites': '15',
         'eprint': 'https://openreview.net/pdf?id=HyEtjoCqFX',
         'gsrank': '1',
         'title': 'Soft q-learning with mutual-information regularization',
         'url': 'https://openreview.net/forum?id=HyEtjoCqFX',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=13711644038639649091&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSoft%2BQ-Learning%2Bwith%2BMutual-Information%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=QwG4EZuLSb4J&ei=eZIqX9quF4-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:QwG4EZuLSb4J:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqU19nYT3Bv99YgSwdp49CymrdyOrBz&scisig=AAGBfm0AAAAAXyqU12yPmd9Y1ToQ_klZTpad9jqQVu0j&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 11:05:35
Got the results of the query
{'bib': {'abstract': 'Alternatives to recurrent neural networks, in '
                     'particular, architectures based on attention or '
                     'convolutions, have been gaining momentum for processing '
                     'input sequences. In spite of their relevance, the '
                     'computational properties of these alternatives have not '
                     'yet been fully explored. We study the computational '
                     'power of two of the most paradigmatic architectures '
                     'exemplifying these mechanisms: the Transformer (Vaswani '
                     'et al., 2017) and the Neural GPU (Kaiser & Sutskever, '
                     '2016). We show both models to be Turing complete '
                     'exclusively based',
         'author': ['J Pérez', 'J Marinković', 'P Barceló'],
         'cites': '13',
         'eprint': 'https://arxiv.org/pdf/1901.03429',
         'gsrank': '1',
         'title': 'On the turing completeness of modern neural network '
                  'architectures',
         'url': 'https://arxiv.org/abs/1901.03429',
         'venue': 'arXiv preprint arXiv:1901.03429',
         'year': '2019'},
 'citations_link': '/scholar?cites=11242133264938493225&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BTuring%2BCompleteness%2Bof%2BModern%2BNeural%2BNetwork%2BArchitectures%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=KeVCHqQUBJwJ&ei=g5IqX9HjM6OGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:KeVCHqQUBJwJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqU4dFH7ucBpNmRuh5twfxvb4yDFfQ3&scisig=AAGBfm0AAAAAXyqU4TFqCy4OgplKEW8YNl8C4Vo-EFNY&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
13
-------------------------------------------------
2020-08-05 11:05:45
Got the results of the query
{'bib': {'abstract': 'The Differentiable Neural Computer (DNC) can learn '
                     'algorithmic and question answering tasks. An analysis of '
                     'its internal activation patterns reveals three problems: '
                     'Most importantly, the lack of key-value separation makes '
                     'the address distribution resulting from content-based '
                     'look-up noisy and flat, since the value influences the '
                     'score calculation, although only the key should. Second, '
                     "DNC's de-allocation of memory results in aliasing, which "
                     'is a problem for content-based look-up. Thirdly, '
                     'chaining memory reads with the temporal linkage matrix',
         'author': ['R Csordás', 'J Schmidhuber'],
         'cites': '3',
         'eprint': 'https://arxiv.org/pdf/1904.10278',
         'gsrank': '1',
         'title': 'Improving differentiable neural computers through memory '
                  'masking, de-allocation, and link distribution sharpness '
                  'control',
         'url': 'https://arxiv.org/abs/1904.10278',
         'venue': 'arXiv preprint arXiv:1904.10278',
         'year': '2019'},
 'citations_link': '/scholar?cites=9465849868631633208&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BDifferentiable%2BNeural%2BComputers%2BThrough%2BMemory%2BMasking,%2BDe-allocation,%2Band%2BLink%2BDistribution%2BSharpness%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=OG33pE10XYMJ&ei=jZIqX63CHbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:OG33pE10XYMJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqU6Q5ObwJAVB7HxfWAqcfgLeTzzAlX&scisig=AAGBfm0AAAAAXyqU6frNDVYFqJkTA5MDprktB5L1ty2w&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
3
-------------------------------------------------
2020-08-05 11:05:53
Got the results of the query
{'bib': {'abstract': 'Neural networks have demonstrated considerable success '
                     'on a wide variety of real-world problems. However, '
                     'networks trained only to optimize for training accuracy '
                     'can often be fooled by adversarial examples-slightly '
                     'perturbed inputs that are misclassified with high',
         'author': ['V Tjeng', 'K Xiao', 'R Tedrake'],
         'cites': '142',
         'eprint': 'https://arxiv.org/pdf/1711.07356',
         'gsrank': '1',
         'title': 'Evaluating robustness of neural networks with mixed integer '
                  'programming',
         'url': 'https://arxiv.org/abs/1711.07356',
         'venue': 'arXiv preprint arXiv:1711.07356',
         'year': '2017'},
 'citations_link': '/scholar?cites=18154476008132424293&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEvaluating%2BRobustness%2Bof%2BNeural%2BNetworks%2Bwith%2BMixed%2BInteger%2BProgramming%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ZbIVLL6n8fsJ&ei=l5IqX9HJKYyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ZbIVLL6n8fsJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqU9CIMc-gEsZReG0zqXDeIDUbVaoqL&scisig=AAGBfm0AAAAAXyqU9CP1r0gXOpMHxhdBJXyMLjrNSlmd&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
142
-------------------------------------------------
2020-08-05 11:06:04
Got the results of the query
{'bib': {'abstract': 'We propose a new learning-based approach to solve '
                     'ill-posed inverse problems in imaging. We address the '
                     'case where ground truth training samples are rare and '
                     'the problem is severely ill-posed---both because of the '
                     'underlying physics and because we can only get',
         'author': ['K Kothari', 'S Gupta', 'M de Hoop'],
         'cites': '3',
         'eprint': 'https://openreview.net/pdf?id=HyGcghRct7',
         'gsrank': '1',
         'title': 'Random mesh projectors for inverse problems',
         'url': 'https://openreview.net/forum?id=HyGcghRct7',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=18402211953102637332&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRandom%2Bmesh%2Bprojectors%2Bfor%2Binverse%2Bproblems%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=FEkhskXKYf8J&ei=npIqX4fpEqiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:FEkhskXKYf8J:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqU-0o4LNiatoOJ4I0KpWYwYQJ5DErq&scisig=AAGBfm0AAAAAXyqU-3dTFx73ovRFoywIavsd0GhhgmQP&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
3
-------------------------------------------------
2020-08-05 11:06:11
Got the results of the query
{'bib': {'abstract': 'Dual learning has attracted much attention in machine '
                     'learning, computer vision and natural language '
                     'processing communities. The core idea of dual learning '
                     'is to leverage the duality between the primal task '
                     '(mapping from domain X to domain Y) and dual task '
                     '(mapping from',
         'author': ['Y Wang', 'Y Xia', 'T He', 'F Tian', 'T Qin'],
         'cites': '18',
         'eprint': 'https://openreview.net/pdf?id=HyGhN2A5tm',
         'gsrank': '1',
         'title': 'Multi-agent dual learning',
         'url': 'https://openreview.net/forum?id=HyGhN2A5tm',
         'venue': '… on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15284831206594964121&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-Agent%2BDual%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=merl-P-gHtQJ&ei=p5IqX-OZFJqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:merl-P-gHtQJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVBL_ciaJa-p4amWDSUpcq5DgCLPx4&scisig=AAGBfm0AAAAAXyqVBEEPyKLIN80HPoOUFwWD55EhcmF7&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
18
-------------------------------------------------
2020-08-05 11:06:21
Got the results of the query
{'bib': {'abstract': 'Learning with a primary objective, such as softmax cross '
                     'entropy for classification and sequence generation, has '
                     'been the norm for training deep neural networks for '
                     'years. Although being a widely-adopted approach, using '
                     'cross entropy as the primary objective',
         'author': ['HY Chen', 'PH Wang', 'CH Liu', 'SC Chang', 'JY Pan'],
         'cites': '9',
         'eprint': 'https://arxiv.org/pdf/1903.01182',
         'gsrank': '1',
         'title': 'Complement objective training',
         'url': 'https://arxiv.org/abs/1903.01182',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=63949908447902569&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DComplement%2BObjective%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=abt0WBoy4wAJ&ei=sJIqX-TFNLGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:abt0WBoy4wAJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVC0saa2KnzST-E-Yu8cpnneosH5p-&scisig=AAGBfm0AAAAAXyqVC_YoOkUXhRQaFSGRRRnwzu8Ii78I&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 11:06:27
Got the results of the query
{'bib': {'abstract': 'Normalization methods are a central building block in '
                     'the deep learning toolbox. They accelerate and stabilize '
                     'training, while decreasing the dependence on manually '
                     'tuned learning rate schedules. When learning from '
                     'multi-modal distributions, the effectiveness of',
         'author': ['L Deecke', 'I Murray', 'H Bilen'],
         'cites': '10',
         'eprint': 'https://arxiv.org/pdf/1810.05466',
         'gsrank': '1',
         'title': 'Mode normalization',
         'url': 'https://arxiv.org/abs/1810.05466',
         'venue': 'arXiv preprint arXiv:1810.05466',
         'year': '2018'},
 'citations_link': '/scholar?cites=10555295858855595157&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMode%2BNormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=lUw9xqjxe5IJ&ei=uZIqX7frDaiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:lUw9xqjxe5IJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVFObrFloE6F-WXH0UIV4yYHjnw-VZ&scisig=AAGBfm0AAAAAXyqVFDNdygDP6cf7h9c9a7clhhxVEWwI&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
10
-------------------------------------------------
2020-08-05 11:06:36
Got the results of the query
{'bib': {'abstract': 'In this work, we attempt to answer a critical question: '
                     'whether there exists some input sequence that will cause '
                     'a well-trained discrete-space neural network '
                     'sequence-to-sequence (seq2seq) model to generate '
                     'egregious outputs (aggressive, malicious, attacking, '
                     'etc.). And if such inputs exist, how to find them '
                     'efficiently. We adopt an empirical methodology, in which '
                     'we first create lists of egregious output sequences, and '
                     'then design a discrete optimization algorithm to find '
                     'input sequences that will cause the model to generate',
         'author': ['T He', 'J Glass'],
         'cites': '10',
         'eprint': 'https://arxiv.org/pdf/1809.04113',
         'gsrank': '1',
         'title': 'Detecting egregious responses in neural '
                  'sequence-to-sequence models',
         'url': 'https://arxiv.org/abs/1809.04113',
         'venue': 'arXiv preprint arXiv:1809.04113',
         'year': '2018'},
 'citations_link': '/scholar?cites=12410178054097232012&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDetecting%2BEgregious%2BResponses%2Bin%2BNeural%2BSequence-to-sequence%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=jGDQMTDPOawJ&ei=v5IqX93aC6iBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:jGDQMTDPOawJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVGTbZ6c6-eHasBjiY2aWgiILq7B4l&scisig=AAGBfm0AAAAAXyqVGWiZUPSIpFqCt6wKtM9F6iVEXLWT&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
10
-------------------------------------------------
2020-08-05 11:06:41
Got the results of the query
{'bib': {'abstract': 'Representation learning is a central challenge across a '
                     'range of machine learning areas. In reinforcement '
                     'learning, effective and functional representations have '
                     'the potential to tremendously accelerate learning '
                     'progress and solve more challenging problems. Most prior '
                     'work on representation learning has focused on '
                     'generative approaches, learning representations that '
                     'capture all underlying factors of variation in the '
                     'observation space in a more disentangled or well-ordered '
                     'manner. In this paper, we instead aim to learn',
         'author': ['D Ghosh', 'A Gupta', 'S Levine'],
         'cites': '23',
         'eprint': 'https://arxiv.org/pdf/1811.07819',
         'gsrank': '1',
         'title': 'Learning actionable representations with goal-conditioned '
                  'policies',
         'url': 'https://arxiv.org/abs/1811.07819',
         'venue': 'arXiv preprint arXiv:1811.07819',
         'year': '2018'},
 'citations_link': '/scholar?cites=17735393609212194781&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BActionable%2BRepresentations%2Bwith%2BGoal%2BConditioned%2BPolicies%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=3W_TfIXGIPYJ&ei=xZIqX_mEN7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:3W_TfIXGIPYJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVI5E5bamHM6-vcrX_pC4AyqAhXRGz&scisig=AAGBfm0AAAAAXyqVI4jH91T5lvcKEaR-Ua_LPDcFnL4C&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
23
-------------------------------------------------
2020-08-05 11:06:51
Got the results of the query
{'bib': {'abstract': 'Prior work on neural network verification has focused on '
                     'specifications that are linear functions of the output '
                     'of the network, eg, invariance of the classifier output '
                     'under adversarial perturbations of the input. In this '
                     'paper, we extend verification algorithms to be able to '
                     'certify richer properties of neural networks. To do this '
                     'we introduce the class of convex-relaxable '
                     'specifications, which constitute nonlinear '
                     'specifications that can be verified using a convex '
                     'relaxation. We show that a number of important '
                     'properties of interest',
         'author': ['C Qin', "B O'Donoghue", 'R Bunel', 'R Stanforth'],
         'cites': '17',
         'eprint': 'https://arxiv.org/pdf/1902.09592',
         'gsrank': '1',
         'title': 'Verification of non-linear specifications for neural '
                  'networks',
         'url': 'https://arxiv.org/abs/1902.09592',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=17519355431971639254&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVerification%2Bof%2BNon-Linear%2BSpecifications%2Bfor%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=1s96oPFAIfMJ&ei=z5IqX-3WIovrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:1s96oPFAIfMJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVLC7FQz2KvqufrlobxCLUvhLKrOr8&scisig=AAGBfm0AAAAAXyqVLIFwHt6n1FRC8bZdAJ3fTKV5LixP&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
17
-------------------------------------------------
2020-08-05 11:07:00
Got the results of the query
{'bib': {'abstract': 'We propose a novel approach for deformation-aware neural '
                     'networks that learn the weighting and synthesis of dense '
                     'volumetric deformation fields. Our method specifically '
                     'targets the space-time representation of physical '
                     'surfaces from liquid simulations. Liquids exhibit highly '
                     'complex, non-linear behavior under changing simulation '
                     'conditions such as different initial conditions. Our '
                     'algorithm captures these complex phenomena in two '
                     'stages: a first neural network computes a weighting '
                     'function for a set of pre-computed deformations',
         'author': ['L Prantl', 'B Bonev', 'N Thuerey'],
         'cites': '3',
         'eprint': 'https://arxiv.org/pdf/1704.07854',
         'gsrank': '1',
         'title': 'Generating liquid simulations with deformation-aware neural '
                  'networks',
         'url': 'https://arxiv.org/abs/1704.07854',
         'venue': 'arXiv preprint arXiv:1704.07854',
         'year': '2017'},
 'citations_link': '/scholar?cites=3505652891247833080&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerating%2BLiquid%2BSimulations%2Bwith%2BDeformation-aware%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=-O-lGLuUpjAJ&ei=2pIqX6KTE4yimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:-O-lGLuUpjAJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVNp_uBSff8GkOQBenQ_waEXXqrT9l&scisig=AAGBfm0AAAAAXyqVNp8F_dZsLtVGuE8JJffyP6JFqul3&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
3
-------------------------------------------------
2020-08-05 11:07:10
Got the results of the query
{'bib': {'abstract': 'Representation Learning over graph structured data has '
                     'received significant attention recently due to its '
                     'ubiquitous applicability. However, most advancements '
                     'have been made in static graph settings while efforts '
                     'for jointly learning dynamic of the graph and dynamic on '
                     'the graph are still in an infant stage. Two fundamental '
                     'questions arise in learning over dynamic graphs:(i) How '
                     'to elegantly model dynamical processes over graphs?(ii) '
                     'How to leverage such a model to effectively encode '
                     'evolving graph information into low',
         'author': ['R Trivedi', 'M Farajtabar', 'P Biswal'],
         'cites': '36',
         'eprint': 'https://openreview.net/pdf?id=HyePrhR5KX',
         'gsrank': '1',
         'title': 'Dyrep: Learning representations over dynamic graphs',
         'url': 'https://openreview.net/forum?id=HyePrhR5KX&noteId=BJeonJJo3X',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=10640403362393637821&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDyRep:%2BLearning%2BRepresentations%2Bover%2BDynamic%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=vUNYmntOqpMJ&ei=4JIqX5HAF4vrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:vUNYmntOqpMJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVO4Sz8KpgbUBhziG5AV3Q2kNXokLY&scisig=AAGBfm0AAAAAXyqVO24obllHK_FXL2Qfj86hwZu0r3va&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
36
-------------------------------------------------
2020-08-05 11:07:15
Got the results of the query
{'bib': {'abstract': 'We present trellis networks, a new architecture for '
                     'sequence modeling. On the one hand, a trellis network is '
                     'a temporal convolutional network with special structure, '
                     'characterized by weight tying across depth and direct '
                     'injection of the input into deep layers. On the other '
                     'hand, we show that truncated recurrent networks are '
                     'equivalent to trellis networks with special sparsity '
                     'structure in their weight matrices. Thus trellis '
                     'networks with general weight matrices generalize '
                     'truncated recurrent networks. We leverage these '
                     'connections to design',
         'author': ['S Bai', 'JZ Kolter', 'V Koltun'],
         'cites': '26',
         'eprint': 'https://arxiv.org/pdf/1810.06682',
         'gsrank': '1',
         'title': 'Trellis networks for sequence modeling',
         'url': 'https://arxiv.org/abs/1810.06682',
         'venue': 'arXiv preprint arXiv:1810.06682',
         'year': '2018'},
 'citations_link': '/scholar?cites=13782940196634240151&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTrellis%2BNetworks%2Bfor%2BSequence%2BModeling%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=lyTWbBXXRr8J&ei=6JIqX_mjHJqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:lyTWbBXXRr8J:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVRNLcqOl7YBGFRe3V-E-d6r1M_ftV&scisig=AAGBfm0AAAAAXyqVRNHVkV5d7WK6lBcZTuTe3WJ8kZJH&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
26
-------------------------------------------------
2020-08-05 11:07:24
Got the results of the query
{'bib': {'abstract': 'Generative adversarial networks (GANs) are an expressive '
                     'class of neural generative models with tremendous '
                     'success in modeling high-dimensional continuous '
                     'measures. In this paper, we present a scalable method '
                     'for unbalanced optimal transport (OT) based on the '
                     'generative-adversarial framework. We formulate '
                     'unbalanced OT as a problem of simultaneously learning a '
                     'transport map and a scaling factor that push a source '
                     'measure to a target measure in a cost-optimal manner. In '
                     'addition, we propose an algorithm for solving',
         'author': ['KD Yang', 'C Uhler'],
         'cites': '9',
         'eprint': 'https://arxiv.org/pdf/1810.11447',
         'gsrank': '1',
         'title': 'Scalable unbalanced optimal transport using generative '
                  'adversarial networks',
         'url': 'https://arxiv.org/abs/1810.11447',
         'venue': 'arXiv preprint arXiv:1810.11447',
         'year': '2018'},
 'citations_link': '/scholar?cites=14112773597586866494&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DScalable%2BUnbalanced%2BOptimal%2BTransport%2Busing%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=PkGtd9Ck2sMJ&ei=8JIqX_Qoo4bL1g-k7aCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:PkGtd9Ck2sMJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVSjgK8VtTFpki3J8F16JIFq6cBt6t&scisig=AAGBfm0AAAAAXyqVSk9-tafxW5PxfVrWwUd8hQCoqbfD&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 11:07:31
Got the results of the query
{'bib': {'abstract': 'Recently, Approximate Policy Iteration (API) algorithms '
                     'have achieved super-human proficiency in two-player '
                     'zero-sum games such as Go, Chess, and Shogi without '
                     'human data. These API algorithms iterate between two '
                     'policies: a slow policy (tree search), and a fast policy '
                     '(a neural network). In these two-player games, a reward '
                     'is always received at the end of the game. However, the '
                     "Rubik's Cube has only a single solved state, and "
                     'episodes are not guaranteed to terminate. This poses a '
                     'major problem for these API algorithms since',
         'author': ['S McAleer', 'F Agostinelli', 'A Shmakov'],
         'cites': '7',
         'eprint': 'https://openreview.net/pdf?id=Hyfn2jCcKm',
         'gsrank': '1',
         'title': "Solving the Rubik's Cube with Approximate Policy Iteration",
         'url': 'https://openreview.net/forum?id=Hyfn2jCcKm',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15124135703316587578&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSolving%2Bthe%2BRubik%255C%2527s%2BCube%2Bwith%2BApproximate%2BPolicy%2BIteration%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=OuT8gkq549EJ&ei=9JIqX72EL8KwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:OuT8gkq549EJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVT0AGVRnzVrmbGvbKQgRenrSMon2J&scisig=AAGBfm0AAAAAXyqVT16y68KPcXlwMwfese8p888lx6MS&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
7
-------------------------------------------------
2020-08-05 11:07:35
Got the results of the query
{'bib': {'abstract': 'We consider reinforcement learning in input-driven '
                     'environments, where an exogenous, stochastic input '
                     'process affects the dynamics of the system. Input '
                     'processes arise in many applications, including queuing '
                     'systems, robotics control with disturbances, and object '
                     'tracking. Since the state dynamics and rewards depend on '
                     'the input process, the state alone provides limited '
                     'information for the expected future returns. Therefore, '
                     'policy gradient methods with standard state-dependent '
                     'baselines suffer high variance during training. We',
         'author': ['H Mao', 'SB Venkatakrishnan', 'M Schwarzkopf'],
         'cites': '19',
         'eprint': 'https://arxiv.org/pdf/1807.02264',
         'gsrank': '1',
         'title': 'Variance reduction for reinforcement learning in '
                  'input-driven environments',
         'url': 'https://arxiv.org/abs/1807.02264',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11915919798056236304&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariance%2BReduction%2Bfor%2BReinforcement%2BLearning%2Bin%2BInput-Driven%2BEnvironments%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ELUvT-zZXaUJ&ei=-5IqX9nRGcKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ELUvT-zZXaUJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVVvlBYD9kG5FmRrB2h7M8mZu58RAN&scisig=AAGBfm0AAAAAXyqVVn3nQpAY7oADfYxmga6e2lBpvRmB&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
19
-------------------------------------------------
2020-08-05 11:07:42
Got the results of the query
{'bib': {'abstract': 'Learning a policy using only observational data is '
                     'challenging because the distribution of states it '
                     'induces at execution time may differ from the '
                     'distribution observed during training. We propose to '
                     'train a policy by unrolling a learned model of the '
                     'environment dynamics over multiple time steps while '
                     'explicitly penalizing two costs: the original cost the '
                     'policy seeks to optimize, and an uncertainty cost which '
                     'represents its divergence from the states it is trained '
                     'on. We measure this second cost by using the uncertainty '
                     'of the dynamics model about its',
         'author': ['M Henaff', 'A Canziani', 'Y LeCun'],
         'cites': '33',
         'eprint': 'https://arxiv.org/pdf/1901.02705',
         'gsrank': '1',
         'title': 'Model-predictive policy learning with uncertainty '
                  'regularization for driving in dense traffic',
         'url': 'https://arxiv.org/abs/1901.02705',
         'venue': 'arXiv preprint arXiv:1901.02705',
         'year': '2019'},
 'citations_link': '/scholar?cites=5048415252406845644&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModel-Predictive%2BPolicy%2BLearning%2Bwith%2BUncertainty%2BRegularization%2Bfor%2BDriving%2Bin%2BDense%2BTraffic%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=zMBfpuySD0YJ&ei=ApMqX7GPG4vrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:zMBfpuySD0YJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVXhVz7w_xSW8zRSvBi4ROsDDtFQD_&scisig=AAGBfm0AAAAAXyqVXnVPnLBrJHtxf0k9Of_j-kb2AUrp&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
33
-------------------------------------------------
2020-08-05 11:07:50
Got the results of the query
{'bib': {'abstract': 'Generative Adversarial Networks (GANs) have recently '
                     'achieved impressive results for many real-world '
                     'applications, and many GAN variants have emerged with '
                     'improvements in sample quality and training stability. '
                     'However, they have not been well visualized or '
                     'understood. How does a GAN represent our visual world '
                     'internally? What causes the artifacts in GAN results? '
                     'How do architectural choices affect GAN learning? '
                     'Answering such questions could enable us to develop new '
                     'insights and better models. In this work, we',
         'author': ['D Bau', 'JY Zhu', 'H Strobelt', 'B Zhou'],
         'cites': '57',
         'eprint': 'https://arxiv.org/pdf/1811.10597',
         'gsrank': '1',
         'title': 'Gan dissection: Visualizing and understanding generative '
                  'adversarial networks',
         'url': 'https://arxiv.org/abs/1811.10597',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=197925763027882731&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGAN%2BDissection:%2BVisualizing%2Band%2BUnderstanding%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=69IszW4svwIJ&ei=CZMqX_jDDaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:69IszW4svwIJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVZd1fvfvyeeF9OrLZel-lV7IU5DV7&scisig=AAGBfm0AAAAAXyqVZZPmau0IhFkClrFoS95pQBn0CJOn&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
57
-------------------------------------------------
2020-08-05 11:07:57
Got the results of the query
{'bib': {'abstract': 'Generative adversarial nets (GANs) are widely used to '
                     'learn the data sampling process and their performance '
                     'may heavily depend on the loss functions, given a '
                     'limited computational budget. This study revisits '
                     'MMD-GAN that uses the maximum mean discrepancy (MMD) as '
                     'the loss function for GAN and makes two contributions. '
                     'First, we argue that the existing MMD loss function may '
                     'discourage the learning of fine details in data as it '
                     'attempts to contract the discriminator outputs of real '
                     'data. To address this issue, we propose a repulsive loss',
         'author': ['W Wang', 'Y Sun', 'S Halgamuge'],
         'cites': '20',
         'eprint': 'https://arxiv.org/pdf/1812.09916',
         'gsrank': '1',
         'title': 'Improving mmd-gan training with repulsive loss function',
         'url': 'https://arxiv.org/abs/1812.09916',
         'venue': 'arXiv preprint arXiv:1812.09916',
         'year': '2018'},
 'citations_link': '/scholar?cites=5981776109708607840&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BMMD-GAN%2BTraining%2Bwith%2BRepulsive%2BLoss%2BFunction%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=YIEZ0a2JA1MJ&ei=EJMqX92DJoyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:YIEZ0a2JA1MJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVa83qTmgjkj-ZU4mDN_Nu5FqvFUEr&scisig=AAGBfm0AAAAAXyqVa-xbfYtI10mFYpJkPhLbe1xbu06D&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
20
-------------------------------------------------
2020-08-05 11:08:03
Got the results of the query
{'bib': {'abstract': 'The ability of overparameterized deep networks to '
                     'generalize well has been linked to the fact that '
                     'stochastic gradient descent (SGD) finds solutions that '
                     'lie in flat, wide minima in the training loss--minima '
                     'where the output of the network is resilient to small '
                     'random noise added to its parameters. So far this '
                     'observation has been used to provide generalization '
                     'guarantees only for neural networks whose parameters are '
                     'either\\textit {stochastic} or\\textit {compressed}. In '
                     'this work, we present a general PAC-Bayesian framework '
                     'that leverages',
         'author': ['V Nagarajan', 'JZ Kolter'],
         'cites': '21',
         'eprint': 'https://arxiv.org/pdf/1905.13344',
         'gsrank': '1',
         'title': 'Deterministic PAC-bayesian generalization bounds for deep '
                  'networks via generalizing noise-resilience',
         'url': 'https://arxiv.org/abs/1905.13344',
         'venue': 'arXiv preprint arXiv:1905.13344',
         'year': '2019'},
 'citations_link': '/scholar?cites=2407471741644905222&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeterministic%2BPAC-Bayesian%2Bgeneralization%2Bbounds%2Bfor%2Bdeep%2Bnetworks%2Bvia%2Bgeneralizing%2Bnoise-resilience%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=BhuJL8sOaSEJ&ei=GJMqX-riFYyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:BhuJL8sOaSEJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVdOejthqtUtD_m0uRDpMXAb4Fhme8&scisig=AAGBfm0AAAAAXyqVdE72i51k3Bv0NY6fen2alyEsCr0z&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
21
-------------------------------------------------
2020-08-05 11:08:13
Got the results of the query
{'bib': {'abstract': 'In many environments only a tiny subset of all states '
                     'yield high reward. In these cases, few of the '
                     'interactions with the environment provide a relevant '
                     'learning signal. Hence, we may want to preferentially '
                     'train on those high-reward states and the probable '
                     'trajectories leading to them. To this end, we advocate '
                     'for the use of a backtracking model that predicts the '
                     'preceding states that terminate at a given high-reward '
                     'state. We can train a model which, starting from a high '
                     'value state (or one that is estimated to have high '
                     'value), predicts and',
         'author': ['A Goyal', 'P Brakel', 'W Fedus', 'S Singhal'],
         'cites': '38',
         'eprint': 'https://arxiv.org/pdf/1804.00379',
         'gsrank': '1',
         'title': 'Recall traces: Backtracking models for efficient '
                  'reinforcement learning',
         'url': 'https://arxiv.org/abs/1804.00379',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=4448196484511573554&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRecall%2BTraces:%2BBacktracking%2BModels%2Bfor%2BEfficient%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=MiZ0liIruz0J&ei=IJMqX7CyGpqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:MiZ0liIruz0J:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVelDOtKLKNjIZQ17qA8GfZvXzuETX&scisig=AAGBfm0AAAAAXyqVemsI-GqRdC3NbzubYUawYy-Hb8uI&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
38
-------------------------------------------------
2020-08-05 11:08:18
Got the results of the query
{'bib': {'abstract': 'Stability is a fundamental property of dynamical '
                     'systems, yet to this date it has had little bearing on '
                     'the practice of recurrent neural networks. In this work, '
                     'we conduct a thorough investigation of stable recurrent '
                     'models. Theoretically, we prove stable recurrent neural',
         'author': ['J Miller', 'M Hardt'],
         'cites': '26',
         'eprint': 'https://arxiv.org/pdf/1805.10369',
         'gsrank': '1',
         'title': 'Stable recurrent models',
         'url': 'https://arxiv.org/abs/1805.10369',
         'venue': 'arXiv preprint arXiv:1805.10369',
         'year': '2018'},
 'citations_link': '/scholar?cites=10865601919039311451&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStable%2BRecurrent%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Ww4kf2BfypYJ&ei=JpMqX6mlOovrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Ww4kf2BfypYJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVg9z7ZJs3b4N05AJKPrjE-NWsOwSF&scisig=AAGBfm0AAAAAXyqVg77CFzJ5O6UiUb-31pg4qI1GgbeB&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
26
-------------------------------------------------
2020-08-05 11:08:27
Got the results of the query
{'bib': {'abstract': 'The adversarial training procedure proposed by Madry et '
                     'al.(2018) is one of the most effective methods to defend '
                     'against adversarial examples in deep neural networks '
                     '(DNNs). In our paper, we shed some lights on the '
                     'practicality and the hardness of adversarial training by '
                     'showing that the effectiveness (robustness on test set) '
                     'of adversarial training has a strong correlation with '
                     'the distance between a test point and the manifold of '
                     'training data embedded by the network. Test examples '
                     'that are relatively far away from this manifold are',
         'author': ['H Zhang', 'H Chen', 'Z Song', 'D Boning', 'IS Dhillon'],
         'cites': '33',
         'eprint': 'https://arxiv.org/pdf/1901.04684',
         'gsrank': '1',
         'title': 'The limitations of adversarial training and the blind-spot '
                  'attack',
         'url': 'https://arxiv.org/abs/1901.04684',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=6221614174421347650&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BLimitations%2Bof%2BAdversarial%2BTraining%2Band%2Bthe%2BBlind-Spot%2BAttack%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Qm3ryyCdV1YJ&ei=L5MqX8yYO7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Qm3ryyCdV1YJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVig1N5cXkUXgZ-roq40OYUZbdR8-j&scisig=AAGBfm0AAAAAXyqVit4L3p82bCO0EeL6LpLHm2yRszwm&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
33
-------------------------------------------------
2020-08-05 11:08:34
Got the results of the query
{'bib': {'abstract': 'We provide a theoretical algorithm for checking local '
                     'optimality and escaping saddles at nondifferentiable '
                     'points of empirical risks of two-layer ReLU networks. '
                     'Our algorithm receives any parameter value and returns: '
                     'local minimum, second-order stationary point, or a '
                     'strict descent direction. The presence of $ M $ data '
                     'points on the nondifferentiability of the ReLU divides '
                     'the parameter space into at most $2^ M $ regions, which '
                     'makes analysis difficult. By exploiting polyhedral '
                     'geometry, we reduce the total computation down to one',
         'author': ['C Yun', 'S Sra', 'A Jadbabaie'],
         'cites': '4',
         'eprint': 'https://arxiv.org/pdf/1809.10858',
         'gsrank': '1',
         'title': 'Efficiently testing local optimality and escaping saddles '
                  'for ReLU networks',
         'url': 'https://arxiv.org/abs/1809.10858',
         'venue': 'arXiv preprint arXiv:1809.10858',
         'year': '2018'},
 'citations_link': '/scholar?cites=17166372605101418764&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficiently%2Btesting%2Blocal%2Boptimality%2Band%2Bescaping%2Bsaddles%2Bfor%2BReLU%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=DCkR2e40O-4J&ei=NJMqX_HdGIjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:DCkR2e40O-4J:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVjlEeQ68qabP5SFcMj-J8DaEufhMr&scisig=AAGBfm0AAAAAXyqVjuWHPnfqH4vRT4kZCm-QMc9HXzLk&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
4
-------------------------------------------------
2020-08-05 11:08:38
Got the results of the query
{'bib': {'abstract': 'Neural architecture search (NAS) has a great impact by '
                     'automatically designing effective neural network '
                     'architectures. However, the prohibitive computational '
                     'demand of conventional NAS algorithms (eg $10^ 4$ GPU '
                     'hours) makes it difficult to\\emph {directly} search the '
                     'architectures on large-scale tasks (eg ImageNet). '
                     'Differentiable NAS can reduce the cost of GPU hours via '
                     'a continuous representation of network architecture but '
                     'suffers from the high GPU memory consumption issue (grow '
                     'linearly wrt candidate set size). As a',
         'author': ['H Cai', 'L Zhu', 'S Han'],
         'cites': '407',
         'eprint': 'https://arxiv.org/pdf/1812.00332.pdf%C3%AF%C2%BC%E2%80%B0',
         'gsrank': '1',
         'title': 'Proxylessnas: Direct neural architecture search on target '
                  'task and hardware',
         'url': 'https://arxiv.org/abs/1812.00332',
         'venue': 'arXiv preprint arXiv:1812.00332',
         'year': '2018'},
 'citations_link': '/scholar?cites=18033301425061747520&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProxylessNAS:%2BDirect%2BNeural%2BArchitecture%2BSearch%2Bon%2BTarget%2BTask%2Band%2BHardware%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=QNtZABooQ_oJ&ei=O5MqX-ekJovrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:QNtZABooQ_oJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVllNyqYhEDU9t32k5fUldz98oqCAi&scisig=AAGBfm0AAAAAXyqVltXFbqKSKrbf5NEH-KwAKuBgOJJC&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
407
-------------------------------------------------
2020-08-05 11:08:46
Got the results of the query
{'bib': {'abstract': 'Real-world tasks are often highly structured. '
                     'Hierarchical reinforcement learning (HRL) has attracted '
                     'research interest as an approach for leveraging the '
                     'hierarchical structure of a given task in reinforcement '
                     'learning (RL). However, identifying the hierarchical '
                     'policy structure that enhances the performance of RL is '
                     'not a trivial task. In this paper, we propose an HRL '
                     'method that learns a latent variable of a hierarchical '
                     'policy using mutual information maximization. Our '
                     'approach can be interpreted as a way to learn a discrete '
                     'and latent',
         'author': ['T Osa', 'V Tangkaratt', 'M Sugiyama'],
         'cites': '14',
         'eprint': 'https://arxiv.org/pdf/1901.01365',
         'gsrank': '1',
         'title': 'Hierarchical reinforcement learning via advantage-weighted '
                  'information maximization',
         'url': 'https://arxiv.org/abs/1901.01365',
         'venue': 'arXiv preprint arXiv:1901.01365',
         'year': '2019'},
 'citations_link': '/scholar?cites=8371143208721459013&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BReinforcement%2BLearning%2Bvia%2BAdvantage-Weighted%2BInformation%2BMaximization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=RSfEZ2VGLHQJ&ei=P5MqX4egPIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:RSfEZ2VGLHQJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVm1uqOHjDLHtprnBQz2lHdQtKuOah&scisig=AAGBfm0AAAAAXyqVm_x2UNsyfT1vXd3v7F4BKinJs9Zh&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
14
-------------------------------------------------
2020-08-05 11:08:51
Got the results of the query
{'bib': {'abstract': 'Deep neural networks (DNNs) have set benchmarks on a '
                     'wide array of supervised learning tasks. Trained DNNs, '
                     'however, often lack robustness to minor adversarial '
                     'perturbations to the input, which undermines their true '
                     'practicality. Recent works have increased the robustness '
                     'of DNNs by fitting networks using '
                     'adversarially-perturbed training samples, but the '
                     'improved performance can still be far below the '
                     'performance seen in non-adversarial settings. A '
                     'significant portion of this gap can be attributed to the '
                     'decrease in generalization',
         'author': ['F Farnia', 'JM Zhang', 'D Tse'],
         'cites': '31',
         'eprint': 'https://arxiv.org/pdf/1811.07457',
         'gsrank': '1',
         'title': 'Generalizable adversarial training via spectral '
                  'normalization',
         'url': 'https://arxiv.org/abs/1811.07457',
         'venue': 'arXiv preprint arXiv:1811.07457',
         'year': '2018'},
 'citations_link': '/scholar?cites=16959420457208400665&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeneralizable%2BAdversarial%2BTraining%2Bvia%2BSpectral%2BNormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=GUtmDA33W-sJ&ei=R5MqX-LlCbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:GUtmDA33W-sJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVojesDBFX8gUahjgU4FasUBghqG4P&scisig=AAGBfm0AAAAAXyqVoiZBWoYrzURUBHiVIWOqk_m9iV5p&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
31
-------------------------------------------------
2020-08-05 11:08:58
Got the results of the query
{'bib': {'abstract': 'Brain-Machine Interfaces (BMIs) have recently emerged as '
                     'a clinically viable option to restore voluntary '
                     'movements after paralysis. These devices are based on '
                     'the ability to extract information about movement intent '
                     'from neural signals recorded using multi-electrode '
                     'arrays chronically implanted in the motor cortices of '
                     'the brain. However, the inherent loss and turnover of '
                     'recorded neurons requires repeated recalibrations of the '
                     'interface, which can potentially alter the day-to-day '
                     'user experience. The resulting need for',
         'author': ['A Farshchian', 'JA Gallego', 'JP Cohen', 'Y Bengio'],
         'cites': '10',
         'eprint': 'https://arxiv.org/pdf/1810.00045',
         'gsrank': '1',
         'title': 'Adversarial domain adaptation for stable brain-machine '
                  'interfaces',
         'url': 'https://arxiv.org/abs/1810.00045',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1197289951589381792&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BDomain%2BAdaptation%2Bfor%2BStable%2BBrain-Machine%2BInterfaces%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=oDahEd6gnRAJ&ei=T5MqX_DbIMKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:oDahEd6gnRAJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVqVbR5nsGPXOeC8YchGIO1OpKb0GO&scisig=AAGBfm0AAAAAXyqVqazvdclQJpDUzLS7vw2KfXm9WeM0&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
10
-------------------------------------------------
2020-08-05 11:09:05
Got the results of the query
{'bib': {'abstract': 'Humans and animals can learn complex predictive models '
                     'that allow them to accurately and reliably reason about '
                     'real-world phenomena, and they can adapt such models '
                     'extremely quickly in the face of unexpected changes. '
                     'Deep neural network models allow us to represent very '
                     'complex functions, but lack this capacity for rapid '
                     'online adaptation. The goal in this paper is to develop '
                     'a method for continual online learning from an incoming '
                     'stream of data, using deep neural network models. We '
                     'formulate an online learning procedure that',
         'author': ['A Nagabandi', 'C Finn', 'S Levine'],
         'cites': '52',
         'eprint': 'https://arxiv.org/pdf/1812.07671',
         'gsrank': '1',
         'title': 'Deep online learning via meta-learning: Continual '
                  'adaptation for model-based rl',
         'url': 'https://arxiv.org/abs/1812.07671',
         'venue': 'arXiv preprint arXiv:1812.07671',
         'year': '2018'},
 'citations_link': '/scholar?cites=11074585999071693969&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BOnline%2BLearning%2BVia%2BMeta-Learning:%2BContinual%2BAdaptation%2Bfor%2BModel-Based%2BRL%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=kcwuXUrVsJkJ&ei=VJMqX5CyKYyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:kcwuXUrVsJkJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVrpnJdq4W6CJblwhqOmfOdrT-z1au&scisig=AAGBfm0AAAAAXyqVrpLiBgWIDoKlElSTij0LhSSTzzRs&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
52
-------------------------------------------------
2020-08-05 11:09:11
Got the results of the query
{'bib': {'abstract': 'It is important to detect anomalous inputs when '
                     'deploying machine learning systems. The use of larger '
                     'and more complex inputs in deep learning magnifies the '
                     'difficulty of distinguishing between anomalous and '
                     'in-distribution examples. At the same time, diverse '
                     'image and text data are available in enormous '
                     'quantities. We propose leveraging these data to improve '
                     'deep anomaly detection by training anomaly detectors '
                     'against an auxiliary dataset of outliers, an approach we '
                     'call Outlier Exposure (OE). This enables anomaly',
         'author': ['D Hendrycks', 'M Mazeika', 'T Dietterich'],
         'cites': '145',
         'eprint': 'https://arxiv.org/pdf/1812.04606',
         'gsrank': '1',
         'title': 'Deep anomaly detection with outlier exposure',
         'url': 'https://arxiv.org/abs/1812.04606',
         'venue': 'arXiv preprint arXiv:1812.04606',
         'year': '2018'},
 'citations_link': '/scholar?cites=13915279318347653817&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BAnomaly%2BDetection%2Bwith%2BOutlier%2BExposure%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=uaJzUNAAHcEJ&ei=WpMqX4PRL7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:uaJzUNAAHcEJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVtkt89YXO48VPZFYv5n3cEJYzx17C&scisig=AAGBfm0AAAAAXyqVtmIVHpR9qeC_09-TJXWldclMULjs&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
145
-------------------------------------------------
2020-08-05 11:09:19
Got the results of the query
{'bib': {'abstract': 'This paper investigates whether learning '
                     'contingency-awareness and controllable aspects of an '
                     'environment can lead to better exploration in '
                     'reinforcement learning. To investigate this question, we '
                     'consider an instantiation of this hypothesis evaluated '
                     'on the Arcade Learning Element (ALE). In this study, we '
                     'develop an attentive dynamics model (ADM) that discovers '
                     'controllable elements of the observations, which are '
                     'often associated with the location of the character in '
                     'Atari games. The ADM is trained in a self-supervised '
                     'fashion to',
         'author': ['J Choi', 'Y Guo', 'M Moczulski', 'J Oh', 'N Wu'],
         'cites': '21',
         'eprint': 'https://arxiv.org/pdf/1811.01483',
         'gsrank': '1',
         'title': 'Contingency-aware exploration in reinforcement learning',
         'url': 'https://arxiv.org/abs/1811.01483',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=6536781875136948962&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DContingency-Aware%2BExploration%2Bin%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=4uq_aXtQt1oJ&ei=YJMqX9bwO5qGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:4uq_aXtQt1oJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVvvzkMejAbA-9yu59jF_hMPL0iycE&scisig=AAGBfm0AAAAAXyqVvlOPt2M9bDdE0_m2DWppov0yhrZ3&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
21
-------------------------------------------------
2020-08-05 11:09:26
Got the results of the query
{'bib': {'abstract': 'We propose a context-adaptive entropy model for use in '
                     'end-to-end optimized image compression. Our model '
                     'exploits two types of contexts, bit-consuming contexts '
                     'and bit-free contexts, distinguished based upon whether '
                     'additional bit allocation is required. Based on these '
                     'contexts, we allow the model to more accurately estimate '
                     'the distribution of each latent representation with a '
                     'more generalized form of the approximation models, which '
                     'accordingly leads to an enhanced compression '
                     'performance. Based on the experimental',
         'author': ['J Lee', 'S Cho', 'SK Beack'],
         'cites': '53',
         'eprint': 'https://arxiv.org/pdf/1809.10452',
         'gsrank': '1',
         'title': 'Context-adaptive entropy model for end-to-end optimized '
                  'image compression',
         'url': 'https://arxiv.org/abs/1809.10452',
         'venue': 'arXiv preprint arXiv:1809.10452',
         'year': '2018'},
 'citations_link': '/scholar?cites=17458297235582784877&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DContext-adaptive%2BEntropy%2BModel%2Bfor%2BEnd-to-end%2BOptimized%2BImage%2BCompression%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=bZUYdtZUSPIJ&ei=a5MqX5-DCqOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:bZUYdtZUSPIJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVxgqlWxF7jxWy6--3MozYFiqClBBQ&scisig=AAGBfm0AAAAAXyqVxqyhDvr29MrMBz-nOWD2_15-s5_4&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
53
-------------------------------------------------
2020-08-05 11:09:35
Got the results of the query
{'bib': {'abstract': 'Adversarial learning methods have been proposed for a '
                     'wide range of applications, but the training of '
                     'adversarial models can be notoriously unstable. '
                     'Effectively balancing the performance of the generator '
                     'and discriminator is critical, since a discriminator '
                     'that achieves very high accuracy will produce relatively '
                     'uninformative gradients. In this work, we propose a '
                     'simple and general technique to constrain information '
                     'flow in the discriminator by means of an information '
                     'bottleneck. By enforcing a constraint on the mutual '
                     'information between',
         'author': ['XB Peng', 'A Kanazawa', 'S Toyer', 'P Abbeel'],
         'cites': '57',
         'eprint': 'https://arxiv.org/pdf/1810.00821',
         'gsrank': '1',
         'title': 'Variational discriminator bottleneck: Improving imitation '
                  'learning, inverse rl, and gans by constraining information '
                  'flow',
         'url': 'https://arxiv.org/abs/1810.00821',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=9333510293426778540&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BDiscriminator%2BBottleneck:%2BImproving%2BImitation%2BLearning,%2BInverse%2BRL,%2Band%2BGANs%2Bby%2BConstraining%2BInformation%2BFlow%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=rLEnKylKh4EJ&ei=cpMqX5upCYvrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:rLEnKylKh4EJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqVz2xGBzP9nxGkrL8xxmKmnjZBlWzk&scisig=AAGBfm0AAAAAXyqVz5Udo6M7eS8SzQjAHZrVBh_OvVla&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
57
-------------------------------------------------
2020-08-05 11:09:43
Got the results of the query
{'bib': {'abstract': 'Adapting deep networks to new concepts from a few '
                     'examples is challenging, due to the high computational '
                     'requirements of standard fine-tuning procedures. Most '
                     'work on few-shot learning has thus focused on simple '
                     'learning techniques for adaptation, such as nearest '
                     'neighbours or gradient descent. Nonetheless, the machine '
                     'learning literature contains a wealth of methods that '
                     'learn non-deep models very efficiently. In this paper, '
                     'we propose to use these fast convergent methods as the '
                     'main adaptation mechanism for few-shot learning',
         'author': ['L Bertinetto', 'JF Henriques', 'PHS Torr'],
         'cites': '122',
         'eprint': 'https://arxiv.org/pdf/1805.08136',
         'gsrank': '1',
         'title': 'Meta-learning with differentiable closed-form solvers',
         'url': 'https://arxiv.org/abs/1805.08136',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=12967631409063437571&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-learning%2Bwith%2Bdifferentiable%2Bclosed-form%2Bsolvers%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=A6Vsow9I9rMJ&ei=fJMqX-W4H7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:A6Vsow9I9rMJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqV2TIzBPd7rPXwPQgcy9WKLAlSFc2F&scisig=AAGBfm0AAAAAXyqV2bABVZux98AMPNE2nPoVIP34RKHd&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
122
-------------------------------------------------
2020-08-05 11:09:53
Got the results of the query
{'bib': {'abstract': 'The success of popular algorithms for deep reinforcement '
                     'learning, such as policy-gradients and Q-learning, '
                     'relies heavily on the availability of an informative '
                     'reward signal at each timestep of the sequential '
                     'decision-making process. When rewards are only sparsely '
                     'available during an episode, or a rewarding feedback is '
                     'provided only after episode termination, these '
                     'algorithms perform sub-optimally due to the difficultly '
                     'in credit assignment. Alternatively, trajectory-based '
                     'policy optimization methods, such as cross-entropy '
                     'method',
         'author': ['T Gangwani', 'Q Liu', 'J Peng'],
         'cites': '22',
         'eprint': 'https://arxiv.org/pdf/1805.10309',
         'gsrank': '1',
         'title': 'Learning self-imitating diverse policies',
         'url': 'https://arxiv.org/abs/1805.10309',
         'venue': 'arXiv preprint arXiv:1805.10309',
         'year': '2018'},
 'citations_link': '/scholar?cites=9114374846526316019&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BSelf-Imitating%2BDiverse%2BPolicies%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=8930BKLDfH4J&ei=hJMqX-yvN4jHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:8930BKLDfH4J:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqV3w9x86uz6Trexg51gqJbcVwMlms6&scisig=AAGBfm0AAAAAXyqV34VAn39406f31MszArUkw0_tVtM3&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
22
-------------------------------------------------
2020-08-05 11:09:59
Got the results of the query
{'bib': {'abstract': 'To make deep neural networks feasible in '
                     'resource-constrained environments (such as mobile '
                     'devices), it is beneficial to quantize models by using '
                     'low-precision weights. One common technique for '
                     'quantizing neural networks is the straight-through '
                     'gradient method, which enables back-propagation through '
                     'the quantization mapping. Despite its empirical success, '
                     'little is understood about why the straight-through '
                     'gradient method works. Building upon a novel observation '
                     'that the straight-through gradient method is in fact '
                     'identical to the',
         'author': ['Y Bai', 'YX Wang', 'E Liberty'],
         'cites': '33',
         'eprint': 'https://arxiv.org/pdf/1810.00861',
         'gsrank': '1',
         'title': 'Proxquant: Quantized neural networks via proximal operators',
         'url': 'https://arxiv.org/abs/1810.00861',
         'venue': 'arXiv preprint arXiv:1810.00861',
         'year': '2018'},
 'citations_link': '/scholar?cites=13740367040689029941&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProxQuant:%2BQuantized%2BNeural%2BNetworks%2Bvia%2BProximal%2BOperators%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=NUPBHwaXr74J&ei=ipMqX_-zDZqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:NUPBHwaXr74J:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqV5XAjqvLSKDlAKP-Ke2BA7Mn0NiIN&scisig=AAGBfm0AAAAAXyqV5Zx0ZUYOedynfeebBf9587XnigXO&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
33
-------------------------------------------------
2020-08-05 11:10:05
Got the results of the query
{'bib': {'abstract': 'Recurrent neural networks (RNNs) sequentially process '
                     'data by updating their state with each new data point, '
                     'and have long been the de facto choice for sequence '
                     'modeling tasks. However, their inherently sequential '
                     'computation makes them slow to train. Feed-forward',
         'author': ['M Dehghani', 'S Gouws', 'O Vinyals', 'J Uszkoreit'],
         'cites': '176',
         'eprint': 'https://arxiv.org/pdf/1807.03819',
         'gsrank': '1',
         'title': 'Universal transformers',
         'url': 'https://arxiv.org/abs/1807.03819',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8443376534582904234&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUniversal%2BTransformers%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=qsWuKDnmLHUJ&ei=k5MqX-mmBYvrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:qsWuKDnmLHUJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqV8MtKh75Zawrbwy86y-caT4NNUiPe&scisig=AAGBfm0AAAAAXyqV8NTboF1WqRjFVHOpJuQHRXWyNzSK&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
176
-------------------------------------------------
2020-08-05 11:10:16
Got the results of the query
{'bib': {'abstract': 'Although reinforcement learning methods can achieve '
                     'impressive results in simulation, the real world '
                     'presents two major challenges: generating samples is '
                     'exceedingly expensive, and unexpected perturbations or '
                     'unseen situations cause proficient but specialized '
                     'policies to fail at test time. Given that it is '
                     'impractical to train separate policies to accommodate '
                     'all situations the agent may see in the real world, this '
                     'work proposes to learn how to quickly and effectively '
                     'adapt online to new tasks. To enable sample-efficient '
                     'learning, we consider',
         'author': ['A Nagabandi', 'I Clavera', 'S Liu', 'RS Fearing'],
         'cites': '74',
         'eprint': 'https://arxiv.org/pdf/1803.11347',
         'gsrank': '1',
         'title': 'Learning to adapt in dynamic, real-world environments '
                  'through meta-reinforcement learning',
         'url': 'https://arxiv.org/abs/1803.11347',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15863647627484548433&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BAdapt%2Bin%2BDynamic,%2BReal-World%2BEnvironments%2Bthrough%2BMeta-Reinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=UU0vznf_JtwJ&ei=nJMqX4Izi-uZAdy8mqgM',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:UU0vznf_JtwJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqV9oqDaeeoK-YRKqxvhlAJaqtW1Cbq&scisig=AAGBfm0AAAAAXyqV9vm_dOtrAi9YNZT8HmAvXIe3RpKC&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
74
-------------------------------------------------
2020-08-05 11:10:22
Got the results of the query
{'bib': {'abstract': 'We study instancewise feature importance scoring as a '
                     'method for model interpretation. Any such method yields, '
                     'for each predicted instance, a vector of importance '
                     'scores associated with the feature vector. Methods based '
                     'on the Shapley score have been proposed as a fair way of '
                     'computing feature attributions of this kind, but incur '
                     'an exponential complexity in the number of features. '
                     'This combinatorial explosion arises from the definition '
                     'of the Shapley value and prevents these methods from '
                     'being scalable to large data sets and complex',
         'author': ['J Chen', 'L Song', 'MJ Wainwright', 'MI Jordan'],
         'cites': '31',
         'eprint': 'https://arxiv.org/pdf/1808.02610',
         'gsrank': '1',
         'title': 'L-shapley and c-shapley: Efficient model interpretation for '
                  'structured data',
         'url': 'https://arxiv.org/abs/1808.02610',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=13478206087371335896&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DL-Shapley%2Band%2BC-Shapley:%2BEfficient%2BModel%2BInterpretation%2Bfor%2BStructured%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=2KhGnwY1DLsJ&ei=o5MqX7rFGoyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:2KhGnwY1DLsJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqWAjErZdJ3JRCOG0rHnW20tY8Phjq5&scisig=AAGBfm0AAAAAXyqWAuaCsi7WsmL2Kl79g_ffkbvYJZVC&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
31
-------------------------------------------------
2020-08-05 11:10:34
Got the results of the query
{'bib': {'abstract': 'Although deep convolutional networks have achieved '
                     'improved performance in many natural language tasks, '
                     'they have been treated as black boxes because they are '
                     'difficult to interpret. Especially, little is known '
                     'about how they represent language in their intermediate '
                     'layers. In an attempt to understand the representations '
                     'of deep convolutional networks trained on language '
                     'tasks, we show that individual units are selectively '
                     'responsive to specific morphemes, words, and phrases, '
                     'rather than responding to arbitrary and',
         'author': ['S Na', 'YJ Choe', 'DH Lee', 'G Kim'],
         'cites': '2',
         'eprint': 'https://arxiv.org/pdf/1902.07249',
         'gsrank': '1',
         'title': 'Discovery of Natural Language Concepts in Individual Units '
                  'of CNNs',
         'url': 'https://arxiv.org/abs/1902.07249',
         'venue': 'arXiv preprint arXiv:1902.07249',
         'year': '2019'},
 'citations_link': '/scholar?cites=16647657304104807726&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiscovery%2Bof%2BNatural%2BLanguage%2BConcepts%2Bin%2BIndividual%2BUnits%2Bof%2BCNNs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Lq2acR1cCOcJ&ei=t5MqX6mXIYyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Lq2acR1cCOcJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqWF644pSODVc_OHsHoaiqCEarZCCh8&scisig=AAGBfm0AAAAAXyqWF_mw8Y5lLWUruHrAbp5wlTgcgs5M&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
2
-------------------------------------------------
2020-08-05 11:10:55
Got the results of the query
{'bib': {'abstract': 'Despite much effort, deep neural networks remain highly '
                     'susceptible to tiny input perturbations and even for '
                     'MNIST, one of the most common toy datasets in computer '
                     'vision, no neural network model exists for which '
                     'adversarial perturbations are large and make semantic '
                     'sense to humans. We show that even the widely recognized '
                     'and by far most successful defense by Madry et al.(1) '
                     "overfits on the L-infinity metric (it's highly "
                     'susceptible to L2 and L0 perturbations),(2) classifies '
                     'unrecognizable images with high certainty,(3)',
         'author': ['L Schott', 'J Rauber', 'M Bethge', 'W Brendel'],
         'cites': '107',
         'eprint': 'https://arxiv.org/pdf/1805.09190',
         'gsrank': '1',
         'title': 'Towards the first adversarially robust neural network model '
                  'on MNIST',
         'url': 'https://arxiv.org/abs/1805.09190',
         'venue': 'arXiv preprint arXiv:1805.09190',
         'year': '2018'},
 'citations_link': '/scholar?cites=7711598507862406800&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2Bthe%2Bfirst%2Badversarially%2Brobust%2Bneural%2Bnetwork%2Bmodel%2Bon%2BMNIST%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=kDaSL_wZBWsJ&ei=w5MqX8qdCI-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:kDaSL_wZBWsJ:scholar.google.com/&output=citation&scisdr=CgUHBCoMGAA:AAGBfm0AAAAAXyqWIN4zG9OnL7ryJVrNTD0Ps4-k68O-&scisig=AAGBfm0AAAAAXyqWILWhBUKPckIdeqUDs7uOp1xD2p2Y&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
107
-------------------------------------------------
2020-08-05 11:11:04
Got the results of the query
-------------------------------------------------
2020-08-05 11:11:34
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://190.217.13.204:999
Got the results of the query
{'bib': {'abstract': 'The recent direction of unpaired image-to-image '
                     'translation is on one hand very exciting as it '
                     'alleviates the big burden in obtaining label-intensive '
                     'pixel-to-pixel supervision, but it is on the other hand '
                     'not fully satisfactory due to the presence of artifacts '
                     'and degenerated',
         'author': ['R Zhang', 'T Pfister', 'J Li'],
         'cites': '11',
         'eprint': 'https://arxiv.org/pdf/1902.09727',
         'gsrank': '1',
         'title': 'Harmonic unpaired image-to-image translation',
         'url': 'https://arxiv.org/abs/1902.09727',
         'venue': 'arXiv preprint arXiv:1902.09727',
         'year': '2019'},
 'citations_link': '/scholar?cites=9289522450448532503&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHarmonic%2BUnpaired%2BImage-to-image%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=F0QAYHMD64AJ&ei=p5QqX9yIE8KwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:F0QAYHMD64AJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXBeINybltkEdJ1BGDOf55d4P55eHd&scisig=AAGBfm0AAAAAXyqXBW16zqRphhR2FnLnjseyFUlbTgCU&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
11
-------------------------------------------------
2020-08-05 11:14:53
Got the results of the query
{'bib': {'abstract': 'The ability of a reinforcement learning (RL) agent to '
                     'learn about many reward functions at the same time has '
                     'many potential benefits, such as the decomposition of '
                     'complex tasks into simpler ones, the exchange of '
                     'information between tasks, and the reuse of skills. We '
                     'focus',
         'author': ['D Borsa', 'A Barreto', 'J Quan', 'D Mankowitz'],
         'cites': '13',
         'eprint': 'https://arxiv.org/pdf/1812.07626',
         'gsrank': '1',
         'title': 'Universal successor features approximators',
         'url': 'https://arxiv.org/abs/1812.07626',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=14497638041903171358&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUniversal%2BSuccessor%2BFeatures%2BApproximators%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=HgfUwfz0MckJ&ei=spQqX73QDo-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:HgfUwfz0MckJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXDPl8f2iuqJqETNYOF-gt_FVfVSEp&scisig=AAGBfm0AAAAAXyqXDMqLoYKQOWiCir5PzFzCObN02Icc&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
13
-------------------------------------------------
2020-08-05 11:15:00
Got the results of the query
{'bib': {'abstract': 'One of the mysteries in the success of neural networks '
                     'is randomly initialized first order methods like '
                     'gradient descent can achieve zero training loss even '
                     'though the objective function is non-convex and '
                     'non-smooth. This paper demystifies this surprising '
                     'phenomenon for two-layer fully connected ReLU activated '
                     'neural networks. For an $ m $ hidden node shallow neural '
                     'network with ReLU activation and $ n $ training data, we '
                     'show as long as $ m $ is large enough and no two inputs '
                     'are parallel, randomly initialized gradient descent',
         'author': ['SS Du', 'X Zhai', 'B Poczos', 'A Singh'],
         'cites': '308',
         'eprint': 'https://arxiv.org/pdf/1810.02054',
         'gsrank': '1',
         'title': 'Gradient descent provably optimizes over-parameterized '
                  'neural networks',
         'url': 'https://arxiv.org/abs/1810.02054',
         'venue': 'arXiv preprint arXiv:1810.02054',
         'year': '2018'},
 'citations_link': '/scholar?cites=8128763459913409987&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGradient%2BDescent%2BProvably%2BOptimizes%2BOver-parameterized%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=w2VQe0wrz3AJ&ei=t5QqX--YFMKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:w2VQe0wrz3AJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXFVJwgRZPZAVbo7bujoAr5oVYKHRh&scisig=AAGBfm0AAAAAXyqXFbC9wGgtMc0qO86T9QAIcLrgUAzm&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
308
-------------------------------------------------
2020-08-05 11:15:09
Got the results of the query
{'bib': {'abstract': 'In many real-world learning scenarios, features are only '
                     'acquirable at a cost constrained under a budget. In this '
                     'paper, we propose a novel approach for cost-sensitive '
                     'feature acquisition at the prediction-time. The '
                     'suggested method acquires features incrementally based '
                     'on a context-aware feature-value function. We formulate '
                     'the problem in the reinforcement learning paradigm, and '
                     'introduce a reward function based on the utility of each '
                     'feature. Specifically, MC dropout sampling is used to '
                     'measure expected variations of',
         'author': ['M Kachuee', 'O Goldstein', 'K Karkkainen'],
         'cites': '7',
         'eprint': 'https://arxiv.org/pdf/1901.00243',
         'gsrank': '1',
         'title': 'Opportunistic learning: Budgeted cost-sensitive learning '
                  'from data streams',
         'url': 'https://arxiv.org/abs/1901.00243',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=926797319762361897&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOpportunistic%2BLearning:%2BBudgeted%2BCost-Sensitive%2BLearning%2Bfrom%2BData%2BStreams%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=KT44f0Cl3AwJ&ei=wZQqX_3gL4jHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:KT44f0Cl3AwJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXHynDAdwGJM-KqVr7xg-E0sbFNIN2&scisig=AAGBfm0AAAAAXyqXHxZM82sQ5fWCMHf3Mo_xzQadwuBi&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
7
-------------------------------------------------
2020-08-05 11:15:19
Got the results of the query
{'bib': {'abstract': 'This paper addresses the scalability challenge of '
                     'architecture search by formulating the task in a '
                     'differentiable manner. Unlike conventional approaches of '
                     'applying evolution or reinforcement learning over a '
                     'discrete and non-differentiable search space, our method '
                     'is',
         'author': ['H Liu', 'K Simonyan', 'Y Yang'],
         'cites': '780',
         'eprint': 'https://arxiv.org/pdf/1806.09055',
         'gsrank': '1',
         'title': 'Darts: Differentiable architecture search',
         'url': 'https://arxiv.org/abs/1806.09055',
         'venue': 'arXiv preprint arXiv:1806.09055',
         'year': '2018'},
 'citations_link': '/scholar?cites=895422516420751823&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDARTS:%2BDifferentiable%2BArchitecture%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=zzEl1wgubQwJ&ei=y5QqX-P5GY-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:zzEl1wgubQwJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXKO_Pg2sLxCfKX8WTweSQoh9z17bc&scisig=AAGBfm0AAAAAXyqXKLJazZ4SJJN_XC1z8nAWtG_W1Wtl&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
780
-------------------------------------------------
2020-08-05 11:15:28
Got the results of the query
{'bib': {'abstract': 'We study the phenomenon of bias amplification in '
                     'classifiers, wherein a machine learning model learns to '
                     'predict classes with a greater disparity than the '
                     'underlying ground truth. We demonstrate that bias '
                     'amplification can arise via an inductive bias in '
                     'gradient descent',
         'author': ['K Leino', 'E Black', 'M Fredrikson', 'S Sen'],
         'cites': '5',
         'eprint': 'https://arxiv.org/pdf/1812.08999',
         'gsrank': '1',
         'title': 'Feature-wise bias amplification',
         'url': 'https://arxiv.org/abs/1812.08999',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=17718568382375744404&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFeature-Wise%2BBias%2BAmplification%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=lFP5FxEA5fUJ&ei=05QqX__oOsKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:lFP5FxEA5fUJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXLvfDMPvcWi0oazIKFffvW_vfUV8g&scisig=AAGBfm0AAAAAXyqXLqatv1xSclzyQawm4FXX5fbxnxRI&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 11:15:34
Got the results of the query
{'bib': {'abstract': 'In standard generative adversarial network (SGAN), the '
                     'discriminator estimates the probability that the input '
                     'data is real. The generator is trained to increase the '
                     'probability that fake data is real. We argue that it '
                     'should also simultaneously decrease the probability that',
         'author': ['A Jolicoeur-Martineau'],
         'cites': '221',
         'eprint': 'https://arxiv.org/pdf/1807.00734',
         'gsrank': '1',
         'title': 'The relativistic discriminator: a key element missing from '
                  'standard GAN',
         'url': 'https://arxiv.org/abs/1807.00734',
         'venue': 'arXiv preprint arXiv:1807.00734',
         'year': '2018'},
 'citations_link': '/scholar?cites=9348243398459465041&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2Brelativistic%2Bdiscriminator:%2Ba%2Bkey%2Belement%2Bmissing%2Bfrom%2Bstandard%2BGAN%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=UR0Rf9ehu4EJ&ei=2JQqX9aiL7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:UR0Rf9ehu4EJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXN2U77yMY6JLxxLd8L7tnUTdfRK5n&scisig=AAGBfm0AAAAAXyqXN7En59cF-LRNVsKI4I-dBmEqK9FV&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
221
-------------------------------------------------
2020-08-05 11:15:43
Got the results of the query
{'bib': {'abstract': 'Autoencoders provide a powerful framework for learning '
                     'compressed representations by encoding all of the '
                     'information needed to reconstruct a data point in a '
                     'latent code. In some cases, autoencoders can" '
                     'interpolate": By decoding the convex combination of the '
                     'latent codes for two datapoints, the autoencoder can '
                     'produce an output which semantically mixes '
                     'characteristics from the datapoints. In this paper, we '
                     'propose a regularization procedure which encourages '
                     'interpolated outputs to appear more realistic by fooling '
                     'a critic network',
         'author': ['D Berthelot', 'C Raffel', 'A Roy', 'I Goodfellow'],
         'cites': '60',
         'eprint': 'https://arxiv.org/pdf/1807.07543.pdf,',
         'gsrank': '1',
         'title': 'Understanding and improving interpolation in autoencoders '
                  'via an adversarial regularizer',
         'url': 'https://arxiv.org/abs/1807.07543',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=4790513317265776731&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2Band%2BImproving%2BInterpolation%2Bin%2BAutoencoders%2Bvia%2Ban%2BAdversarial%2BRegularizer%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=W6BnF3tSe0IJ&ei=5JQqX_7VOoyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:W6BnF3tSe0IJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXQILUfBcUfatQ4v60-LVZUS_afuUl&scisig=AAGBfm0AAAAAXyqXQN2JDgBQGdauEAH6GU4Jl9eWHVO-&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
60
-------------------------------------------------
2020-08-05 11:15:52
Got the results of the query
{'bib': {'abstract': 'Momentum-based acceleration of stochastic gradient '
                     'descent (SGD) is widely used in deep learning. We '
                     'propose the quasi-hyperbolic momentum algorithm (QHM) as '
                     'an extremely simple alteration of momentum SGD, '
                     'averaging a plain SGD step with a momentum step. We '
                     'describe numerous connections to and identities with '
                     'other algorithms, and we characterize the set of '
                     'two-state optimization algorithms that QHM can recover. '
                     'Finally, we propose a QH variant of Adam called QHAdam, '
                     'and we empirically demonstrate that our',
         'author': ['J Ma', 'D Yarats'],
         'cites': '22',
         'eprint': 'https://arxiv.org/pdf/1810.06801',
         'gsrank': '1',
         'title': 'Quasi-hyperbolic momentum and adam for deep learning',
         'url': 'https://arxiv.org/abs/1810.06801',
         'venue': 'arXiv preprint arXiv:1810.06801',
         'year': '2018'},
 'citations_link': '/scholar?cites=4018448922538302075&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQuasi-hyperbolic%2Bmomentum%2Band%2BAdam%2Bfor%2Bdeep%2Blearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=e1ryHwFmxDcJ&ei=7pQqX4fIIcKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:e1ryHwFmxDcJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXS_8Sch8tqd4ubNAmGp6tNEB4wQ7o&scisig=AAGBfm0AAAAAXyqXSx4RkX2Sqob2HAb9gEd8TA2M7yNt&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
22
-------------------------------------------------
2020-08-05 11:16:03
Got the results of the query
{'bib': {'abstract': 'Mini-batch stochastic gradient descent (SGD) is state of '
                     'the art in large scale distributed training. The scheme '
                     'can reach a linear speedup with respect to the number of '
                     'workers, but this is rarely seen in practice as the '
                     'scheme often suffers from large network delays and '
                     'bandwidth limits. To overcome this communication '
                     'bottleneck recent works propose to reduce the '
                     'communication frequency. An algorithm of this type is '
                     'local SGD that runs SGD independently in parallel on '
                     'different workers and averages the sequences only once '
                     'in a',
         'author': ['SU Stich'],
         'cites': '120',
         'eprint': 'https://arxiv.org/pdf/1805.09767',
         'gsrank': '1',
         'title': 'Local SGD converges fast and communicates little',
         'url': 'https://arxiv.org/abs/1805.09767',
         'venue': 'arXiv preprint arXiv:1805.09767',
         'year': '2018'},
 'citations_link': '/scholar?cites=17405656068558747431&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLocal%2BSGD%2BConverges%2BFast%2Band%2BCommunicates%2BLittle%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=J_cQqflPjfEJ&ei=-ZQqX4fkLaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:J_cQqflPjfEJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXV-Py7zUj6lgQy2SerjoSopZZ6TGF&scisig=AAGBfm0AAAAAXyqXV8exjp_aUlNHhC6d_WAqD1gmB02i&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
120
-------------------------------------------------
2020-08-05 11:16:15
Got the results of the query
{'bib': {'abstract': 'Recurrent neural networks (RNNs) are an effective '
                     'representation of control policies for a wide range of '
                     'reinforcement and imitation learning problems. RNN '
                     'policies, however, are particularly difficult to '
                     'explain, understand, and analyze due to their use of '
                     'continuous-valued memory vectors and observation '
                     'features. In this paper, we introduce a new technique, '
                     'Quantized Bottleneck Insertion, to learn finite '
                     'representations of these vectors and features. The '
                     'result is a quantized representation of the RNN that can '
                     'be analyzed to',
         'author': ['A Koul', 'S Greydanus', 'A Fern'],
         'cites': '17',
         'eprint': 'https://arxiv.org/pdf/1811.12530',
         'gsrank': '1',
         'title': 'Learning finite state representations of recurrent policy '
                  'networks',
         'url': 'https://arxiv.org/abs/1811.12530',
         'venue': 'arXiv preprint arXiv:1811.12530',
         'year': '2018'},
 'citations_link': '/scholar?cites=9225081332116410200&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BFinite%2BState%2BRepresentations%2Bof%2BRecurrent%2BPolicy%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=WM_iUJgSBoAJ&ei=A5UqX8fYJ7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:WM_iUJgSBoAJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXYeLilCO4vbghzOUxAvvIOeyYP_DM&scisig=AAGBfm0AAAAAXyqXYeSeHCoBEAyV8P12zEHifz8VuYCF&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
17
-------------------------------------------------
2020-08-05 11:16:25
Got the results of the query
{'bib': {'abstract': 'Multilingual machine translation, which translates '
                     'multiple languages with a single model, has attracted '
                     'much attention due to its efficiency of offline training '
                     'and online serving. However, traditional multilingual '
                     'translation usually yields inferior accuracy compared '
                     'with the counterpart using individual models for each '
                     'language pair, due to language diversity and model '
                     'capacity limitations. In this paper, we propose a '
                     'distillation-based approach to boost the accuracy of '
                     'multilingual machine translation. Specifically, '
                     'individual models are',
         'author': ['X Tan', 'Y Ren', 'D He', 'T Qin', 'Z Zhao', 'TY Liu'],
         'cites': '45',
         'eprint': 'https://arxiv.org/pdf/1902.10461',
         'gsrank': '1',
         'title': 'Multilingual neural machine translation with knowledge '
                  'distillation',
         'url': 'https://arxiv.org/abs/1902.10461',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=5753623392275205285&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMultilingual%2BNeural%2BMachine%2BTranslation%2Bwith%2BKnowledge%2BDistillation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=pQzxw_352E8J&ei=DZUqX_TQE6iBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:pQzxw_352E8J:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXa34Y7nO_n8shux2OEuqUpFm6UswT&scisig=AAGBfm0AAAAAXyqXa-sQ2zpgyuxMyShqc_9B4dtbrKKD&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
45
-------------------------------------------------
2020-08-05 11:16:35
Got the results of the query
{'bib': {'abstract': 'Generative adversarial networks (GANs) have been shown '
                     'to provide an effective way to model complex '
                     'distributions and have obtained impressive results on '
                     'various challenging tasks. However, typical GANs require '
                     'fully-observed data during training. In this paper, we',
         'author': ['SCX Li', 'B Jiang', 'B Marlin'],
         'cites': '28',
         'eprint': 'https://arxiv.org/pdf/1902.09599',
         'gsrank': '1',
         'title': 'Misgan: Learning from incomplete data with generative '
                  'adversarial networks',
         'url': 'https://arxiv.org/abs/1902.09599',
         'venue': 'arXiv preprint arXiv:1902.09599',
         'year': '2019'},
 'citations_link': '/scholar?cites=4415656656646533426&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMisGAN:%2BLearning%2Bfrom%2BIncomplete%2BData%2Bwith%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=MnGBhVWQRz0J&ei=HZUqX_DcJKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:MnGBhVWQRz0J:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXej89kqMN87T2jsdrr_IyPBhFyFI6&scisig=AAGBfm0AAAAAXyqXevX5LWQ4uyzz60sydhMoeYefq9kP&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
28
-------------------------------------------------
2020-08-05 11:16:50
Got the results of the query
{'bib': {'abstract': 'Deep neural networks have been shown to perform well in '
                     'many classical machine learning problems, especially in '
                     'image classification tasks. However, researchers have '
                     'found that neural networks can be easily fooled, and '
                     'they are surprisingly sensitive to small perturbations '
                     'imperceptible to humans. Carefully crafted input images '
                     '(adversarial examples) can force a well-trained neural '
                     'network to provide arbitrary outputs. Including '
                     'adversarial examples during training is a popular '
                     'defense mechanism against adversarial',
         'author': ['H Wang', 'CN Yu'],
         'cites': '19',
         'eprint': 'https://arxiv.org/pdf/1905.09591',
         'gsrank': '1',
         'title': 'A direct approach to robust deep learning using adversarial '
                  'networks',
         'url': 'https://arxiv.org/abs/1905.09591',
         'venue': 'arXiv preprint arXiv:1905.09591',
         'year': '2019'},
 'citations_link': '/scholar?cites=2332293430655643076&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BDirect%2BApproach%2Bto%2BRobust%2BDeep%2BLearning%2BUsing%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=xO00poT4XSAJ&ei=JpUqX7bkDovrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:xO00poT4XSAJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXgFK4GeV9Q-IEQJJUs6J9UXewo3zC&scisig=AAGBfm0AAAAAXyqXgFv61NJ0NiBehSio0dkg6MN1mCp3&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
19
-------------------------------------------------
2020-08-05 11:16:56
Got the results of the query
{'bib': {'abstract': 'Binarized Neural Networks (BNNs) have recently attracted '
                     'significant interest due to their computational '
                     'efficiency. Concurrently, it has been shown that neural '
                     'networks may be overly sensitive to" attacks"-tiny '
                     'adversarial changes in the input-which may be '
                     'detrimental to their use in safety-critical domains. '
                     'Designing attack algorithms that effectively fool '
                     'trained models is a key step towards learning robust '
                     'neural networks. The discrete, non-differentiable nature '
                     'of BNNs, which distinguishes them from their '
                     'full-precision',
         'author': ['EB Khalil', 'A Gupta', 'B Dilkina'],
         'cites': '16',
         'eprint': 'https://arxiv.org/pdf/1810.03538',
         'gsrank': '1',
         'title': 'Combinatorial attacks on binarized neural networks',
         'url': 'https://arxiv.org/abs/1810.03538',
         'venue': 'arXiv preprint arXiv:1810.03538',
         'year': '2018'},
 'citations_link': '/scholar?cites=16660057879161292403&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCombinatorial%2BAttacks%2Bon%2BBinarized%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=c0bbUF9qNOcJ&ei=L5UqX_2FE4jHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:c0bbUF9qNOcJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXjF7BzkwgJDjgxw115QFe7z-MPE1l&scisig=AAGBfm0AAAAAXyqXjEQBcH1qL5JljLfxjEL_e5bApBGo&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
16
-------------------------------------------------
2020-08-05 11:17:08
Got the results of the query
{'bib': {'abstract': 'Image-to-image translation has recently received '
                     'significant attention due to advances in deep learning. '
                     'Most works focus on learning either a one-to-one mapping '
                     'in an unsupervised way or a many-to-many mapping in a '
                     'supervised way. However, a more practical setting is '
                     'many-to-many mapping in an unsupervised way, which is '
                     'harder due to the lack of supervision and the complex '
                     'inner-and cross-domain variations. To alleviate these '
                     'issues, we propose the Exemplar Guided & Semantically '
                     'Consistent Image-to-image',
         'author': ['L Ma', 'X Jia', 'S Georgoulis', 'T Tuytelaars'],
         'cites': '45',
         'eprint': 'https://arxiv.org/pdf/1805.11145',
         'gsrank': '1',
         'title': 'Exemplar guided unsupervised image-to-image translation '
                  'with semantic consistency',
         'url': 'https://arxiv.org/abs/1805.11145',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=6219807346238873083&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DExemplar%2BGuided%2BUnsupervised%2BImage-to-Image%2BTranslation%2Bwith%2BSemantic%2BConsistency%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=-93u09MxUVYJ&ei=OJUqX6O4KbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:-93u09MxUVYJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXl-2KHxEcUjOrkYjUeWvOVphm4Nta&scisig=AAGBfm0AAAAAXyqXl5qmP4oi9niqR23-RIxa_GUfB8SQ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
45
-------------------------------------------------
2020-08-05 11:17:19
Got the results of the query
{'bib': {'abstract': 'To backpropagate the gradients through stochastic binary '
                     'layers, we propose the augment-REINFORCE-merge (ARM) '
                     'estimator that is unbiased, exhibits low variance, and '
                     'has low computational complexity. Exploiting variable '
                     'augmentation, REINFORCE, and reparameterization, the ARM '
                     'estimator achieves adaptive variance reduction for Monte '
                     'Carlo integration by merging two expectations via common '
                     'random numbers. The variance-reduction mechanism of the '
                     'ARM estimator can also be attributed to either '
                     'antithetic',
         'author': ['M Yin', 'M Zhou'],
         'cites': '19',
         'eprint': 'https://openreview.net/pdf?id=S1lg0jAcYm',
         'gsrank': '1',
         'title': 'ARM: Augment-REINFORCE-merge gradient for stochastic binary '
                  'networks',
         'url': 'https://openreview.net/forum?id=S1lg0jAcYm&noteId=rklw0lkOA7',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1199474822347449770&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DARM:%2BAugment-REINFORCE-Merge%2BGradient%2Bfor%2BStochastic%2BBinary%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=qmUL7_5jpRAJ&ei=RZUqX7X2O4jHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:qmUL7_5jpRAJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXon2i25jggt8pkToWCpRyMqsXGNK1&scisig=AAGBfm0AAAAAXyqXoqYHp-o3r4q5OT_f7q-IlyZ1CGNQ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
19
-------------------------------------------------
2020-08-05 11:17:31
Got the results of the query
{'bib': {'abstract': 'We propose a neural machine-reading model that '
                     'constructs dynamic knowledge graphs from procedural '
                     'text. It builds these graphs recurrently for each step '
                     'of the described procedure, and uses them to track the '
                     'evolving states of participant entities. We harness and '
                     'extend a recently proposed machine reading comprehension '
                     '(MRC) model to query for entity states, since these '
                     'states are generally communicated in spans of text and '
                     'MRC models perform well in extracting entity-centric '
                     'spans. The explicit, structured, and evolving',
         'author': ['R Das', 'T Munkhdalai', 'X Yuan', 'A Trischler'],
         'cites': '30',
         'eprint': 'https://arxiv.org/pdf/1810.05682',
         'gsrank': '1',
         'title': 'Building dynamic knowledge graphs from text using machine '
                  'reading comprehension',
         'url': 'https://arxiv.org/abs/1810.05682',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=6748557175668250759&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBuilding%2BDynamic%2BKnowledge%2BGraphs%2Bfrom%2BText%2Busing%2BMachine%2BReading%2BComprehension%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=h5ge8v6wp10J&ei=UZUqX_ORKo-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:h5ge8v6wp10J:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXs71QF-NOU9tfyIdzFgl81N0ZDXt6&scisig=AAGBfm0AAAAAXyqXs-5ZRuy5lqftFV0THQSOS1XgO1ZJ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
30
-------------------------------------------------
2020-08-05 11:17:47
Got the results of the query
{'bib': {'abstract': 'Many real world tasks exhibit rich structure that is '
                     'repeated across different parts of the state space or in '
                     'time. In this work we study the possibility of '
                     'leveraging such repeated structure to speed up and '
                     'regularize learning. We start from the KL regularized '
                     'expected reward objective which introduces an additional '
                     'component, a default policy. Instead of relying on a '
                     'fixed default policy, we learn it from data. But '
                     'crucially, we restrict the amount of information the '
                     'default policy receives, forcing it to learn reusable '
                     'behaviors that help the policy learn',
         'author': ['A Galashov', 'SM Jayakumar', 'L Hasenclever'],
         'cites': '17',
         'eprint': 'https://arxiv.org/pdf/1905.01240',
         'gsrank': '1',
         'title': 'Information asymmetry in KL-regularized RL',
         'url': 'https://arxiv.org/abs/1905.01240',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=12116543825498538093&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInformation%2Basymmetry%2Bin%2BKL-regularized%2BRL%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=bQBLwGmcJqgJ&ei=XpUqX76ZNsKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:bQBLwGmcJqgJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXvRliIuV9mASVsdlOQt864zgGIAAg&scisig=AAGBfm0AAAAAXyqXvZny1urI3FynoXGzOl9RTet6EkYT&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
17
-------------------------------------------------
2020-08-05 11:17:57
Got the results of the query
{'bib': {'abstract': 'In this work, we address the problem of musical timbre '
                     'transfer, where the goal is to manipulate the timbre of '
                     'a sound sample from one instrument to match another '
                     'instrument while preserving other musical content, such '
                     'as pitch, rhythm, and loudness. In principle, one could '
                     'apply image-based style transfer techniques to a '
                     'time-frequency representation of an audio signal, but '
                     'this depends on having a representation that allows '
                     'independent manipulation of timbre as well as '
                     'high-quality waveform generation. We introduce',
         'author': ['S Huang', 'Q Li', 'C Anil', 'X Bao', 'S Oore'],
         'cites': '19',
         'eprint': 'https://arxiv.org/pdf/1811.09620',
         'gsrank': '1',
         'title': 'Timbretron: A wavenet (cyclegan (cqt (audio))) pipeline for '
                  'musical timbre transfer',
         'url': 'https://arxiv.org/abs/1811.09620',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11196022310662002190&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTimbreTron:%2BA%2BWaveNet(CycleGAN(CQT(Audio)))%2BPipeline%2Bfor%2BMusical%2BTimbre%2BTransfer%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=DlZb9PhCYJsJ&ei=apUqX-O_G6OGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:DlZb9PhCYJsJ:scholar.google.com/&output=citation&scisdr=CgU_pp7qGAA:AAGBfm0AAAAAXyqXyXyzOMNlGbu1enl46aIgJu_N61m8&scisig=AAGBfm0AAAAAXyqXybs8ni1w0pLJLG7EOCDa7oiZsnBF&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
19
-------------------------------------------------
2020-08-05 11:18:09
Got the results of the query
-------------------------------------------------
2020-08-05 11:18:46
Trying new proxy
Working proxy: http://140.238.17.59:80
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://206.198.131.142:80
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://85.47.31.179:3128
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://200.255.122.170:8080
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://51.38.134.168:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Got the results of the query
-------------------------------------------------
2020-08-05 11:35:27
Got the results of the query
{'bib': {'abstract': 'Neural networks are vulnerable to small adversarial '
                     'perturbations. While existing literature largely focused '
                     'on the vulnerability of learned models, we demonstrate '
                     'an intriguing phenomenon that adversarial robustness, '
                     'unlike clean accuracy, is sensitive to the input data '
                     'distribution. Even a semantics-preserving '
                     'transformations on the input data distribution can cause '
                     'a significantly different robustness for the '
                     'adversarially trained model that is both trained and '
                     'evaluated on the new distribution. We show this by '
                     'constructing',
         'author': ['GW Ding', 'KYC Lui', 'X Jin', 'L Wang', 'R Huang'],
         'cites': '15',
         'eprint': 'http://openaccess.thecvf.com/content_CVPRW_2019/papers/Uncertainty%20and%20Robustness%20in%20Deep%20Visual%20Learning/Ding_On_the_Sensitivity_of_Adversarial_Robustness_to_Input_Data_Distributions_CVPRW_2019_paper.pdf',
         'gsrank': '1',
         'title': 'On the Sensitivity of Adversarial Robustness to Input Data '
                  'Distributions.',
         'url': 'http://openaccess.thecvf.com/content_CVPRW_2019/papers/Uncertainty%20and%20Robustness%20in%20Deep%20Visual%20Learning/Ding_On_the_Sensitivity_of_Adversarial_Robustness_to_Input_Data_Distributions_CVPRW_2019_paper.pdf',
         'venue': 'ICLR (Poster)',
         'year': '2019'},
 'citations_link': '/scholar?cites=925121948530123203&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BSensitivity%2Bof%2BAdversarial%2BRobustness%2Bto%2BInput%2BData%2BDistributions%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=wyEOvIKx1gwJ&ei=hZkqX-3DM4-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:wyEOvIKx1gwJ:scholar.google.com/&output=citation&scisdr=CgWyM1kLGAA:AAGBfm0AAAAAXyqb476gc18X23kmRZ2t9VkXmu79RNiY&scisig=AAGBfm0AAAAAXyqb45AblOkmpoTqR7HW0ZN4dWodp1eI&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 11:35:39
Trying new proxy
Working proxy: http://103.28.121.58:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://117.20.26.21:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Got the results of the query
{'bib': {'abstract': 'The human ability to recognize objects is impaired when '
                     'the object is not shown in full." Minimal images" are '
                     'the smallest regions of an image that remain '
                     'recognizable for humans. Ullman et al. 2016 show that a '
                     'slight modification of the location and size of the '
                     'visible region of the minimal image produces a sharp '
                     'drop in human recognition accuracy. In this paper, we '
                     'demonstrate that such drops in accuracy due to changes '
                     'of the visible region are a common phenomenon between '
                     'humans and existing state-of-the-art deep neural',
         'author': ['S Srivastava', 'G Ben-Yosef', 'X Boix'],
         'cites': '7',
         'eprint': 'https://arxiv.org/pdf/1902.03227',
         'gsrank': '1',
         'title': 'Minimal images in deep neural networks: Fragile object '
                  'recognition in natural images',
         'url': 'https://arxiv.org/abs/1902.03227',
         'venue': 'arXiv preprint arXiv:1902.03227',
         'year': '2019'},
 'citations_link': '/scholar?cites=9004031207048584319&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMinimal%2BImages%2Bin%2BDeep%2BNeural%2BNetworks:%2BFragile%2BObject%2BRecognition%2Bin%2BNatural%2BImages%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=f5ypaq2-9HwJ&ei=bJoqX9u0K4KTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:f5ypaq2-9HwJ:scholar.google.com/&output=citation&scisdr=CgVGh4uNGAA:AAGBfm0AAAAAXyqcyvc8qHJaTlrBK2TB3toMxuDo83sM&scisig=AAGBfm0AAAAAXyqcymA4ga9bju9krsoAgP1wZzXyC5bI&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
7
-------------------------------------------------
2020-08-05 11:39:30
Got the results of the query
-------------------------------------------------
2020-08-05 11:39:51
Trying new proxy
Working proxy: http://212.57.22.178:8080
Got the results of the query
{'bib': {'abstract': 'Sequence-to-sequence models are commonly trained via '
                     'maximum likelihood estimation (MLE). However, standard '
                     'MLE training considers a word-level objective, '
                     'predicting the next word given the previous ground-truth '
                     'partial sentence. This procedure focuses on modeling '
                     'local syntactic patterns, and may fail to capture '
                     'long-range semantic structure. We present a novel '
                     'solution to alleviate these issues. Our approach imposes '
                     'global sequence-level guidance via new supervision based '
                     'on optimal transport, enabling the overall',
         'author': ['L Chen', 'Y Zhang', 'R Zhang', 'C Tao', 'Z Gan'],
         'cites': '19',
         'eprint': 'https://arxiv.org/pdf/1901.06283',
         'gsrank': '1',
         'title': 'Improving sequence-to-sequence learning via optimal '
                  'transport',
         'url': 'https://arxiv.org/abs/1901.06283',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=9397580809910077889&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BSequence-to-Sequence%2BLearning%2Bvia%2BOptimal%2BTransport%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=wcm3wvTpaoIJ&ei=uZoqX5biFKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:wcm3wvTpaoIJ:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqdIbiEyn6Pa1b1RyM_EIyObrBMn3Kr&scisig=AAGBfm0AAAAAXyqdIRVwGuHE9u3dlqKhQFKa5lYnrLWj&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
19
-------------------------------------------------
2020-08-05 11:40:58
Got the results of the query
{'bib': {'abstract': 'Machine learning has the potential to assist many '
                     'communities in using the large datasets that are '
                     'becoming more and more available. Unfortunately, much of '
                     'that potential is not being realized because it would '
                     'require sharing data in a way that compromises privacy. '
                     'In this paper, we investigate a method for ensuring '
                     '(differential) privacy of the generator of the '
                     'Generative Adversarial Nets (GAN) framework. The '
                     'resulting model can be used for generating synthetic '
                     'data on which algorithms can be trained and validated, '
                     'and on which',
         'author': ['J Jordon', 'J Yoon', 'M van der Schaar'],
         'cites': '33',
         'eprint': 'https://openreview.net/pdf?id=S1zk9iRqF7',
         'gsrank': '1',
         'title': 'PATE-GAN: Generating synthetic data with differential '
                  'privacy guarantees',
         'url': 'https://openreview.net/forum?id=S1zk9iRqF7&noteId=B1x60Mr0aQ',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=9154435771423490892&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPATE-GAN:%2BGenerating%2BSynthetic%2BData%2Bwith%2BDifferential%2BPrivacy%2BGuarantees%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=TIPT8tQWC38J&ei=1JoqX67mL4jHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:TIPT8tQWC38J:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqdNGcI7eOBw1ZKxx-rorZk73yWbJ1d&scisig=AAGBfm0AAAAAXyqdNNXtvpQcE1gPMCr2D_PU12LBTu30&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
33
-------------------------------------------------
2020-08-05 11:41:16
Got the results of the query
{'bib': {'abstract': 'We consider the problem of using variational '
                     'latent-variable models for data compression. For such '
                     'models to produce a compressed binary sequence, which is '
                     'the universal data representation in a digital world, '
                     'the latent representation needs to be subjected to '
                     'entropy coding. Range coding as an entropy coding '
                     'technique is optimal, but it can fail catastrophically '
                     'if the computation of the prior differs even slightly '
                     'between the sending and the receiving side. '
                     'Unfortunately, this is a common scenario when floating '
                     'point math is used',
         'author': ['J Ballé', 'N Johnston', 'D Minnen'],
         'cites': '5',
         'eprint': 'https://openreview.net/pdf?id=S1zz2i0cY7',
         'gsrank': '1',
         'title': 'Integer networks for data compression with latent-variable '
                  'models',
         'url': 'https://openreview.net/forum?id=S1zz2i0cY7',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8035159381180309667&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInteger%2BNetworks%2Bfor%2BData%2BCompression%2Bwith%2BLatent-Variable%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=o2DmlOKegm8J&ei=4ZoqX-bHE8KwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:o2DmlOKegm8J:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqdQeruJMrKyGqA_KHfy-pzjkc8jU30&scisig=AAGBfm0AAAAAXyqdQRl6AIcRVx_hA8ms3qkMaqyMdpv7&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 11:41:29
Got the results of the query
{'bib': {'abstract': 'We present Value Propagation (VProp), a set of '
                     'parameter-efficient differentiable planning modules '
                     'built on Value Iteration which can successfully be '
                     'trained using reinforcement learning to solve unseen '
                     'tasks, has the capability to generalize to larger map '
                     'sizes, and can',
         'author': ['N Nardelli', 'G Synnaeve', 'Z Lin', 'P Kohli'],
         'cites': '9',
         'eprint': 'https://arxiv.org/pdf/1805.11199',
         'gsrank': '1',
         'title': 'Value propagation networks',
         'url': 'https://arxiv.org/abs/1805.11199',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=9180230208406770561&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DValue%2BPropagation%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=gVNlULy6Zn8J&ei=75oqX9fgMIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:gVNlULy6Zn8J:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqdT0CxLiV2xOqBO3SHJnhM9qyO3sXa&scisig=AAGBfm0AAAAAXyqdT7lugk5Jty2xnjwXPZMnpKhcBBD6&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 11:41:43
Got the results of the query
{'bib': {'abstract': 'Addressing uncertainty is critical for autonomous '
                     'systems to robustly adapt to the real world. We '
                     'formulate the problem of model uncertainty as a '
                     'continuous Bayes-Adaptive Markov Decision Process '
                     '(BAMDP), where an agent maintains a posterior '
                     'distribution over latent model parameters given a '
                     'history of observations and maximizes its expected '
                     'long-term reward with respect to this belief '
                     'distribution. Our algorithm, Bayesian Policy '
                     'Optimization, builds on recent policy optimization '
                     'algorithms to learn a universal policy that navigates '
                     'the',
         'author': ['G Lee', 'B Hou', 'A Mandalika', 'J Lee', 'S Choudhury'],
         'cites': '13',
         'eprint': 'https://arxiv.org/pdf/1810.01014',
         'gsrank': '1',
         'title': 'Bayesian policy optimization for model uncertainty',
         'url': 'https://arxiv.org/abs/1810.01014',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1420595428589112582&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBayesian%2BPolicy%2BOptimization%2Bfor%2BModel%2BUncertainty%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Bg2ZAgT4thMJ&ei=-5oqX8H8DKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Bg2ZAgT4thMJ:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqdWZ28AOlafNn-e-RbowNBH_Hcf3kc&scisig=AAGBfm0AAAAAXyqdWbnXjoO8oGT1EUkFSiBQNoS9-5_8&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
13
-------------------------------------------------
2020-08-05 11:41:53
Got the results of the query
{'bib': {'abstract': 'Bayesian phylogenetic inference is currently done via '
                     'Markov chain Monte Carlo with simple mechanisms for '
                     'proposing new states, which hinders exploration '
                     'efficiency and often requires long runs to deliver '
                     'accurate posterior estimates. In this paper we present '
                     'an alternative approach: a variational framework for '
                     'Bayesian phylogenetic analysis. We approximate the true '
                     'posterior using an expressive graphical model for tree '
                     'distributions, called a subsplit Bayesian network, '
                     'together with appropriate branch length distributions',
         'author': ['C Zhang', 'FA Matsen IV'],
         'cites': '4',
         'eprint': 'https://openreview.net/pdf?id=SJVmjjR9FX',
         'gsrank': '1',
         'title': 'Variational Bayesian phylogenetic inference',
         'url': 'https://openreview.net/forum?id=SJVmjjR9FX',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=6268977396925453691&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BBayesian%2BPhylogenetic%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=e9WxYLrh_1YJ&ei=BpsqX-WEHcKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:e9WxYLrh_1YJ:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqdZjxYXGiZEHNAxaG_YW-Urpc9bWNQ&scisig=AAGBfm0AAAAAXyqdZnLwkW5EJQ8pCg4rqMyb_EKaZ4l1&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
4
-------------------------------------------------
2020-08-05 11:42:06
Got the results of the query
{'bib': {'abstract': 'Domain adaptation for visual recognition has undergone '
                     'great progress in the past few years. Nevertheless, most '
                     'existing methods work in the so-called closed-set '
                     'scenario, assuming that the classes depicted by the '
                     'target images are exactly the same as those of the '
                     'source domain. In this paper, we tackle the more '
                     'challenging, yet more realistic case of open-set domain '
                     'adaptation, where new, unknown classes can be present in '
                     'the target data. While, in the unsupervised scenario, '
                     'one cannot expect to be able to identify each specific',
         'author': ['M Baktashmotlagh', 'M Faraki', 'T Drummond'],
         'cites': '18',
         'eprint': 'https://arxiv.org/pdf/1805.12277',
         'gsrank': '1',
         'title': 'Learning factorized representations for open-set domain '
                  'adaptation',
         'url': 'https://arxiv.org/abs/1805.12277',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=991348150001731721&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BFactorized%2BRepresentations%2Bfor%2BOpen-Set%2BDomain%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=iQiY2eP5wQ0J&ei=E5sqX-iIJLGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:iQiY2eP5wQ0J:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqdc_QtHrvBe_Oc_9bfwaUX0YvjhAvZ&scisig=AAGBfm0AAAAAXyqdcxhIAbb87IOBZ6ineyRrByPcrTop&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
18
-------------------------------------------------
2020-08-05 11:42:20
Got the results of the query
{'bib': {'abstract': 'Compression is a key step to deploy large neural '
                     'networks on resource-constrained platforms. As a popular '
                     'compression technique, quantization constrains the '
                     'number of distinct weight values and thus reducing the '
                     'number of bits required to represent and store each '
                     'weight. In this paper, we study the representation power '
                     'of quantized neural networks. First, we prove the '
                     'universal approximability of quantized ReLU networks on '
                     'a wide class of functions. Then we provide upper bounds '
                     'on the number of weights and the memory size for',
         'author': ['Y Ding', 'J Liu', 'J Xiong', 'Y Shi'],
         'cites': '9',
         'eprint': 'https://arxiv.org/pdf/1802.03646',
         'gsrank': '1',
         'title': 'On the universal approximability and complexity bounds of '
                  'quantized relu neural networks',
         'url': 'https://arxiv.org/abs/1802.03646',
         'venue': 'arXiv preprint arXiv:1802.03646',
         'year': '2018'},
 'citations_link': '/scholar?cites=9956095606815471877&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BUniversal%2BApproximability%2Band%2BComplexity%2BBounds%2Bof%2BQuantized%2BReLU%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=BTkQajQoK4oJ&ei=IZsqX5qxFsKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:BTkQajQoK4oJ:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqdgowyGU_Anrcgf3o3Q51upBdHabDf&scisig=AAGBfm0AAAAAXyqdgv7UjgxxUHMOru0zLwWvoIL44Xdu&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 11:42:35
Got the results of the query
{'bib': {'abstract': 'Point clouds are an important type of geometric data and '
                     'have widespread use in computer graphics and vision. '
                     'However, learning representations for point clouds is '
                     'particularly challenging due to their nature as being an '
                     'unordered collection of points irregularly distributed '
                     'in 3D space. Graph convolution, a generalization of the '
                     'convolution operation for data defined over graphs, has '
                     'been recently shown to be very successful at extracting '
                     'localized features from point clouds in supervised or '
                     'semi-supervised tasks such as',
         'author': ['D Valsesia', 'G Fracastoro', 'E Magli'],
         'cites': '23',
         'eprint': 'https://openreview.net/pdf?id=SJeXSo09FQ',
         'gsrank': '1',
         'title': 'Learning localized generative models for 3d point clouds '
                  'via graph convolution',
         'url': 'https://openreview.net/forum?id=SJeXSo09FQ&source=post_page---------------------------',
         'venue': 'International conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=9880881475010180729&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BLocalized%2BGenerative%2BModels%2Bfor%2B3D%2BPoint%2BClouds%2Bvia%2BGraph%2BConvolution%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=eTa4sVnxH4kJ&ei=OZsqX-rYIaiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:eTa4sVnxH4kJ:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqdl0IZcDFumQ5gtnasTA1JM70VQ535&scisig=AAGBfm0AAAAAXyqdlxQCG7f3FTmkMe1so5cy3V7kWFss&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
23
-------------------------------------------------
2020-08-05 11:42:56
Got the results of the query
{'bib': {'abstract': 'Langevin diffusion is a powerful method for nonconvex '
                     'optimization, which enables the escape from local minima '
                     'by injecting noise into the gradient. In particular, the '
                     'temperature parameter controlling the noise level gives '
                     'rise to a tradeoff between``global '
                     "exploration''and``local exploitation'', which correspond "
                     'to high and low temperatures. To attain the advantages '
                     'of both regimes, we propose to use replica exchange, '
                     'which swaps between two Langevin diffusions with '
                     'different temperatures. We theoretically analyze the',
         'author': ['Y Chen', 'J Chen', 'J Dong', 'J Peng', 'Z Wang'],
         'cites': '6',
         'eprint': 'https://arxiv.org/pdf/2007.01990',
         'gsrank': '1',
         'title': 'Accelerating nonconvex learning via replica exchange '
                  'Langevin diffusion',
         'url': 'https://arxiv.org/abs/2007.01990',
         'venue': 'arXiv preprint arXiv …',
         'year': '2020'},
 'citations_link': '/scholar?cites=8840692952355845551&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAccelerating%2BNonconvex%2BLearning%2Bvia%2BReplica%2BExchange%2BLangevin%2Bdiffusion%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=rzV4cWZzsHoJ&ei=SJsqX-ORFqiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:rzV4cWZzsHoJ:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqdpq6RF9X0xLZ3pKcMZuB6Gkk7cwCF&scisig=AAGBfm0AAAAAXyqdpjDSd9Q94ryz7GzSjIXb3ac76pko&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 11:43:10
Got the results of the query
{'bib': {'abstract': 'In this paper, we propose a new control framework called '
                     'the moving endpoint control to restore images corrupted '
                     'by different degradation levels in one model. The '
                     'proposed control problem contains a restoration dynamics '
                     'which is modeled by an RNN. The moving endpoint, which '
                     'is essentially the terminal time of the associated '
                     'dynamics, is determined by a policy network. We call the '
                     'proposed model the dynamically unfolding recurrent '
                     'restorer (DURR). Numerical experiments show that DURR is '
                     'able to achieve state-of-the-art',
         'author': ['X Zhang', 'Y Lu', 'J Liu', 'B Dong'],
         'cites': '23',
         'eprint': 'https://arxiv.org/pdf/1805.07709',
         'gsrank': '1',
         'title': 'Dynamically unfolding recurrent restorer: A moving endpoint '
                  'control method for image restoration',
         'url': 'https://arxiv.org/abs/1805.07709',
         'venue': 'arXiv preprint arXiv:1805.07709',
         'year': '2018'},
 'citations_link': '/scholar?cites=5404835983364980694&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDynamically%2BUnfolding%2BRecurrent%2BRestorer:%2BA%2BMoving%2BEndpoint%2BControl%2BMethod%2Bfor%2BImage%2BRestoration%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=1v-32bDVAUsJ&ei=U5sqX5-QEpqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:1v-32bDVAUsJ:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqdsohikiIdwRGXD5ljzFWZ0aEZ3b3b&scisig=AAGBfm0AAAAAXyqdsmGYd41cNGgaI55NYvf6Y69In-55&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
23
-------------------------------------------------
2020-08-05 11:43:23
Got the results of the query
{'bib': {'abstract': 'We consider the problem of uncertainty estimation in the '
                     'context of (non-Bayesian) deep neural classification. In '
                     'this context, all known methods are based on extracting '
                     'uncertainty signals from a trained network optimized to '
                     'solve the classification problem at hand. We demonstrate '
                     'that such techniques tend to introduce biased estimates '
                     'for instances whose predictions are supposed to be '
                     'highly confident. We argue that this deficiency is an '
                     'artifact of the dynamics of training with SGD-like '
                     'optimizers, and it has some properties similar to',
         'author': ['Y Geifman', 'G Uziel', 'R El-Yaniv'],
         'cites': '19',
         'eprint': 'https://arxiv.org/pdf/1805.08206',
         'gsrank': '1',
         'title': 'Bias-reduced uncertainty estimation for deep neural '
                  'classifiers',
         'url': 'https://arxiv.org/abs/1805.08206',
         'venue': 'arXiv preprint arXiv:1805.08206',
         'year': '2018'},
 'citations_link': '/scholar?cites=12377365363481099199&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBias-Reduced%2BUncertainty%2BEstimation%2Bfor%2BDeep%2BNeural%2BClassifiers%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=v7uDUTg8xasJ&ei=X5sqX-IjiMeYAdXakKgM',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:v7uDUTg8xasJ:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqdvpF3DACibde2SrWtU2n9hLvDnuXV&scisig=AAGBfm0AAAAAXyqdviHgAf9t6CmJUhMN3m7zxkq7qM5i&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
19
-------------------------------------------------
2020-08-05 11:43:34
Got the results of the query
{'bib': {'abstract': 'In this paper, we conduct an intriguing experimental '
                     'study about the physical adversarial attack on object '
                     'detectors in the wild. In particular, we learn a '
                     'camouflage pattern to hide vehicles from being detected '
                     'by state-of-the-art convolutional neural network based '
                     'detectors. Our approach alternates between two threads. '
                     'In the first, we train a neural approximation function '
                     'to imitate how a simulator applies a camouflage to '
                     'vehicles and how a vehicle detector performs given '
                     'images of the camouflaged vehicles. In the second, we',
         'author': ['Y Zhang', 'H Foroosh', 'P David', 'B Gong'],
         'cites': '9',
         'eprint': 'https://openreview.net/pdf?id=SJgEl3A5tm',
         'gsrank': '1',
         'title': 'CAMOU: Learning physical vehicle camouflages to '
                  'adversarially attack detectors in the wild',
         'url': 'https://openreview.net/forum?id=SJgEl3A5tm',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=497992744230955037&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCAMOU:%2BLearning%2BPhysical%2BVehicle%2BCamouflages%2Bto%2BAdversarially%2BAttack%2BDetectors%2Bin%2Bthe%2BWild%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=HaibLMM56QYJ&ei=bpsqX563MKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:HaibLMM56QYJ:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqd0uSMkTN7LG6xHNw4ozfnsWT3_lNa&scisig=AAGBfm0AAAAAXyqd0sWjA4ynrYqba9pPo2eU586BQBFF&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 11:43:54
Got the results of the query
{'bib': {'abstract': 'We investigate a variant of variational autoencoders '
                     'where there is a superstructure of discrete latent '
                     'variables on top of the latent features. In general, our '
                     'superstructure is a tree structure of multiple super '
                     'latent variables and it is automatically learned from '
                     'data. When there is only one latent variable in the '
                     'superstructure, our model reduces to one that assumes '
                     'the latent features to be generated from a Gaussian '
                     'mixture model. We call our model the latent tree '
                     'variational autoencoder (LTVAE). Whereas previous deep '
                     'learning',
         'author': ['X Li', 'Z Chen', 'LKM Poon', 'NL Zhang'],
         'cites': '8',
         'eprint': 'https://arxiv.org/pdf/1803.05206',
         'gsrank': '1',
         'title': 'Learning latent superstructures in variational autoencoders '
                  'for deep multidimensional clustering',
         'url': 'https://arxiv.org/abs/1803.05206',
         'venue': 'arXiv preprint arXiv:1803.05206',
         'year': '2018'},
 'citations_link': '/scholar?cites=5927867859070170464&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BLatent%2BSuperstructures%2Bin%2BVariational%2BAutoencoders%2Bfor%2BDeep%2BMultidimensional%2BClustering%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=YDVVH2kERFIJ&ei=g5sqX7zkJoyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:YDVVH2kERFIJ:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqd5xQwCbrgOSL5TEp9Lu5QbuNpgMWL&scisig=AAGBfm0AAAAAXyqd54gDrdvfB49toQVdl98psx7Nbk38&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
8
-------------------------------------------------
2020-08-05 11:44:16
Got the results of the query
{'bib': {'abstract': 'We present the perceptor gradients algorithm--a novel '
                     'approach to learning symbolic representations based on '
                     "the idea of decomposing an agent's policy into i) a "
                     'perceptor network extracting symbols from raw '
                     'observation data and ii) a task encoding program which '
                     'maps the input symbols to output actions. We show that '
                     'the proposed algorithm is able to learn representations '
                     'that can be directly fed into a Linear-Quadratic '
                     'Regulator (LQR) or a general purpose A* planner. Our '
                     'experimental results confirm that the perceptor',
         'author': ['S Penkov', 'S Ramamoorthy'],
         'cites': '6',
         'eprint': 'https://arxiv.org/pdf/1905.00956',
         'gsrank': '1',
         'title': 'Learning programmatically structured representations with '
                  'perceptor gradients',
         'url': 'https://arxiv.org/abs/1905.00956',
         'venue': 'arXiv preprint arXiv:1905.00956',
         'year': '2019'},
 'citations_link': '/scholar?cites=10506348952741337181&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BProgrammatically%2BStructured%2BRepresentations%2Bwith%2BPerceptor%2BGradients%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=XdhxHrUMzpEJ&ei=l5sqX4CKLZqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:XdhxHrUMzpEJ:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqd-NY5hBg9kG6Dp-7LpNLJ6R7wYLie&scisig=AAGBfm0AAAAAXyqd-GQe1KYINWibLi6uWB1p8BdqRgXz&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 11:44:33
Got the results of the query
{'bib': {'abstract': 'We propose a method for learning the dependency '
                     'structure between latent variables in deep latent '
                     'variable models. Our general modeling and inference '
                     'framework combines the complementary strengths of deep '
                     'generative models and probabilistic graphical models. In '
                     'particular, we express the latent variable space of a '
                     'variational autoencoder (VAE) in terms of a Bayesian '
                     'network with a learned, flexible dependency structure. '
                     'The network parameters, variational parameters as well '
                     'as the latent topology are optimized',
         'author': ['J He', 'Y Gong', 'J Marino', 'G Mori'],
         'cites': '5',
         'eprint': 'https://openreview.net/pdf?id=SJgsCjCqt7',
         'gsrank': '1',
         'title': 'Variational autoencoders with jointly optimized latent '
                  'dependency structure',
         'url': 'https://openreview.net/forum?id=SJgsCjCqt7',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=16010026349597999737&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BAutoencoders%2Bwith%2BJointly%2BOptimized%2BLatent%2BDependency%2BStructure%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ecJu5CMKL94J&ei=rJsqX9K5CYjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ecJu5CMKL94J:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqeCkOIPvwr_UIde3yKQ3WE8HvuTEWf&scisig=AAGBfm0AAAAAXyqeCuDE7JWkox8fGtBO_9wQFOujCCGU&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 11:44:51
Got the results of the query
{'bib': {'abstract': 'We examine two different techniques for parameter '
                     'averaging in GAN training. Moving Average (MA) computes '
                     'the time-average of parameters, whereas Exponential '
                     'Moving Average (EMA) computes an exponentially '
                     'discounted sum. Whilst MA is known to lead to',
         'author': ['Y Yazici', 'CS Foo', 'S Winkler', 'KH Yap', 'G Piliouras'],
         'cites': '9',
         'gsrank': '1',
         'title': 'The unusual effectiveness of averaging in GAN training',
         'url': 'https://oar.a-star.edu.sg/jspui/handle/123456789/3059',
         'venue': '',
         'year': '2019'},
 'citations_link': '/scholar?cites=2700944634230259942&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BUnusual%2BEffectiveness%2Bof%2BAveraging%2Bin%2BGAN%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=5sBKqdWueyUJ&ei=uZsqX6jUIYvrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:5sBKqdWueyUJ:scholar.google.com/&output=citation&scisdr=CgVVR3uyGAA:AAGBfm0AAAAAXyqeI_QCUgx_5fX69OQGfZz_MrYLuIMW&scisig=AAGBfm0AAAAAXyqeI6JgbcuiQzbcQ1Ek2SLfgmvXGvtN&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 11:45:16
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://51.38.134.168:3128
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://34.91.135.38:80
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://51.38.134.168:3128
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://206.198.131.142:80
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://51.38.134.168:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://18.221.58.204:3838
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://51.38.134.168:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://51.38.134.168:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://206.198.131.142:80
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://117.20.26.21:8080
Got the results of the query
{'bib': {'abstract': 'Many machine learning image classifiers are vulnerable '
                     'to adversarial attacks, inputs with perturbations '
                     'designed to intentionally trigger misclassification. '
                     'Current adversarial methods directly alter pixel colors '
                     'and evaluate against pixel norm-balls: pixel '
                     'perturbations smaller than a specified magnitude, '
                     'according to a measurement norm. This evaluation, '
                     'however, has limited practical utility since '
                     'perturbations in the pixel space do not correspond to '
                     'underlying real-world phenomena of image formation that '
                     'lead to them and has no security',
         'author': ['HTD Liu', 'M Tao', 'CL Li', 'D Nowrouzezahrai'],
         'cites': '26',
         'eprint': 'https://arxiv.org/pdf/1808.02651',
         'gsrank': '1',
         'title': 'Beyond pixel norm-balls: Parametric adversaries using an '
                  'analytically differentiable renderer',
         'url': 'https://arxiv.org/abs/1808.02651',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=12447741163485778888&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBeyond%2BPixel%2BNorm-Balls:%2BParametric%2BAdversaries%2Busing%2Ban%2BAnalytically%2BDifferentiable%2BRenderer%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=yNOTIaNCv6wJ&ei=w6MqX_uVG4-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:yNOTIaNCv6wJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqmHjLKYxMbeHgS_PGTbdmOjDPNYAs4&scisig=AAGBfm0AAAAAXyqmHjUK-5iMEkBb47kh0MlIEuTrd_Y_&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
26
-------------------------------------------------
2020-08-05 12:19:18
Got the results of the query
{'bib': {'abstract': 'Intelligent creatures can explore their environments and '
                     'learn useful skills without supervision. In this paper, '
                     "we propose DIAYN ('Diversity is All You Need'), a method "
                     'for learning useful skills without a reward function. '
                     'Our proposed method learns skills by maximizing an '
                     'information theoretic objective using a maximum entropy '
                     'policy. On a variety of simulated robotic tasks, we show '
                     'that this simple objective results in the unsupervised '
                     'emergence of diverse skills, such as walking and '
                     'jumping. In a number of reinforcement',
         'author': ['B Eysenbach', 'A Gupta', 'J Ibarz', 'S Levine'],
         'cites': '182',
         'eprint': 'https://arxiv.org/pdf/1802.06070',
         'gsrank': '1',
         'title': 'Diversity is all you need: Learning skills without a reward '
                  'function',
         'url': 'https://arxiv.org/abs/1802.06070',
         'venue': 'arXiv preprint arXiv:1802.06070',
         'year': '2018'},
 'citations_link': '/scholar?cites=12324439663284457782&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiversity%2Bis%2BAll%2BYou%2BNeed:%2BLearning%2BSkills%2Bwithout%2Ba%2BReward%2BFunction%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=NhHOeZM0CasJ&ei=zaMqX5LkG6OGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:NhHOeZM0CasJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqmOkY-oVEMwO9uMvvXtKNLrW6PVeAH&scisig=AAGBfm0AAAAAXyqmOu2pfY4QFNCCoUzk7z5ymWSlf4gm&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
182
-------------------------------------------------
2020-08-05 12:19:46
Got the results of the query
{'bib': {'abstract': 'We propose a new sample-efficient methodology, called '
                     'Supervised Policy Update (SPU), for deep reinforcement '
                     'learning. Starting with data generated by the current '
                     'policy, SPU formulates and solves a constrained '
                     'optimization problem in the non-parameterized proximal '
                     'policy space. Using supervised regression, it then '
                     'converts the optimal non-parameterized policy to a '
                     'parameterized policy, from which it draws new samples. '
                     'The methodology is general in that it applies to both '
                     'discrete and continuous action spaces, and',
         'author': ['Q Vuong', 'Y Zhang', 'KW Ross'],
         'cites': '9',
         'eprint': 'https://arxiv.org/pdf/1805.11706',
         'gsrank': '1',
         'title': 'Supervised policy update for deep reinforcement learning',
         'url': 'https://arxiv.org/abs/1805.11706',
         'venue': 'arXiv preprint arXiv:1805.11706',
         'year': '2018'},
 'citations_link': '/scholar?cites=9669638111330201224&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSupervised%2BPolicy%2BUpdate%2Bfor%2BDeep%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=iG5qXKF0MYYJ&ei=56MqX5vMJ6OGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:iG5qXKF0MYYJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqmSvJgKq0ZfWFu16rYZNlxxPS5TuzH&scisig=AAGBfm0AAAAAXyqmSpseo2r-gfJx_0SRTRRDYKElaw0H&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 12:20:02
Got the results of the query
{'bib': {'abstract': 'We present a representation for describing transition '
                     'models in complex uncertain domains using relational '
                     'rules. For any action, a rule selects a set of relevant '
                     'objects and computes a distribution over properties of '
                     'just those objects in the resulting state given their '
                     'properties in the previous state. An iterative greedy '
                     'algorithm is used to construct a set of deictic '
                     'references that determine which objects are relevant in '
                     'any given state. Feed-forward neural networks are used '
                     'to learn the transition distribution on the relevant '
                     "objects' properties. This",
         'author': ['V Xia', 'Z Wang', 'LP Kaelbling'],
         'cites': '7',
         'eprint': 'https://arxiv.org/pdf/1810.11177',
         'gsrank': '1',
         'title': 'Learning sparse relational transition models',
         'url': 'https://arxiv.org/abs/1810.11177',
         'venue': 'arXiv preprint arXiv:1810.11177',
         'year': '2018'},
 'citations_link': '/scholar?cites=7997110422667369125&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bsparse%2Brelational%2Btransition%2Bmodels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=pZqpFI9x-24J&ei=CKQqX5WgAY-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:pZqpFI9x-24J:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqma8xuP6gmhu-ou8zBpWabNYCVSDg0&scisig=AAGBfm0AAAAAXyqma-CmsXBrSbQfGkegVORIiYWD9REm&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
7
-------------------------------------------------
2020-08-05 12:20:39
Got the results of the query
{'bib': {'abstract': 'Many real-world reinforcement learning tasks require '
                     'multiple agents to make sequential decisions under the '
                     "agents' interaction, where well-coordinated actions "
                     'among the agents are crucial to achieve the target goal '
                     'better at these tasks. One way to accelerate the '
                     'coordination effect is to enable multiple agents to '
                     'communicate with each other in a distributed manner and '
                     'behave as a group. In this paper, we study a practical '
                     'scenario when (i) the communication bandwidth is limited '
                     'and (ii) the agents share the communication',
         'author': ['D Kim', 'S Moon', 'D Hostallero', 'WJ Kang', 'T Lee'],
         'cites': '22',
         'eprint': 'https://arxiv.org/pdf/1902.01554',
         'gsrank': '1',
         'title': 'Learning to schedule communication in multi-agent '
                  'reinforcement learning',
         'url': 'https://arxiv.org/abs/1902.01554',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=2430706253185717368&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BSchedule%2BCommunication%2Bin%2BMulti-agent%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=eJR0dnWauyEJ&ei=I6QqX770Ao-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:eJR0dnWauyEJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqmjn8ONaJEGWn7HYSDoqaNB7ucxSVh&scisig=AAGBfm0AAAAAXyqmjqU702MUMvKg5JTNMSt0-hmrUfb5&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
22
-------------------------------------------------
2020-08-05 12:21:10
Got the results of the query
{'bib': {'abstract': 'In this paper we introduce a simple, robust approach to '
                     'hierarchically training an agent in the setting of '
                     'sparse reward tasks. The agent is split into a low-level '
                     'and a high-level policy. The low-level policy only '
                     'accesses internal, proprioceptive dimensions of the '
                     'state observation. The low-level policies are trained '
                     'with a simple reward that encourages changing the values '
                     'of the non-proprioceptive dimensions. Furthermore, it is '
                     'induced to be periodic with the use a``phase '
                     "function.''The high-level policy is trained using a "
                     'sparse, task',
         'author': ['K Marino', 'A Gupta', 'R Fergus', 'A Szlam'],
         'cites': '5',
         'eprint': 'https://openreview.net/pdf?id=SJz1x20cFQ',
         'gsrank': '1',
         'title': 'Hierarchical RL using an ensemble of proprioceptive '
                  'periodic policies',
         'url': 'https://openreview.net/forum?id=SJz1x20cFQ',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=7300179530988775512&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BRL%2BUsing%2Ban%2BEnsemble%2Bof%2BProprioceptive%2BPeriodic%2BPolicies%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=WOgp_ZpyT2UJ&ei=R6QqX-2qJbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:WOgp_ZpyT2UJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqmpmd4KIe3d2eQX6ePRrTxbD3EqddW&scisig=AAGBfm0AAAAAXyqmpnLOqIB65_Nru2M_UqpOoGYNiTkN&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 12:21:34
Got the results of the query
{'bib': {'abstract': 'This work presents a new strategy for multi-class '
                     'classification that requires no class-specific labels, '
                     'but instead leverages pairwise similarity between '
                     'examples, which is a weaker form of annotation. The '
                     'proposed method, meta classification learning, optimizes '
                     'a binary classifier for pairwise similarity prediction '
                     'and through this process learns a multi-class classifier '
                     'as a submodule. We formulate this approach, present a '
                     'probabilistic graphical model for it, and derive a '
                     'surprisingly simple loss function that can be used to '
                     'learn neural',
         'author': ['YC Hsu', 'Z Lv', 'J Schlosser', 'P Odom', 'Z Kira'],
         'cites': '16',
         'eprint': 'https://arxiv.org/pdf/1901.00544',
         'gsrank': '1',
         'title': 'Multi-class classification without multi-class labels',
         'url': 'https://arxiv.org/abs/1901.00544',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=15660059153270341215&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-class%2Bclassification%2Bwithout%2Bmulti-class%2Blabels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=X35IUNS0U9kJ&ei=U6QqX6KvM5qGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:X35IUNS0U9kJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqmsXAGXY-P1cg9QRRuSb_HL576IPrM&scisig=AAGBfm0AAAAAXyqmsdDhG_FcWPOK7jsmvFKI-06QQ4Jp&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
16
-------------------------------------------------
2020-08-05 12:21:45
Got the results of the query
{'bib': {'abstract': 'Contextualized representation models such as ELMo '
                     '(Peters et al., 2018a) and BERT (Devlin et al., 2018) '
                     'have recently achieved state-of-the-art results on a '
                     'diverse array of downstream NLP tasks. Building on '
                     'recent token-level probing work, we introduce a novel '
                     'edge probing task design and construct a broad suite of '
                     'sub-sentence tasks derived from the traditional '
                     'structured NLP pipeline. We probe word-level contextual '
                     'representations from four recent models and investigate '
                     'how they encode sentence structure across a range of',
         'author': ['I Tenney', 'P Xia', 'B Chen', 'A Wang', 'A Poliak'],
         'cites': '138',
         'eprint': 'https://arxiv.org/pdf/1905.06316',
         'gsrank': '1',
         'title': 'What do you learn from context? probing for sentence '
                  'structure in contextualized word representations',
         'url': 'https://arxiv.org/abs/1905.06316',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=446886033048011777&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWhat%2Bdo%2Byou%2Blearn%2Bfrom%2Bcontext%253F%2BProbing%2Bfor%2Bsentence%2Bstructure%2Bin%2Bcontextualized%2Bword%2Brepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=AQTptnqoMwYJ&ei=X6QqX6WhMIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:AQTptnqoMwYJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqmw5hNVSjXNiGGCnjus6dzYuMAvZZG&scisig=AAGBfm0AAAAAXyqmww3kfEj-EVV_P8ydm7FwOcEGTWgV&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
138
-------------------------------------------------
2020-08-05 12:22:03
Got the results of the query
{'bib': {'abstract': 'We present Spectral Inference Networks, a framework for '
                     'learning eigenfunctions of linear operators by '
                     'stochastic optimization. Spectral Inference Networks '
                     'generalize Slow Feature Analysis to generic symmetric '
                     'operators, and are closely related to Variational Monte '
                     'Carlo methods from computational physics. As such, they '
                     'can be a powerful tool for unsupervised representation '
                     'learning from video or graph-structured data. We cast '
                     'training Spectral Inference Networks as a bilevel '
                     'optimization problem, which allows for online learning '
                     'of',
         'author': ['D Pfau', 'S Petersen', 'A Agarwal', 'DGT Barrett'],
         'cites': '8',
         'eprint': 'https://arxiv.org/pdf/1806.02215',
         'gsrank': '1',
         'title': 'Spectral inference networks: Unifying deep and spectral '
                  'learning',
         'url': 'https://arxiv.org/abs/1806.02215',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=16660579419089969631&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpectral%2BInference%2BNetworks:%2BUnifying%2BDeep%2Band%2BSpectral%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=36EKy7VENucJ&ei=c6QqX4qxMcKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:36EKy7VENucJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqm1RMNdhbnL4BDQ0RGfrfNHrEVflaR&scisig=AAGBfm0AAAAAXyqm1eEOOVrh-JkJaK8GudsJfB5SQxot&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
8
-------------------------------------------------
2020-08-05 12:22:22
Got the results of the query
{'bib': {'abstract': 'Deep learning systems have become ubiquitous in many '
                     'aspects of our lives. Unfortunately, it has been shown '
                     'that such systems are vulnerable to adversarial attacks, '
                     'making them prone to potential unlawful uses. Designing '
                     'deep neural networks that are robust to adversarial '
                     'attacks is a fundamental step in making such systems '
                     'safer and deployable in a broader variety of '
                     'applications (eg autonomous driving), but more '
                     'importantly is a necessary step to design novel and more '
                     'advanced architectures built on new computational',
         'author': ['J Svoboda', 'J Masci', 'F Monti', 'MM Bronstein'],
         'cites': '20',
         'eprint': 'https://arxiv.org/pdf/1806.00088',
         'gsrank': '1',
         'title': 'Peernets: Exploiting peer wisdom against adversarial '
                  'attacks',
         'url': 'https://arxiv.org/abs/1806.00088',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=4885092075703365076&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPeerNets:%2BExploiting%2BPeer%2BWisdom%2BAgainst%2BAdversarial%2BAttacks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=1O1fS1xVy0MJ&ei=hqQqX8DaL5qGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:1O1fS1xVy0MJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqm5sPml7LOd6BKx4re2m4yDDPhMTlz&scisig=AAGBfm0AAAAAXyqm5iyo1hpQByrU1SA012qzcUCg-tfw&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
20
-------------------------------------------------
2020-08-05 12:22:39
Got the results of the query
{'bib': {'abstract': 'Neural Processes (NPs)(Garnelo et al 2018a; b) approach '
                     'regression by learning to map a context set of observed '
                     'input-output pairs to a distribution over regression '
                     'functions. Each function models the distribution of the '
                     'output given an input, conditioned on the context. NPs',
         'author': ['H Kim', 'A Mnih', 'J Schwarz', 'M Garnelo', 'A Eslami'],
         'cites': '60',
         'eprint': 'https://arxiv.org/pdf/1901.05761',
         'gsrank': '1',
         'title': 'Attentive neural processes',
         'url': 'https://arxiv.org/abs/1901.05761',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=6519833436864425356&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAttentive%2BNeural%2BProcesses%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=jK0En_cZe1oJ&ei=lKQqX-LNH5qGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:jK0En_cZe1oJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqm8m8DP-IYwwt42VhLvcluQLaH9DdZ&scisig=AAGBfm0AAAAAXyqm8vPgawguIzIDm6ATLDIwKmUP4Jbs&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
60
-------------------------------------------------
2020-08-05 12:22:50
Got the results of the query
{'bib': {'abstract': 'We study an interesting problem in training neural '
                     'network-based models for natural language generation '
                     'tasks, which we call the\\emph {representation '
                     'degeneration problem}. We observe that when training a '
                     'model for natural language generation tasks through '
                     'likelihood maximization with the weight tying trick, '
                     'especially with big training datasets, most of the '
                     'learnt word embeddings tend to degenerate and be '
                     'distributed into a narrow cone, which largely limits the '
                     'representation power of word embeddings. We analyze the',
         'author': ['J Gao', 'D He', 'X Tan', 'T Qin', 'L Wang', 'TY Liu'],
         'cites': '9',
         'eprint': 'https://arxiv.org/pdf/1907.12009',
         'gsrank': '1',
         'title': 'Representation degeneration problem in training natural '
                  'language generation models',
         'url': 'https://arxiv.org/abs/1907.12009',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=3878205322217052013&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRepresentation%2BDegeneration%2BProblem%2Bin%2BTraining%2BNatural%2BLanguage%2BGeneration%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=bdfGZDEn0jUJ&ei=oqQqX-XZCqiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:bdfGZDEn0jUJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqnAqqRurrv60AKkDgCuxbg5Pqxscxf&scisig=AAGBfm0AAAAAXyqnAsPG4tKbGL49pD2JzuNQGPMVx-Rm&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 12:23:07
Got the results of the query
{'bib': {'abstract': 'Deep neural networks (DNNs) have achieved impressive '
                     'predictive performance due to their ability to learn '
                     'complex, non-linear relationships between variables. '
                     'However, the inability to effectively visualize these '
                     'relationships has led to DNNs being characterized as '
                     'black boxes and consequently limited their applications. '
                     'To ameliorate this problem, we introduce the use of '
                     'hierarchical interpretations to explain DNN predictions '
                     'through our proposed method, agglomerative contextual '
                     'decomposition (ACD). Given a prediction from',
         'author': ['C Singh', 'WJ Murdoch', 'B Yu'],
         'cites': '31',
         'eprint': 'https://arxiv.org/pdf/1806.05337',
         'gsrank': '1',
         'title': 'Hierarchical interpretations for neural network predictions',
         'url': 'https://arxiv.org/abs/1806.05337',
         'venue': 'arXiv preprint arXiv:1806.05337',
         'year': '2018'},
 'citations_link': '/scholar?cites=14523630218994203463&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2Binterpretations%2Bfor%2Bneural%2Bnetwork%2Bpredictions%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=RztHFLxMjskJ&ei=tqQqX6-fK4yimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:RztHFLxMjskJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqnHP7iNu7dcSeOVymT6KR_MbAxoWZl&scisig=AAGBfm0AAAAAXyqnHFdMkUK8pRi-X8DXzJvEYnN5P31P&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
31
-------------------------------------------------
2020-08-05 12:23:33
Got the results of the query
{'bib': {'abstract': 'Discretizing multi-dimensional data distributions is a '
                     'fundamental step of modern indexing methods. '
                     'State-of-the-art techniques learn parameters of '
                     'quantizers on training data for optimal performance, '
                     'thus adapting quantizers to the data. In this work, we '
                     'propose to reverse this paradigm and adapt the data to '
                     'the quantizer: we train a neural net which last layer '
                     'forms a fixed parameter-free quantizer, such as '
                     'pre-defined points of a hyper-sphere. As a proxy '
                     'objective, we design and train a neural network that '
                     'favors uniformity in the',
         'author': ['A Sablayrolles', 'M Douze', 'C Schmid'],
         'cites': '12',
         'eprint': 'https://arxiv.org/pdf/1806.03198',
         'gsrank': '1',
         'title': 'Spreading vectors for similarity search',
         'url': 'https://arxiv.org/abs/1806.03198',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=7912762574684423820&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpreading%2Bvectors%2Bfor%2Bsimilarity%2Bsearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=jAYSWKPHz20J&ei=0KQqX_u1D5qGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:jAYSWKPHz20J:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqnOXJoRVv49aRGIerlAcAm5hz07oN5&scisig=AAGBfm0AAAAAXyqnOVwOJQCHuel5jCCfUQxQ-5u1uIHV&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 12:24:02
Got the results of the query
{'bib': {'abstract': 'We analyze speed of convergence to global optimum for '
                     'gradient descent training a deep linear neural network '
                     '(parameterized as $ x\\mapsto W_N W_ {N-1}\\cdots W_1 x '
                     '$) by minimizing the $\\ell_2 $ loss over whitened data. '
                     'Convergence at a linear rate is guaranteed when the '
                     'following hold:(i) dimensions of hidden layers are at '
                     'least the minimum of the input and output '
                     'dimensions;(ii) weight matrices at initialization are '
                     'approximately balanced; and (iii) the initial loss is '
                     'smaller than the loss of any rank-deficient solution. '
                     'The assumptions on',
         'author': ['S Arora', 'N Cohen', 'N Golowich', 'W Hu'],
         'cites': '70',
         'eprint': 'https://arxiv.org/pdf/1810.02281',
         'gsrank': '1',
         'title': 'A convergence analysis of gradient descent for deep linear '
                  'neural networks',
         'url': 'https://arxiv.org/abs/1810.02281',
         'venue': 'arXiv preprint arXiv:1810.02281',
         'year': '2018'},
 'citations_link': '/scholar?cites=9015925541758289839&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BConvergence%2BAnalysis%2Bof%2BGradient%2BDescent%2Bfor%2BDeep%2BLinear%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=r8ffBYMAH30J&ei=5qQqX_aeIovrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:r8ffBYMAH30J:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqnVIXP-osx9Co0U9edsS7d_I04yIKL&scisig=AAGBfm0AAAAAXyqnVLUDsmdqQhyvf1IMEzNus_Cx5QoE&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
70
-------------------------------------------------
2020-08-05 12:24:29
Got the results of the query
{'bib': {'abstract': 'Probabilistic Neural Networks deal with various sources '
                     'of stochasticity: input noise, dropout, stochastic '
                     'neurons, parameter uncertainties modeled as random '
                     'variables, etc. In this paper we revisit a feed-forward '
                     'propagation approach that allows one to estimate for '
                     'each neuron its mean and variance wrt all mentioned '
                     'sources of stochasticity. In contrast, standard NNs '
                     'propagate only point estimates, discarding the '
                     'uncertainty. Methods propagating also the variance have '
                     'been proposed by several authors in different context',
         'author': ['A Shekhovtsov', 'B Flach'],
         'cites': '3',
         'eprint': 'https://openreview.net/pdf?id=SkMuPjRcKQ',
         'gsrank': '1',
         'title': 'Feed-forward propagation in probabilistic neural networks '
                  'with categorical and max layers',
         'url': 'https://openreview.net/forum?id=SkMuPjRcKQ',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1018527712043791327&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFeed-forward%2BPropagation%2Bin%2BProbabilistic%2BNeural%2BNetworks%2Bwith%2BCategorical%2Band%2BMax%2BLayers%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=3_NCxo6JIg4J&ei=AqUqX63mOLGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:3_NCxo6JIg4J:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqnZ1H-wr8vhoO5Ky642XpHJn_dUkc9&scisig=AAGBfm0AAAAAXyqnZ5o0792UFUMS9i6w-PPf1Js7S1uK&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
3
-------------------------------------------------
2020-08-05 12:24:47
Got the results of the query
{'bib': {'abstract': 'To optimize a neural network one often thinks of '
                     'optimizing its parameters, but it is ultimately a matter '
                     'of optimizing the function that maps inputs to outputs. '
                     'Since a change in the parameters might serve as a poor '
                     'proxy for the change in the function, it is of some '
                     'concern that primacy is given to parameters but that the '
                     'correspondence has not been tested. Here, we show that '
                     'it is simple and computationally feasible to calculate '
                     'distances between functions in a $ L^ 2$ Hilbert space. '
                     'We examine how typical networks behave in this space',
         'author': ['AS Benjamin', 'D Rolnick', 'K Kording'],
         'cites': '9',
         'eprint': 'https://arxiv.org/pdf/1805.08289',
         'gsrank': '1',
         'title': 'Measuring and regularizing networks in function space',
         'url': 'https://arxiv.org/abs/1805.08289',
         'venue': 'arXiv preprint arXiv:1805.08289',
         'year': '2018'},
 'citations_link': '/scholar?cites=15363433308760579134&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeasuring%2Band%2Bregularizing%2Bnetworks%2Bin%2Bfunction%2Bspace%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=PhwUAzLhNdUJ&ei=FKUqX_K5GsKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:PhwUAzLhNdUJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqncl22PBO8mu8qaY69AyOlPJ4fVziR&scisig=AAGBfm0AAAAAXyqncnuuiqaJwBT7rsVBPaQs1sgdw7HA&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 12:24:58
Got the results of the query
{'bib': {'abstract': 'The notion of the stationary equilibrium ensemble has '
                     'played a central role in statistical mechanics. In '
                     'machine learning as well, training serves as generalized '
                     'equilibration that drives the probability distribution '
                     'of model parameters toward stationarity. Here, we derive '
                     'stationary fluctuation-dissipation relations that link '
                     'measurable quantities and hyperparameters in the '
                     'stochastic gradient descent algorithm. These relations '
                     'hold exactly for any stationary state and can in '
                     'particular be used to adaptively set training schedule. '
                     'We',
         'author': ['S Yaida'],
         'cites': '20',
         'eprint': 'https://arxiv.org/pdf/1810.00004',
         'gsrank': '1',
         'title': 'Fluctuation-dissipation relations for stochastic gradient '
                  'descent',
         'url': 'https://arxiv.org/abs/1810.00004',
         'venue': 'arXiv preprint arXiv:1810.00004',
         'year': '2018'},
 'citations_link': '/scholar?cites=6542696282294112644&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFluctuation-dissipation%2Brelations%2Bfor%2Bstochastic%2Bgradient%2Bdescent%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=hNFoo5pTzFoJ&ei=KqUqX5-hN4jHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:hNFoo5pTzFoJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqnjSRKt5gzjLBVcCHyI52FGwGDNsXn&scisig=AAGBfm0AAAAAXyqnjSiB5FKgsGST9mVFIc_NwYtADDO5&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
20
-------------------------------------------------
2020-08-05 12:25:26
Got the results of the query
{'bib': {'abstract': 'Words are not created equal. In fact, they form an '
                     'aristocratic graph with a latent hierarchical structure '
                     'that the next generation of unsupervised learned word '
                     'embeddings should reveal. In this paper, justified by '
                     'the notion of delta-hyperbolicity or tree-likeliness of '
                     'a space, we propose to embed words in a Cartesian '
                     'product of hyperbolic spaces which we theoretically '
                     'connect to the Gaussian word embeddings and their Fisher '
                     'geometry. This connection allows us to introduce a novel '
                     'principled hypernymy score for word embeddings. Moreover',
         'author': ['A Tifrea', 'G Bécigneul', 'OE Ganea'],
         'cites': '50',
         'eprint': 'https://arxiv.org/pdf/1810.06546',
         'gsrank': '1',
         'title': "Poincar\\'e GloVe: Hyperbolic Word Embeddings",
         'url': 'https://arxiv.org/abs/1810.06546',
         'venue': 'arXiv preprint arXiv:1810.06546',
         'year': '2018'},
 'citations_link': '/scholar?cites=4245081116962532671&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPoincare%2BGlove:%2BHyperbolic%2BWord%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Py1F08iO6ToJ&ei=QaUqX635MYjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Py1F08iO6ToJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqnpEMOYyipxxWqjXX5_nwOgad25V4h&scisig=AAGBfm0AAAAAXyqnpBGXgPdjhoe67-6lSM5WGgwCX0C2&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
50
-------------------------------------------------
2020-08-05 12:25:49
Got the results of the query
{'bib': {'abstract': "Rewards are sparse in the real world and most of today's "
                     'reinforcement learning algorithms struggle with such '
                     'sparsity. One solution to this problem is to allow the '
                     'agent to create rewards for itself-thus making rewards '
                     'dense and more suitable for learning. In particular',
         'author': ['N Savinov', 'A Raichuk', 'R Marinier', 'D Vincent'],
         'cites': '56',
         'eprint': 'https://arxiv.org/pdf/1810.02274',
         'gsrank': '1',
         'title': 'Episodic curiosity through reachability',
         'url': 'https://arxiv.org/abs/1810.02274',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=3202653392377789217&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEpisodic%2BCuriosity%2Bthrough%2BReachability%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Ibtgv0occiwJ&ei=V6UqX56wFIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Ibtgv0occiwJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqnyA2K3gE8UUUkuQdXY4Jak8fA-5Wd&scisig=AAGBfm0AAAAAXyqnyKIn-jUEzvfsqqXm2YRmdl9xMYnS&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
56
-------------------------------------------------
2020-08-05 12:26:24
Got the results of the query
{'bib': {'abstract': 'Most deep learning-based models for speech enhancement '
                     'have mainly focused on estimating the magnitude of '
                     'spectrogram while reusing the phase from noisy speech '
                     'for reconstruction. This is due to the difficulty of '
                     'estimating the phase of clean speech. To improve speech '
                     'enhancement performance, we tackle the phase estimation '
                     'problem in three ways. First, we propose Deep Complex '
                     'U-Net, an advanced U-Net structured model incorporating '
                     'well-defined complex-valued building blocks to deal with '
                     'complex-valued',
         'author': ['HS Choi', 'JH Kim', 'J Huh', 'A Kim', 'JW Ha'],
         'cites': '22',
         'eprint': 'https://openreview.net/pdf?id=SkeRTsAcYm',
         'gsrank': '1',
         'title': 'Phase-aware speech enhancement with deep complex u-net',
         'url': 'https://openreview.net/forum?id=SkeRTsAcYm',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11525798829336957800&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPhase-Aware%2BSpeech%2BEnhancement%2Bwith%2BDeep%2BComplex%2BU-Net%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=aBM-D_jc858J&ei=e6UqX8LEGqOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:aBM-D_jc858J:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqn3re8tDJ9gW8--LDYeYZ3i8yBYknj&scisig=AAGBfm0AAAAAXyqn3p3ZiophRO7SclRrPtpxiW6klsPX&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
22
-------------------------------------------------
2020-08-05 12:26:50
Got the results of the query
{'bib': {'abstract': 'We propose Generative Predecessor Models for Imitation '
                     'Learning (GPRIL), a novel imitation learning algorithm '
                     'that matches the state-action distribution to the '
                     'distribution observed in expert demonstrations, using '
                     'generative models to reason probabilistically about '
                     'alternative histories of demonstrated states. We show '
                     'that this approach allows an agent to learn robust '
                     'policies using only a small number of expert '
                     'demonstrations and self-supervised interactions with the '
                     'environment. We derive this approach from first '
                     'principles',
         'author': ['Y Schroecker', 'M Vecerik', 'J Scholz'],
         'cites': '12',
         'eprint': 'https://arxiv.org/pdf/1904.01139',
         'gsrank': '1',
         'title': 'Generative predecessor models for sample-efficient '
                  'imitation learning',
         'url': 'https://arxiv.org/abs/1904.01139',
         'venue': 'arXiv preprint arXiv:1904.01139',
         'year': '2019'},
 'citations_link': '/scholar?cites=8821752117050196964&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2Bpredecessor%2Bmodels%2Bfor%2Bsample-efficient%2Bimitation%2Blearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=5MNTkc8obXoJ&ei=kKUqX6CHBY-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:5MNTkc8obXoJ:scholar.google.com/&output=citation&scisdr=CgX8rw8GGAA:AAGBfm0AAAAAXyqn9xbgAzOnchPl3vUYLbHjyDARYuSE&scisig=AAGBfm0AAAAAXyqn937SkZVztTgiJSmcuWEJ6ol3cTqV&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 12:27:11
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://81.201.60.130:80
Got the results of the query
{'bib': {'abstract': 'To improve how neural networks function it is crucial to '
                     'understand their learning process. The information '
                     'bottleneck theory of deep learning proposes that neural '
                     'networks achieve good generalization by compressing '
                     'their representations to disregard information that is '
                     'not relevant to the task. However, empirical evidence '
                     'for this theory is conflicting, as compression was only '
                     'observed when networks used saturating activation '
                     'functions. In contrast, networks with non-saturating '
                     'activation functions achieved comparable levels of',
         'author': ['I Chelombiev', 'C Houghton', "C O'Donnell"],
         'cites': '11',
         'eprint': 'https://arxiv.org/pdf/1902.09037',
         'gsrank': '1',
         'title': 'Adaptive estimators show information compression in deep '
                  'neural networks',
         'url': 'https://arxiv.org/abs/1902.09037',
         'venue': 'arXiv preprint arXiv:1902.09037',
         'year': '2019'},
 'citations_link': '/scholar?cites=2654288184895561029&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdaptive%2BEstimators%2BShow%2BInformation%2BCompression%2Bin%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=RV03iArt1SQJ&ei=8aUqX4CoK7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:RV03iArt1SQJ:scholar.google.com/&output=citation&scisdr=CgUX_2UGGAA:AAGBfm0AAAAAXyqoTrdykGqmnXq2exRL6n5mmNlN9mjr&scisig=AAGBfm0AAAAAXyqoTg70fvwym-N8pfn8ArAQI9TxlNTu&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
11
-------------------------------------------------
2020-08-05 12:28:38
Got the results of the query
{'bib': {'abstract': 'Multilingual training of neural machine translation '
                     '(NMT) systems has led to impressive accuracy '
                     'improvements on low-resource languages. However, there '
                     'are still significant challenges in efficiently learning '
                     'word representations in the face of paucity of data. In '
                     'this paper, we propose Soft Decoupled Encoding (SDE), a '
                     'multilingual lexicon encoding framework specifically '
                     'designed to share lexical-level information '
                     'intelligently without requiring heuristic preprocessing '
                     'such as pre-segmenting the data. SDE represents a word',
         'author': ['X Wang', 'H Pham', 'P Arthur', 'G Neubig'],
         'cites': '17',
         'eprint': 'https://arxiv.org/pdf/1902.03499',
         'gsrank': '1',
         'title': 'Multilingual neural machine translation with soft decoupled '
                  'encoding',
         'url': 'https://arxiv.org/abs/1902.03499',
         'venue': 'arXiv preprint arXiv:1902.03499',
         'year': '2019'},
 'citations_link': '/scholar?cites=1841872742547049658&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMultilingual%2BNeural%2BMachine%2BTranslation%2BWith%2BSoft%2BDecoupled%2BEncoding%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=usz9LICljxkJ&ei=-6UqX4y5JcKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:usz9LICljxkJ:scholar.google.com/&output=citation&scisdr=CgU4oggMGAA:AAGBfm0AAAAAXyqoYo0aKRwIywNdyEmZg8njRtjE42W8&scisig=AAGBfm0AAAAAXyqoYudSf0wGvZVskwbNYkV3qsEceAHr&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
17
-------------------------------------------------
2020-08-05 12:28:58
Got the results of the query
{'bib': {'abstract': 'Deep Neural Networks (DNNs) excel on many complex '
                     'perceptual tasks but it has proven notoriously difficult '
                     'to understand how they reach their decisions. We here '
                     'introduce a high-performance DNN architecture on '
                     'ImageNet whose decisions are considerably easier to '
                     'explain. Our model, a simple variant of the ResNet-50 '
                     'architecture called BagNet, classifies an image based on '
                     'the occurrences of small local image features without '
                     'taking into account their spatial ordering. This '
                     'strategy is closely related to the bag-of-feature (BoF) '
                     'models',
         'author': ['W Brendel', 'M Bethge'],
         'cites': '136',
         'eprint': 'https://arxiv.org/pdf/1904.00760',
         'gsrank': '1',
         'title': 'Approximating cnns with bag-of-local-features models works '
                  'surprisingly well on imagenet',
         'url': 'https://arxiv.org/abs/1904.00760',
         'venue': 'arXiv preprint arXiv:1904.00760',
         'year': '2019'},
 'citations_link': '/scholar?cites=13421262728275736184&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DApproximating%2BCNNs%2Bwith%2BBag-of-local-Features%2Bmodels%2Bworks%2Bsurprisingly%2Bwell%2Bon%2BImageNet%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=eLYQrFfnQboJ&ei=EKYqX72yB4vrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:eLYQrFfnQboJ:scholar.google.com/&output=citation&scisdr=CgU4oggMGAA:AAGBfm0AAAAAXyqobg-BEfzBVZ4Bn4XoQv3stTRfNpRY&scisig=AAGBfm0AAAAAXyqobomeWwRZTDLyiearxsS5_xomazMt&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
136
-------------------------------------------------
2020-08-05 12:29:10
Trying new proxy
Working proxy: http://188.165.16.230:3129
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Got the results of the query
{'bib': {'abstract': 'Solving tasks in Reinforcement Learning is no easy feat. '
                     'As the goal of the agent is to maximize the accumulated '
                     'reward, it often learns to exploit loopholes and '
                     'misspecifications in the reward signal resulting in '
                     'unwanted behavior. While constraints may solve this '
                     'issue',
         'author': ['C Tessler', 'DJ Mankowitz', 'S Mannor'],
         'cites': '44',
         'eprint': 'https://arxiv.org/pdf/1805.11074',
         'gsrank': '1',
         'title': 'Reward constrained policy optimization',
         'url': 'https://arxiv.org/abs/1805.11074',
         'venue': 'arXiv preprint arXiv:1805.11074',
         'year': '2018'},
 'citations_link': '/scholar?cites=8528215054992084387&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DReward%2BConstrained%2BPolicy%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ow3xeGhOWnYJ&ei=A6cqX_PMBoKTygT3w4zwCg',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ow3xeGhOWnYJ:scholar.google.com/&output=citation&scisdr=CgVBVBkWGAA:AAGBfm0AAAAAXyqpXoYHHSt2dlz0qqzOMaww2e7uAT5P&scisig=AAGBfm0AAAAAXyqpXo7PNj4DSE3Xgthyw7rTYHDdM3Xi&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
44
-------------------------------------------------
2020-08-05 12:33:11
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Got the results of the query
{'bib': {'abstract': 'Stochastic Gradient Descent (SGD) based training of '
                     'neural networks with a large learning rate or a small '
                     'batch-size typically ends in well-generalizing, flat '
                     'regions of the weight space, as indicated by small '
                     'eigenvalues of the Hessian of the training loss. '
                     'However, the curvature along the SGD trajectory is '
                     'poorly understood. An empirical investigation shows that '
                     'initially SGD visits increasingly sharp regions, '
                     'reaching a maximum sharpness determined by both the '
                     'learning rate and the batch-size of SGD. When studying '
                     'the SGD dynamics in relation to',
         'author': ['S Jastrzebski', 'Z Kenton', 'N Ballas', 'A Fischer'],
         'cites': '21',
         'eprint': 'https://arxiv.org/pdf/1807.05031',
         'gsrank': '1',
         'title': 'On the relation between the sharpest directions of DNN loss '
                  'and the SGD step length',
         'url': 'https://arxiv.org/abs/1807.05031',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=363039249847439697&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BRelation%2BBetween%2Bthe%2BSharpest%2BDirections%2Bof%2BDNN%2BLoss%2Band%2Bthe%2BSGD%2BStep%2BLength%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=UVGuNkbGCQUJ&ei=lKgqX5D_HojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:UVGuNkbGCQUJ:scholar.google.com/&output=citation&scisdr=CgU4ogjXGAA:AAGBfm0AAAAAXyqq8FOzyEl_z-uFe6QXUjqDxzJuZndx&scisig=AAGBfm0AAAAAXyqq8CBGy9TUbis9EGDAx6jovKDM79PI&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
21
-------------------------------------------------
2020-08-05 12:39:52
Got the results of the query
{'bib': {'abstract': 'In model-based reinforcement learning, the agent '
                     'interleaves between model learning and planning. These '
                     'two components are inextricably intertwined. If the '
                     'model is not able to provide sensible long-term '
                     'prediction, the executed planer would exploit model '
                     'flaws, which can yield catastrophic failures. This paper '
                     'focuses on building a model that reasons about the '
                     'long-term future and demonstrates how to use this for '
                     'efficient planning and exploration. To this end, we '
                     'build a latent-variable autoregressive model by '
                     'leveraging recent ideas in',
         'author': ['NR Ke', 'A Singh', 'A Touati', 'A Goyal'],
         'cites': '10',
         'eprint': 'https://openreview.net/pdf?id=SkgQBn0cF7',
         'gsrank': '1',
         'title': 'Modeling the long term future in model-based reinforcement '
                  'learning',
         'url': 'https://openreview.net/forum?id=SkgQBn0cF7&noteId=HJlR5FsT2Q',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=14570382599498236785&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModeling%2Bthe%2BLong%2BTerm%2BFuture%2Bin%2BModel-Based%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=cQOg7MZlNMoJ&ei=nqgqX4_ZJIjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:cQOg7MZlNMoJ:scholar.google.com/&output=citation&scisdr=CgU4ogv9GAA:AAGBfm0AAAAAXyqq_ufV0JVbkiH015L2MZl3tVnttW_c&scisig=AAGBfm0AAAAAXyqq_sBYdcvs5mqjCI3QdZzWsU0GtlM2&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
10
-------------------------------------------------
2020-08-05 12:40:07
Got the results of the query
{'bib': {'abstract': 'Training activation quantized neural networks involves '
                     'minimizing a piecewise constant function whose gradient '
                     'vanishes almost everywhere, which is undesirable for the '
                     'standard back-propagation or chain rule. An empirical '
                     'way around this issue is to use a straight-through '
                     'estimator (STE)(Bengio et al., 2013) in the backward '
                     'pass only, so that the" gradient" through the modified '
                     'chain rule becomes non-trivial. Since this unusual" '
                     'gradient" is certainly not the gradient of loss '
                     'function, the following question arises: why searching '
                     'in',
         'author': ['P Yin', 'J Lyu', 'S Zhang', 'S Osher', 'Y Qi', 'J Xin'],
         'cites': '35',
         'eprint': 'https://arxiv.org/pdf/1903.05662',
         'gsrank': '1',
         'title': 'Understanding straight-through estimator in training '
                  'activation quantized neural nets',
         'url': 'https://arxiv.org/abs/1903.05662',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=3760388634450301972&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnderstanding%2BStraight-Through%2BEstimator%2Bin%2BTraining%2BActivation%2BQuantized%2BNeural%2BNets%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=FJTTJYqVLzQJ&ei=q6gqX1GPmJgB5pWsqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:FJTTJYqVLzQJ:scholar.google.com/&output=citation&scisdr=CgXahJ8MGAA:AAGBfm0AAAAAXyqrCB0Qk6z8Z_Nc4Te6g2ZLYU2XF3fN&scisig=AAGBfm0AAAAAXyqrCIgNk7siwMZLb8hvPnV9tp0W_bcf&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
35
-------------------------------------------------
2020-08-05 12:40:16
Got the results of the query
{'bib': {'abstract': 'We propose Distributional Concavity (DC) regularization '
                     'for Generative Adversarial Networks (GANs), a functional '
                     'gradient-based method that promotes the entropy of the '
                     'generator distribution and works against mode collapse. '
                     'Our DC regularization is an easy-to-implement method '
                     'that can be used in combination with the current state '
                     'of the art methods like Spectral Normalization and '
                     'Wasserstein GAN with gradient penalty to further improve '
                     'the performance. We will not only show that our DC '
                     'regularization can achieve highly',
         'author': ['S Yamaguchi', 'M Koyama'],
         'cites': '4',
         'eprint': 'https://openreview.net/pdf?id=SklEEnC5tQ',
         'gsrank': '1',
         'title': 'Distributional concavity regularization for GANs',
         'url': 'https://openreview.net/forum?id=SklEEnC5tQ&noteId=rJllM_RzaQ',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=10380234928917973092&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDistributional%2BConcavity%2BRegularization%2Bfor%2BGANs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ZAhCx6sADpAJ&ei=tqgqX824EojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ZAhCx6sADpAJ:scholar.google.com/&output=citation&scisdr=CgUX_2URGAA:AAGBfm0AAAAAXyqrFPpN5NEhvMIdEc4zbkOI8UhNnlAE&scisig=AAGBfm0AAAAAXyqrFJYJMv9GixlWGatk4E9lIw6WUfWs&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
4
-------------------------------------------------
2020-08-05 12:40:29
Got the results of the query
{'bib': {'abstract': 'Neuronal assemblies, loosely defined as subsets of '
                     'neurons with reoccurring spatio-temporally coordinated '
                     'activation patterns, or" motifs", are thought to be '
                     'building blocks of neural representations and '
                     'information processing. We here propose LeMoNADe, a new '
                     'exploratory data analysis method that facilitates '
                     'hunting for motifs in calcium imaging videos, the '
                     'dominant microscopic functional imaging modality in '
                     'neurophysiology. Our nonparametric method extracts '
                     'motifs directly from videos, bypassing the difficult',
         'author': ['E Kirschbaum', 'M Haußmann', 'S Wolf', 'H Sonntag'],
         'cites': '2',
         'eprint': 'https://arxiv.org/pdf/1806.09963',
         'gsrank': '1',
         'title': 'LeMoNADe: Learned Motif and Neuronal Assembly Detection in '
                  'calcium imaging videos',
         'url': 'https://arxiv.org/abs/1806.09963',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=16794354699308703573&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLeMoNADe:%2BLearned%2BMotif%2Band%2BNeuronal%2BAssembly%2BDetection%2Bin%2Bcalcium%2Bimaging%2Bvideos%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=VR81ZJ6IEekJ&ei=yKgqX53LLaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:VR81ZJ6IEekJ:scholar.google.com/&output=citation&scisdr=CgU4ognzGAA:AAGBfm0AAAAAXyqrJWbhwgfL8L3QV4AwQaJ3dlUff0vG&scisig=AAGBfm0AAAAAXyqrJaFqIMCzP4HASOyfmkyw3qbQyDoU&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
2
-------------------------------------------------
2020-08-05 12:40:45
Got the results of the query
{'bib': {'abstract': 'Deep learning has achieved remarkable successes in '
                     'solving challenging reinforcement learning (RL) problems '
                     'when dense reward function is provided. However, in '
                     'sparse reward environment it still often suffers from '
                     'the need to carefully shape reward function to guide '
                     'policy optimization. This limits the applicability of RL '
                     'in the real world since both reinforcement learning and '
                     'domain-specific knowledge are required. It is therefore '
                     'of great practical importance to develop algorithms '
                     'which can learn from a binary signal indicating',
         'author': ['H Liu', 'A Trott', 'R Socher', 'C Xiong'],
         'cites': '10',
         'eprint': 'https://arxiv.org/pdf/1902.00528',
         'gsrank': '1',
         'title': 'Competitive experience replay',
         'url': 'https://arxiv.org/abs/1902.00528',
         'venue': 'arXiv preprint arXiv:1902.00528',
         'year': '2019'},
 'citations_link': '/scholar?cites=8704818254702169597&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCompetitive%2Bexperience%2Breplay%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=_c3bIRW6zXgJ&ei=3KgqX8HmG6OGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:_c3bIRW6zXgJ:scholar.google.com/&output=citation&scisdr=CgU9q1ywGAA:AAGBfm0AAAAAXyqrOIJaFaNFCGvsOJODqRnF6T28nMBn&scisig=AAGBfm0AAAAAXyqrOHV2jqArKGhdOqLh2Ptmtf0HOzG9&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
10
-------------------------------------------------
2020-08-05 12:41:05
Got the results of the query
{'bib': {'abstract': 'The goal of this paper is to learn cross-domain '
                     'representations for slot filling task in spoken language '
                     'understanding (SLU). Most of the recently published SLU '
                     'models are domain-specific ones that work on individual '
                     'task domains. Annotating data for each individual task',
         'author': ['B Liu', 'I Lane'],
         'cites': '18',
         'eprint': 'https://arxiv.org/pdf/1711.11310',
         'gsrank': '1',
         'title': 'Multi-domain adversarial learning for slot filling in '
                  'spoken language understanding',
         'url': 'https://arxiv.org/abs/1711.11310',
         'venue': 'arXiv preprint arXiv:1711.11310',
         'year': '2017'},
 'citations_link': '/scholar?cites=13270376286682899607&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-Domain%2BAdversarial%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=l2SAW-zYKbgJ&ei=8qgqX_3oM4vrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:l2SAW-zYKbgJ:scholar.google.com/&output=citation&scisdr=CgXsOAkqGAA:AAGBfm0AAAAAXyqrUFVBUSgZAyrxvukezmFQYS8-Scps&scisig=AAGBfm0AAAAAXyqrUMeL0ncA_i-gUO9S9nVS7Z6-h_Vj&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
18
-------------------------------------------------
2020-08-05 12:41:28
Got the results of the query
{'bib': {'abstract': 'Credit assignment in Meta-reinforcement learning '
                     '(Meta-RL) is still poorly understood. Existing methods '
                     'either neglect credit assignment to pre-adaptation '
                     'behavior or implement it naively. This leads to poor '
                     'sample-efficiency during meta-training as well as '
                     'ineffective task',
         'author': ['J Rothfuss', 'D Lee', 'I Clavera', 'T Asfour'],
         'cites': '54',
         'eprint': 'https://arxiv.org/pdf/1810.06784',
         'gsrank': '1',
         'title': 'Promp: Proximal meta-policy search',
         'url': 'https://arxiv.org/abs/1810.06784',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=5271959514847376578&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProMP:%2BProximal%2BMeta-Policy%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=wvS3KD_DKUkJ&ei=_6gqX-ukIY-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:wvS3KD_DKUkJ:scholar.google.com/&output=citation&scisdr=CgXsOAkqGAA:AAGBfm0AAAAAXyqrW_BD3X44f-k5YriGrkbvsZ_VZPdo&scisig=AAGBfm0AAAAAXyqrW_pxCHDo6z9sVIMFoazNJ0jIEBk9&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
54
-------------------------------------------------
2020-08-05 12:41:39
Got the results of the query
{'bib': {'abstract': 'Recent literature suggests that averaged word vectors '
                     'followed by simple post-processing outperform many deep '
                     'learning methods on semantic textual similarity tasks. '
                     'Furthermore, when averaged word vectors are trained '
                     'supervised on large corpora of paraphrases, they achieve '
                     'state-of-the-art results on standard STS benchmarks. '
                     'Inspired by these insights, we push the limits of word '
                     'embeddings even further. We propose a novel fuzzy '
                     'bag-of-words (FBoW) representation for text that '
                     'contains all the words in the vocabulary simultaneously',
         'author': ['V Zhelezniak', 'A Savkov', 'A Shen', 'F Moramarco'],
         'cites': '16',
         'eprint': 'https://arxiv.org/pdf/1904.13264',
         'gsrank': '1',
         'title': "Don't Settle for Average, Go for the Max: Fuzzy Sets and "
                  'Max-Pooled Word Vectors',
         'url': 'https://arxiv.org/abs/1904.13264',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=17199150617564073243&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDon%255C%2527t%2BSettle%2Bfor%2BAverage,%2BGo%2Bfor%2Bthe%2BMax:%2BFuzzy%2BSets%2Band%2BMax-Pooled%2BWord%2BVectors%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=G81Yllyor-4J&ei=CKkqX-EKqIHL1g-ag5aoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:G81Yllyor-4J:scholar.google.com/&output=citation&scisdr=CgXsOAkqGAA:AAGBfm0AAAAAXyqrZHjdMJa-E-6wsX1sGz_8nnsagbee&scisig=AAGBfm0AAAAAXyqrZLjO_DixWfbnKZw024WfzHgz2bwr&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
16
-------------------------------------------------
2020-08-05 12:41:48
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://206.198.131.142:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://14.140.131.82:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Got the results of the query
{'bib': {'abstract': 'A growing number of learning methods are actually '
                     'differentiable games whose players optimise multiple, '
                     'interdependent objectives in parallel--from GANs and '
                     'intrinsic curiosity to multi-agent RL. Opponent shaping '
                     'is a powerful approach to improve learning dynamics in',
         'author': ['A Letcher', 'J Foerster', 'D Balduzzi', 'T Rocktäschel'],
         'cites': '26',
         'eprint': 'https://arxiv.org/pdf/1811.08469',
         'gsrank': '1',
         'title': 'Stable opponent shaping in differentiable games',
         'url': 'https://arxiv.org/abs/1811.08469',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=9514174304819895562&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStable%2BOpponent%2BShaping%2Bin%2BDifferentiable%2BGames%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Cl3wOR8jCYQJ&ei=j6oqX56tCIjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Cl3wOR8jCYQJ:scholar.google.com/&output=citation&scisdr=CgUmJmqGGAA:AAGBfm0AAAAAXyqs78TS4g87J1LPBnrjZqSOCXy1wzaN&scisig=AAGBfm0AAAAAXyqs79Lsdicj8MljBPWZp6MR6Zk2pAqw&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
26
-------------------------------------------------
2020-08-05 12:48:23
Got the results of the query
-------------------------------------------------
2020-08-05 12:48:38
Got the results of the query
{'bib': {'abstract': 'Numerous past works have tackled the problem of '
                     'task-driven navigation. But, how to effectively explore '
                     'a new environment to enable a variety of down-stream '
                     'tasks has received much less attention. In this work, we '
                     'study how agents can autonomously explore realistic and '
                     'complex 3D environments without the context of '
                     'task-rewards. We propose a learning-based approach and '
                     'investigate different policy architectures, reward '
                     'functions, and training paradigms. We find that the use '
                     'of policies with spatial memory that are bootstrapped '
                     'with',
         'author': ['T Chen', 'S Gupta', 'A Gupta'],
         'cites': '30',
         'eprint': 'https://arxiv.org/pdf/1903.01959',
         'gsrank': '1',
         'title': 'Learning exploration policies for navigation',
         'url': 'https://arxiv.org/abs/1903.01959',
         'venue': 'arXiv preprint arXiv:1903.01959',
         'year': '2019'},
 'citations_link': '/scholar?cites=1526633576375251578&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BExploration%2BPolicies%2Bfor%2BNavigation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=evIHNiaxLxUJ&ei=sKoqX_6pIYvrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:evIHNiaxLxUJ:scholar.google.com/&output=citation&scisdr=CgWyM0r_GAA:AAGBfm0AAAAAXyqtDd_uslEHBTspSRrNHV84l9mRdUxK&scisig=AAGBfm0AAAAAXyqtDe34k-156DmSQby2yjT9yB1KgP-a&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
30
-------------------------------------------------
2020-08-05 12:48:53
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://104.45.188.43:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://117.20.26.21:8080
Trying new proxy
Working proxy: http://117.20.26.21:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://159.192.242.45:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://14.140.131.82:3128
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://13.75.114.68:25222
Trying new proxy
Working proxy: http://159.192.242.45:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://80.23.125.226:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://206.198.131.172:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://13.75.114.68:25222
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://18.221.58.204:3838
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Got the results of the query
{'bib': {'abstract': 'We investigate the properties of multidimensional '
                     'probability distributions in the context of latent space '
                     'prior distributions of implicit generative models. Our '
                     'work revolves around the phenomena arising while '
                     'decoding linear interpolations between two random latent '
                     'vectors--regions of latent space in close proximity to '
                     'the origin of the space are oversampled, which restricts '
                     'the usability of linear interpolations as a tool to '
                     'analyse the latent space. We show that the distribution '
                     'mismatch can be eliminated completely by a proper choice '
                     'of the latent',
         'author': ['D Leśniak', 'I Sieradzki', 'I Podolak'],
         'cites': '3',
         'gsrank': '1',
         'title': 'Distribution-Interpolation Trade off in Generative Models',
         'url': 'https://openreview.net/forum?id=SyMhLo0qKQ&source=post_page---------------------------',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=5332441111538875953&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDistribution-Interpolation%2BTrade%2Boff%2Bin%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Mb72PvCiAEoJ&ei=cbIqX9PONaiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Mb72PvCiAEoJ:scholar.google.com/&output=citation&scisdr=CgUX_2UFGAA:AAGBfm0AAAAAXyq00JtNDBBkPYV13INunvXdyf8Ld2YH&scisig=AAGBfm0AAAAAXyq00DcNboQ5_JOOdT2BxmWBey25qrNt&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
3
-------------------------------------------------
2020-08-05 13:22:00
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://185.21.217.58:3128
Got the results of the query
-------------------------------------------------
2020-08-05 13:23:16
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://18.221.58.204:3838
Trying new proxy
Working proxy: http://81.201.60.130:80
Got the results of the query
{'bib': {'abstract': 'As people learn to navigate the world, autonomic nervous '
                     'system (eg," fight or flight") responses provide '
                     'intrinsic feedback about the potential consequence of '
                     'action choices (eg, becoming nervous when close to a '
                     'cliff edge or driving fast around a bend.) Physiological '
                     'changes are correlated with these biological '
                     'preparations to protect one-self from danger. We present '
                     'a novel approach to reinforcement learning that '
                     'leverages a task-independent intrinsic reward function '
                     'trained on peripheral pulse measurements that are '
                     'correlated with',
         'author': ['D McDuff', 'A Kapoor'],
         'cites': '5',
         'eprint': 'https://arxiv.org/pdf/1805.09975',
         'gsrank': '1',
         'title': 'Visceral machines: Risk-aversion in reinforcement learning '
                  'with intrinsic physiological rewards',
         'url': 'https://arxiv.org/abs/1805.09975',
         'venue': 'arXiv preprint arXiv:1805.09975',
         'year': '2018'},
 'citations_link': '/scholar?cites=13666297261471235343&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVisceral%2BMachines:%2BRisk-Aversion%2Bin%2BReinforcement%2BLearning%2Bwith%2BIntrinsic%2BPhysiological%2BRewards%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=D_3iwPNwqL0J&ei=UrMqX_yCHoyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:D_3iwPNwqL0J:scholar.google.com/&output=citation&scisdr=CgU4oggCGAA:AAGBfm0AAAAAXyq1r5J62t-uXVY5r_xiYu5_pMZNYpvV&scisig=AAGBfm0AAAAAXyq1r8GDGkX2Z26CLVBqx3WaTIG9idKv&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 13:25:43
Got the results of the query
{'bib': {'abstract': 'Learning a deep neural network requires solving a '
                     'challenging optimization problem: it is a '
                     'high-dimensional, non-convex and non-smooth minimization '
                     'problem with a large number of terms. The current '
                     'practice in neural network optimization is to rely on '
                     'the stochastic gradient descent (SGD) algorithm or its '
                     'adaptive variants. However, SGD requires a hand-designed '
                     'schedule for the learning rate. In addition, its '
                     'adaptive variants tend to produce solutions that '
                     'generalize less well on unseen data than SGD with a '
                     'hand-designed',
         'author': ['L Berrada', 'A Zisserman', 'MP Kumar'],
         'cites': '11',
         'eprint': 'https://arxiv.org/pdf/1811.07591',
         'gsrank': '1',
         'title': 'Deep frank-wolfe for neural network optimization',
         'url': 'https://arxiv.org/abs/1811.07591',
         'venue': 'arXiv preprint arXiv:1811.07591',
         'year': '2018'},
 'citations_link': '/scholar?cites=17584931574409094808&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BFrank-Wolfe%2BFor%2BNeural%2BNetwork%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=mGY0FBk6CvQJ&ei=W7MqX4i3EY-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:mGY0FBk6CvQJ:scholar.google.com/&output=citation&scisdr=CgWyOr3eGAA:AAGBfm0AAAAAXyq1tx507-sPbwRXySMA41NfssanHeGd&scisig=AAGBfm0AAAAAXyq1t63Sp7fH23nNmjzskvwSKMHWiwZc&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
11
-------------------------------------------------
2020-08-05 13:25:51
Got the results of the query
{'bib': {'abstract': 'The goal of few-shot learning is to learn a classifier '
                     'that generalizes well even when trained with a limited '
                     'number of training instances per class. The recently '
                     'introduced meta-learning approaches tackle this problem '
                     'by learning a generic classifier across a large number '
                     'of multiclass classification tasks and generalizing the '
                     'model to a new task. Yet, even with such meta-learning, '
                     'the low-data problem in the novel classification task '
                     'still remains. In this paper, we propose Transductive '
                     'Propagation Network (TPN), a novel meta-learning',
         'author': ['Y Liu', 'J Lee', 'M Park', 'S Kim', 'E Yang', 'SJ Hwang'],
         'cites': '122',
         'eprint': 'https://arxiv.org/pdf/1805.10002',
         'gsrank': '1',
         'title': 'Learning to propagate labels: Transductive propagation '
                  'network for few-shot learning',
         'url': 'https://arxiv.org/abs/1805.10002',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=6189607533521090437&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BPropagate%2BLabels:%2BTransductive%2BPropagation%2BNetwork%2Bfor%2BFew-Shot%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=hTeb_kHn5VUJ&ei=ZrMqX8bDEYyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:hTeb_kHn5VUJ:scholar.google.com/&output=citation&scisdr=CgUX_2UJGAA:AAGBfm0AAAAAXyq1xHNYmD6BY5yJW9ryfpsknDbjfnRd&scisig=AAGBfm0AAAAAXyq1xGOXssOO-lrCqKfcMTMu60_U1Fnk&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
122
-------------------------------------------------
2020-08-05 13:26:04
Got the results of the query
{'bib': {'abstract': 'By injecting adversarial examples into training data, '
                     'adversarial training is promising for improving the '
                     'robustness of deep learning models. However, most '
                     'existing adversarial training approaches are based on a '
                     'specific type of adversarial attack. It may not provide '
                     'sufficiently representative samples from the adversarial '
                     'domain, leading to a weak generalization ability on '
                     'adversarial examples from other attacks. Moreover, '
                     'during the adversarial training, adversarial '
                     'perturbations on inputs are usually crafted by fast '
                     'single',
         'author': ['C Song', 'K He', 'L Wang', 'JE Hopcroft'],
         'cites': '34',
         'eprint': 'https://arxiv.org/pdf/1810.00740',
         'gsrank': '1',
         'title': 'Improving the generalization of adversarial training with '
                  'domain adaptation',
         'url': 'https://arxiv.org/abs/1810.00740',
         'venue': 'arXiv preprint arXiv:1810.00740',
         'year': '2018'},
 'citations_link': '/scholar?cites=12534049630846932475&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2Bthe%2BGeneralization%2Bof%2BAdversarial%2BTraining%2Bwith%2BDomain%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=-1W_I7vj8a0J&ei=crMqX6CqAovrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:-1W_I7vj8a0J:scholar.google.com/&output=citation&scisdr=CgUX_2UJGAA:AAGBfm0AAAAAXyq1zwQRkOS3nKqk6fuQk7N6tUR6lx7o&scisig=AAGBfm0AAAAAXyq1z_5-hKU2ceY-zDXoAKn0-DGqc_DP&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
34
-------------------------------------------------
2020-08-05 13:26:15
Got the results of the query
{'bib': {'abstract': 'Most deep learning models rely on expressive '
                     'high-dimensional representations to achieve good '
                     'performance on tasks such as classification. However, '
                     'the high dimensionality of these representations makes '
                     'them difficult to interpret and prone to over-fitting. '
                     'We propose a simple, intuitive and scalable dimension '
                     'reduction framework that takes into account the soft '
                     'probabilistic interpretation of standard deep models for '
                     'classification. When applying our framework to '
                     'visualization, our representations more accurately '
                     'reflect inter-class distances',
         'author': ['MT Law', 'J Snell', 'A Farahmand', 'R Urtasun'],
         'cites': '8',
         'gsrank': '1',
         'title': 'Dimensionality reduction for representing the knowledge of '
                  'probabilistic models',
         'url': 'https://openreview.net/forum?id=SygD-hCcF7&noteId=B1gOV3x90Q',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=9583060097070031668&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDimensionality%2BReduction%2Bfor%2BRepresenting%2Bthe%2BKnowledge%2Bof%2BProbabilistic%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=NGMUk2Le_YQJ&ei=fLMqX_2QJ5qGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:NGMUk2Le_YQJ:scholar.google.com/&output=citation&scisdr=CgUX_2UJGAA:AAGBfm0AAAAAXyq12gVJt7LvWAzVN4vs0Unji9iCfxlp&scisig=AAGBfm0AAAAAXyq12qvDS1Nt9nFMBrrsOZXp0XHb2Uf5&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
8
-------------------------------------------------
2020-08-05 13:26:26
Got the results of the query
{'bib': {'abstract': 'Inferring the structural properties of a protein from '
                     'its amino acid sequence is a challenging yet important '
                     'problem in biology. Structures are not known for the '
                     'vast majority of protein sequences, but structure is '
                     'critical for understanding function. Existing approaches '
                     'for detecting structural similarity between proteins '
                     'from sequence are unable to recognize and exploit '
                     'structural patterns when sequences have diverged too '
                     'far, limiting our ability to transfer knowledge between '
                     'structurally related proteins. We newly approach this '
                     'problem',
         'author': ['T Bepler', 'B Berger'],
         'cites': '30',
         'eprint': 'https://arxiv.org/pdf/1902.08661',
         'gsrank': '1',
         'title': 'Learning protein sequence embeddings using information from '
                  'structure',
         'url': 'https://arxiv.org/abs/1902.08661',
         'venue': 'arXiv preprint arXiv:1902.08661',
         'year': '2019'},
 'citations_link': '/scholar?cites=15164585032422536283&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bprotein%2Bsequence%2Bembeddings%2Busing%2Binformation%2Bfrom%2Bstructure%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=WzTA171tc9IJ&ei=hrMqX-CPEIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:WzTA171tc9IJ:scholar.google.com/&output=citation&scisdr=CgU4oggRGAA:AAGBfm0AAAAAXyq151odv9P1xaQnZ4vg-b3WhZ6dT9gD&scisig=AAGBfm0AAAAAXyq15xn24Oxh3RJDLjHae7Ng2w97fgfx&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
30
-------------------------------------------------
2020-08-05 13:26:40
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://34.217.107.252:8888
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://104.45.188.43:3128
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Got the results of the query
{'bib': {'abstract': 'We present a new theoretical perspective of data noising '
                     'in recurrent neural network language models (Xie et al., '
                     '2017). We show that each variant of data noising is an '
                     'instance of Bayesian recurrent neural networks with a '
                     'particular variational distribution (ie, a mixture of '
                     'Gaussians whose weights depend on statistics derived '
                     'from the corpus such as the unigram distribution). We '
                     'use this insight to propose a more principled method to '
                     'apply at prediction time and propose natural extensions '
                     'to data noising under the variational',
         'author': ['L Kong', 'G Melis', 'W Ling', 'L Yu', 'D Yogatama'],
         'cites': '1',
         'eprint': 'https://arxiv.org/pdf/1901.09296',
         'gsrank': '1',
         'title': 'Variational Smoothing in Recurrent Neural Network Language '
                  'Models',
         'url': 'https://arxiv.org/abs/1901.09296',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=10055945762623652654&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BSmoothing%2Bin%2BRecurrent%2BNeural%2BNetwork%2BLanguage%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=LgPHdWTljYsJ&ei=x7YqX9KPO4yimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:LgPHdWTljYsJ:scholar.google.com/&output=citation&scisdr=CgUzb8bFGAA:AAGBfm0AAAAAXyq5J-lOf5sbLN1102C-dGEq8RLcbTvX&scisig=AAGBfm0AAAAAXyq5J0ZBvTO-oXzPKHjNbyHWNTrX8MX2&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
1
-------------------------------------------------
2020-08-05 13:40:31
Got the results of the query
{'bib': {'abstract': 'The backpropagation (BP) algorithm is often thought to '
                     'be biologically implausible in the brain. One of the '
                     'main reasons is that BP requires symmetric weight '
                     'matrices in the feedforward and feedback pathways. To '
                     'address this" weight transport problem"(Grossberg, '
                     '1987), two more biologically plausible algorithms, '
                     'proposed by Liao et al.(2016) and Lillicrap et '
                     "al.(2016), relax BP's weight symmetry requirements and "
                     'demonstrate comparable learning capabilities to that of '
                     'BP on small datasets. However, a recent study by '
                     'Bartunov et',
         'author': ['W Xiao', 'H Chen', 'Q Liao', 'T Poggio'],
         'cites': '24',
         'eprint': 'https://arxiv.org/pdf/1811.03567',
         'gsrank': '1',
         'title': 'Biologically-plausible learning algorithms can scale to '
                  'large datasets',
         'url': 'https://arxiv.org/abs/1811.03567',
         'venue': 'arXiv preprint arXiv:1811.03567',
         'year': '2018'},
 'citations_link': '/scholar?cites=10952740218459903429&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBiologically-Plausible%2BLearning%2BAlgorithms%2BCan%2BScale%2Bto%2BLarge%2BDatasets%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=xdVv1jLz_5cJ&ei=1LYqX-fRDaiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:xdVv1jLz_5cJ:scholar.google.com/&output=citation&scisdr=CgU4ogiBGAA:AAGBfm0AAAAAXyq5MuhW-3SODqEFlfaOiT6RdCv-XnHh&scisig=AAGBfm0AAAAAXyq5Mg5B2RA0NpfXIiOc6u4eqPBKXk4q&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
24
-------------------------------------------------
2020-08-05 13:40:42
Got the results of the query
{'bib': {'abstract': 'End-to-end neural models have made significant progress '
                     'in question answering, however recent studies show that '
                     'these models implicitly assume that the answer and '
                     'evidence appear close together in a single document. In '
                     'this work, we propose the Coarse-grain Fine-grain '
                     'Coattention Network (CFC), a new question answering '
                     'model that combines information from evidence across '
                     'multiple documents. The CFC consists of a coarse-grain '
                     'module that interprets documents with respect to the '
                     'query then finds a relevant answer, and',
         'author': ['V Zhong', 'C Xiong', 'NS Keskar', 'R Socher'],
         'cites': '25',
         'eprint': 'https://arxiv.org/pdf/1901.00603',
         'gsrank': '1',
         'title': 'Coarse-grain fine-grain coattention network for '
                  'multi-evidence question answering',
         'url': 'https://arxiv.org/abs/1901.00603',
         'venue': 'arXiv preprint arXiv:1901.00603',
         'year': '2019'},
 'citations_link': '/scholar?cites=6052706847759570603&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCoarse-grain%2BFine-grain%2BCoattention%2BNetwork%2Bfor%2BMulti-evidence%2BQuestion%2BAnswering%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=q5ZINs-I_1MJ&ei=4LYqX4vaEsKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:q5ZINs-I_1MJ:scholar.google.com/&output=citation&scisdr=CgU4ckB1GAA:AAGBfm0AAAAAXyq5PJIK0XipYII-9XNDFiqxmCZeP2_r&scisig=AAGBfm0AAAAAXyq5PLXm0arCLkaT_-zIbu3Jlj8Eq-MB&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
25
-------------------------------------------------
2020-08-05 13:40:52
Got the results of the query
-------------------------------------------------
2020-08-05 13:41:18
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://206.198.131.142:80
Trying new proxy
Working proxy: http://47.56.193.27:8089
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://139.59.126.114:3128
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://34.91.135.38:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://193.41.88.58:53281
Got the results of the query
{'bib': {'abstract': 'Deep networks realize complex mappings that are often '
                     'understood by their locally linear behavior at or around '
                     'points of interest. For example, we use the derivative '
                     'of the mapping with respect to its inputs for '
                     'sensitivity analysis, or to explain (obtain coordinate '
                     'relevance for) a prediction. One key challenge is that '
                     'such derivatives are themselves inherently unstable. In '
                     'this paper, we propose a new learning problem to '
                     'encourage deep networks to have stable derivatives over '
                     'larger regions. While the problem is challenging in '
                     'general, we',
         'author': ['GH Lee', 'D Alvarez-Melis', 'TS Jaakkola'],
         'cites': '15',
         'eprint': 'https://arxiv.org/pdf/1907.03207',
         'gsrank': '1',
         'title': 'Towards robust, locally linear deep networks',
         'url': 'https://arxiv.org/abs/1907.03207',
         'venue': 'arXiv preprint arXiv:1907.03207',
         'year': '2019'},
 'citations_link': '/scholar?cites=12036498269952329745&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BRobust,%2BLocally%2BLinear%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=EcgciGc7CqcJ&ei=troqX7OyGY-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:EcgciGc7CqcJ:scholar.google.com/&output=citation&scisdr=CgVAVzU6GAA:AAGBfm0AAAAAXyq9KnkYT_d978mpzl0e9BeBhP2tUCCw&scisig=AAGBfm0AAAAAXyq9KgYjPWiXLT4kQle3HS99rCeXfrrG&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 13:57:38
Got the results of the query
{'bib': {'abstract': 'Dashed arrows connect the relevant pathways with the '
                     'respective published work. For the sake of simplicity, '
                     'only the main findings of the papers are considered. '
                     'Please refer to the main text for details. We apologise '
                     'to authors of relevant work that had to be omitted',
         'author': ['LF Barros', 'B Weber'],
         'cites': '50',
         'eprint': 'https://www.ncbi.nlm.nih.gov/pmc/articles/pmc5792514/',
         'gsrank': '1',
         'title': 'CrossTalk proposal: an important astrocyte‐to‐neuron '
                  'lactate shuttle couples neuronal activity to glucose '
                  'utilisation in the brain',
         'url': 'https://www.ncbi.nlm.nih.gov/pmc/articles/pmc5792514/',
         'venue': 'The Journal of physiology',
         'year': '2018'},
 'citations_link': '/scholar?cites=13805007170721552523&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHow%2BImportant%2Bis%2Ba%2BNeuron%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ixiUNOE8lb8J&ei=27oqX42LH7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ixiUNOE8lb8J:scholar.google.com/&output=citation&scisdr=CgVAVzU6GAA:AAGBfm0AAAAAXyq9WgHWE5lq6V3Q6AkvZ8usTQaCQaMA&scisig=AAGBfm0AAAAAXyq9WhbGZTGbmAhnCWfwABvI3yjENzK1&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
50
-------------------------------------------------
2020-08-05 13:58:27
Got the results of the query
{'bib': {'abstract': 'Analogical reasoning has been a principal focus of '
                     'various waves of AI research. Analogy is particularly '
                     'challenging for machines because it requires relational '
                     'structures to be represented such that they can be '
                     'flexibly applied across diverse domains of experience. '
                     'Here, we study how analogical reasoning can be induced '
                     'in neural networks that learn to perceive and reason '
                     'about raw visual data. We find that the critical factor '
                     'for inducing such a capacity is not an elaborate '
                     'architecture, but rather, careful attention to the '
                     'choice of data',
         'author': ['F Hill', 'A Santoro', 'DGT Barrett', 'AS Morcos'],
         'cites': '15',
         'eprint': 'https://arxiv.org/pdf/1902.00120',
         'gsrank': '1',
         'title': 'Learning to make analogies by contrasting abstract '
                  'relational structure',
         'url': 'https://arxiv.org/abs/1902.00120',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=15521573039503233138&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BMake%2BAnalogies%2Bby%2BContrasting%2BAbstract%2BRelational%2BStructure%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ctDeTnG0Z9cJ&ei=DrsqX6miMIjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ctDeTnG0Z9cJ:scholar.google.com/&output=citation&scisdr=CgVAVzU6GAA:AAGBfm0AAAAAXyq9dfS_EXdxLwamF3PHvNpVEF-5QeV5&scisig=AAGBfm0AAAAAXyq9dUVwn3Q8c3eWh_tXEcbs8aGecGWo&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 13:58:53
Got the results of the query
{'bib': {'abstract': 'Intelligent agents can learn to represent the action '
                     'spaces of other agents simply by observing them act. '
                     'Such representations help agents quickly learn to '
                     'predict the effects of their own actions on the '
                     'environment and to plan complex action sequences. In '
                     "this work, we address the problem of learning an agent's "
                     'action space purely from visual observation. We use '
                     'stochastic video prediction to learn a latent variable '
                     "that captures the scene's dynamics while being minimally "
                     "sensitive to the scene's static content. We introduce a "
                     'loss',
         'author': ['O Rybkin', 'K Pertsch', 'KG Derpanis', 'K Daniilidis'],
         'cites': '2',
         'eprint': 'https://arxiv.org/pdf/1806.09655',
         'gsrank': '1',
         'title': 'Learning what you can do before doing anything',
         'url': 'https://arxiv.org/abs/1806.09655',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=6991177284121513943&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bwhat%2Byou%2Bcan%2Bdo%2Bbefore%2Bdoing%2Banything%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=18_X9bKmBWEJ&ei=JLsqX4SfO4-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:18_X9bKmBWEJ:scholar.google.com/&output=citation&scisdr=CgVAVzU6GAA:AAGBfm0AAAAAXyq9jLLVtGEnLPO_zYZO37CTKG6NO0eo&scisig=AAGBfm0AAAAAXyq9jAPaf7sv9oMQN2On831soo_Fquzn&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
2
-------------------------------------------------
2020-08-05 13:59:17
Got the results of the query
{'bib': {'abstract': 'This paper proposes a representational model for grid '
                     'cells. In this model, the 2D self-position of the agent '
                     'is represented by a high-dimensional vector, and the 2D '
                     'self-motion or displacement of the agent is represented '
                     'by a matrix that transforms the vector. Each component '
                     'of the vector is a unit or a cell. The model consists of '
                     'the following three sub-models.(1) Vector-matrix '
                     'multiplication. The movement from the current position '
                     'to the next position is modeled by matrix-vector '
                     'multiplication, ie, the vector of the next position is',
         'author': ['R Gao', 'J Xie', 'SC Zhu', 'YN Wu'],
         'cites': '9',
         'eprint': 'https://arxiv.org/pdf/1810.05597',
         'gsrank': '1',
         'title': 'Learning grid cells as vector representation of '
                  'self-position coupled with matrix representation of '
                  'self-motion',
         'url': 'https://arxiv.org/abs/1810.05597',
         'venue': 'arXiv preprint arXiv:1810.05597',
         'year': '2018'},
 'citations_link': '/scholar?cites=1267366913161335013&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BGrid%2BCells%2Bas%2BVector%2BRepresentation%2Bof%2BSelf-Position%2BCoupled%2Bwith%2BMatrix%2BRepresentation%2Bof%2BSelf-Motion%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=5ZAQI36XlhEJ&ei=R7sqX4SYG7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:5ZAQI36XlhEJ:scholar.google.com/&output=citation&scisdr=CgVAVzU6GAA:AAGBfm0AAAAAXyq9p82uOSvGa3fHAlhtLYp2p1hbrXxH&scisig=AAGBfm0AAAAAXyq9p7ao5bMydlwJHz3n2hK9IsqKrmFT&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 13:59:43
Got the results of the query
{'bib': {'abstract': 'Although stochastic gradient descent (SGD) method and '
                     'its variants (eg, stochastic momentum methods, AdaGrad) '
                     'are the choice of algorithms for solving non-convex '
                     'problems (especially deep learning), there still remain '
                     'big gaps between the theory and the practice with many '
                     'questions unresolved. For example, there is still a lack '
                     'of theories of convergence for SGD and its variants that '
                     'use stagewise step size and return an averaged solution '
                     'in practice. In addition, theoretical insights of why '
                     'adaptive step size of AdaGrad',
         'author': ['Z Chen', 'Z Yuan', 'J Yi', 'B Zhou', 'E Chen'],
         'cites': '24',
         'eprint': 'https://arxiv.org/pdf/1808.06296',
         'gsrank': '1',
         'title': 'Universal stagewise learning for non-convex problems with '
                  'convergence on averaged solutions',
         'url': 'https://arxiv.org/abs/1808.06296',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=13646070350942826341&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUniversal%2BStagewise%2BLearning%2Bfor%2BNon-Convex%2BProblems%2Bwith%2BConvergence%2Bon%2BAveraged%2BSolutions%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Ze_vJ6-UYL0J&ei=YbsqX6WIJaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Ze_vJ6-UYL0J:scholar.google.com/&output=citation&scisdr=CgVAVzU6GAA:AAGBfm0AAAAAXyq90lnPGS6ZXxxeRIdkqpA-tl_qJ6qp&scisig=AAGBfm0AAAAAXyq90i444ltkz244nkSJUEAaLTZpohAX&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
24
-------------------------------------------------
2020-08-05 14:00:26
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://85.47.31.179:3128
Trying new proxy
Working proxy: http://85.47.31.179:3128
Trying new proxy
Working proxy: http://103.53.188.110:80
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://103.28.121.58:3128
Trying new proxy
Working proxy: http://141.255.157.242:8080
Got the results of the query
{'bib': {'abstract': 'Invariant and equivariant networks have been '
                     'successfully used for learning images, sets, point '
                     'clouds, and graphs. A basic challenge in developing such '
                     'networks is finding the maximal collection of invariant '
                     'and equivariant linear layers. Although this question is '
                     'answered for the first three examples (for popular '
                     'transformations, at-least), a full characterization of '
                     'invariant and equivariant linear layers for graphs is '
                     'not known. In this paper we provide a characterization '
                     'of all permutation invariant and equivariant linear '
                     'layers',
         'author': ['H Maron', 'H Ben-Hamu', 'N Shamir', 'Y Lipman'],
         'cites': '59',
         'eprint': 'https://arxiv.org/pdf/1812.09902',
         'gsrank': '1',
         'title': 'Invariant and equivariant graph networks',
         'url': 'https://arxiv.org/abs/1812.09902',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=17830445355098449552&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInvariant%2Band%2BEquivariant%2BGraph%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=kCaoqZR3cvcJ&ei=JL0qX6_KLKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:kCaoqZR3cvcJ:scholar.google.com/&output=citation&scisdr=CgUMgfDyGAA:AAGBfm0AAAAAXyq_gtIpXIPHgJYzqTmvObsKm-d6XvKu&scisig=AAGBfm0AAAAAXyq_gjKszOfJrL0tPaFTfx7jH_m4Yl4O&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
59
-------------------------------------------------
2020-08-05 14:07:38
Got the results of the query
{'bib': {'abstract': 'We show that there may exist an inherent tension between '
                     'the goal of adversarial robustness and that of standard '
                     'generalization. Specifically, training robust models may '
                     'not only be more resource-consuming, but also lead to a '
                     'reduction of standard accuracy. We demonstrate that this '
                     'trade-off between the standard accuracy of a model and '
                     'its robustness to adversarial perturbations provably '
                     'exists in a fairly simple and natural setting. These '
                     'findings also corroborate a similar phenomenon observed '
                     'empirically in more complex',
         'author': ['D Tsipras', 'S Santurkar', 'L Engstrom', 'A Turner'],
         'cites': '301',
         'eprint': 'https://arxiv.org/pdf/1805.12152.pdf,',
         'gsrank': '1',
         'title': 'Robustness may be at odds with accuracy',
         'url': 'https://arxiv.org/abs/1805.12152',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=5850945088404252192&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRobustness%2BMay%2BBe%2Bat%2BOdds%2Bwith%2BAccuracy%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=IOam6o67MlEJ&ei=L70qX4r0MIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:IOam6o67MlEJ:scholar.google.com/&output=citation&scisdr=CgUMgfDyGAA:AAGBfm0AAAAAXyq_jnFzRlmE6dPmRCjkMl3_ZGq7cRyE&scisig=AAGBfm0AAAAAXyq_jtkXu2-NVWVHhj5Oxwb8j5VDtmYW&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
301
-------------------------------------------------
2020-08-05 14:07:50
Got the results of the query
{'bib': {'abstract': 'A well-trained model should classify objects with a '
                     'unanimous score for every category. This requires the '
                     'high-level semantic features should be as much alike as '
                     'possible among samples. To achive this, previous works '
                     'focus on re-designing the loss or proposing new '
                     'regularization constraints. In this paper, we provide a '
                     'new perspective. For each category, it is assumed that '
                     'there are two feature sets: one with reliable '
                     'information and the other with less reliable source. We '
                     'argue that the reliable set could guide the feature '
                     'learning of the',
         'author': ['H Li', 'B Dai', 'S Shi', 'W Ouyang', 'X Wang'],
         'cites': '5',
         'eprint': 'https://arxiv.org/pdf/1903.11851',
         'gsrank': '1',
         'title': 'Feature intertwiner for object detection',
         'url': 'https://arxiv.org/abs/1903.11851',
         'venue': 'arXiv preprint arXiv:1903.11851',
         'year': '2019'},
 'citations_link': '/scholar?cites=1331733591833237522&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFeature%2BIntertwiner%2Bfor%2BObject%2BDetection%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=EtisXKVEexIJ&ei=Or0qX6SxPKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:EtisXKVEexIJ:scholar.google.com/&output=citation&scisdr=CgUMgfDyGAA:AAGBfm0AAAAAXyq_lhefVCRPPn3N5D6oMby0PZn4AELm&scisig=AAGBfm0AAAAAXyq_lv595IJHpfxG653tMdoRkDULsE4D&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 14:07:59
Got the results of the query
{'bib': {'abstract': 'Deep neural networks are susceptible to\\emph '
                     '{adversarial} attacks. In computer vision, well-crafted '
                     'perturbations to images can cause neural networks to '
                     'make mistakes such as confusing a cat with a computer. '
                     'Previous adversarial attacks have been designed to '
                     'degrade performance of models or cause machine learning '
                     'models to produce specific outputs chosen ahead of time '
                     'by the attacker. We introduce attacks that instead {\\em '
                     'reprogram} the target model to perform a task chosen by '
                     'the attacker---without the attacker',
         'author': ['GF Elsayed', 'I Goodfellow', 'J Sohl-Dickstein'],
         'cites': '22',
         'eprint': 'https://arxiv.org/pdf/1806.11146.pdf?source=post_page---------------------------',
         'gsrank': '1',
         'title': 'Adversarial reprogramming of neural networks',
         'url': 'https://arxiv.org/abs/1806.11146',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=14670912168580243999&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BReprogramming%2Bof%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=H2IuM-OMmcsJ&ei=Qr0qX56PG5qGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:H2IuM-OMmcsJ:scholar.google.com/&output=citation&scisdr=CgUMgfDyGAA:AAGBfm0AAAAAXyq_ndWcHLL_vMMWvRHMztYPxB-VvDIp&scisig=AAGBfm0AAAAAXyq_nVeX283dHT-410cZnBPzYnvHL1UR&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
22
-------------------------------------------------
2020-08-05 14:08:05
Got the results of the query
{'bib': {'abstract': 'It is well known that neural networks with rectified '
                     'linear units (ReLU) activation functions are positively '
                     'scale-invariant. Conventional algorithms like stochastic '
                     'gradient descent optimize the neural networks in the '
                     'vector space of weights, which is, however, not '
                     'positively scale-invariant. This mismatch may lead to '
                     'problems during the optimization process. Then, a '
                     'natural question is:\\emph {can we construct a new '
                     'vector space that is positively scale-invariant and '
                     'sufficient to represent ReLU neural networks so as to '
                     'better facilitate the',
         'author': ['Q Meng', 'S Zheng', 'H Zhang', 'W Chen', 'ZM Ma'],
         'cites': '9',
         'eprint': 'https://arxiv.org/pdf/1802.03713',
         'gsrank': '1',
         'title': '-SGD: Optimizing ReLU Neural Networks in its Positively '
                  'Scale-Invariant Space',
         'url': 'https://arxiv.org/abs/1802.03713',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=17842882787808230795&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DG-SGD:%2BOptimizing%2BReLU%2BNeural%2BNetworks%2Bin%2Bits%2BPositively%2BScale-Invariant%2BSpace%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=i709IFynnvcJ&ei=Sr0qX7_kJpqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:i709IFynnvcJ:scholar.google.com/&output=citation&scisdr=CgUMgfDyGAA:AAGBfm0AAAAAXyq_pgHpkQxHfX-DkXV2ez5LGeLLNPhN&scisig=AAGBfm0AAAAAXyq_plFTSPOQvIPTnVxBv2UqCl8yOi41&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 14:08:14
Got the results of the query
{'bib': {'abstract': 'Nonlinearity is crucial to the performance of a deep '
                     '(neural) network (DN). To date there has been little '
                     'progress understanding the menagerie of available '
                     'nonlinearities, but recently progress has been made on '
                     'understanding the rôle played by piecewise affine and '
                     'convex nonlinearities like the ReLU and absolute value '
                     'activation functions and max-pooling. In particular, DN '
                     'layers constructed from these operations can be '
                     'interpreted as {\\em max-affine spline operators}(MASOs) '
                     'that have an elegant link to vector quantization (VQ) '
                     'and',
         'author': ['R Balestriero', 'RG Baraniuk'],
         'cites': '2',
         'eprint': 'https://arxiv.org/pdf/1810.09274',
         'gsrank': '1',
         'title': 'From Hard to Soft: Understanding Deep Network '
                  'Nonlinearities via Vector Quantization and Statistical '
                  'Inference',
         'url': 'https://arxiv.org/abs/1810.09274',
         'venue': 'arXiv preprint arXiv:1810.09274',
         'year': '2018'},
 'citations_link': '/scholar?cites=6884032427867360186&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFrom%2BHard%2Bto%2BSoft:%2BUnderstanding%2BDeep%2BNetwork%2BNonlinearities%2Bvia%2BVector%2BQuantization%2Band%2BStatistical%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ustNHQX_iF8J&ei=Ub0qX_7YJojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ustNHQX_iF8J:scholar.google.com/&output=citation&scisdr=CgUMgfDyGAA:AAGBfm0AAAAAXyq_sJMIx73LBy_Sz1F_IPtR6arcGotB&scisig=AAGBfm0AAAAAXyq_sEDSFiH-C9QqPNCJY6jJadcfgk7S&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
2
-------------------------------------------------
2020-08-05 14:08:24
Got the results of the query
{'bib': {'abstract': 'Momentum is a simple and widely used trick which allows '
                     'gradient-based optimizers to pick up speed along low '
                     'curvature directions. Its performance depends crucially '
                     'on a damping coefficient $\\beta $. Large $\\beta $ '
                     'values can potentially deliver much larger speedups, but '
                     'are prone to oscillations and instability; hence one '
                     'typically resorts to small values such as 0.5 or 0.9. We '
                     'propose Aggregated Momentum (AggMo), a variant of '
                     'momentum which combines multiple velocity vectors with '
                     'different $\\beta $ parameters. AggMo is trivial to',
         'author': ['J Lucas', 'S Sun', 'R Zemel', 'R Grosse'],
         'cites': '19',
         'eprint': 'https://arxiv.org/pdf/1804.00325',
         'gsrank': '1',
         'title': 'Aggregated momentum: Stability through passive damping',
         'url': 'https://arxiv.org/abs/1804.00325',
         'venue': 'arXiv preprint arXiv:1804.00325',
         'year': '2018'},
 'citations_link': '/scholar?cites=3877232077315711794&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAggregated%2BMomentum:%2BStability%2BThrough%2BPassive%2BDamping%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=MiMnKAiyzjUJ&ei=XL0qX-SkD6iBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:MiMnKAiyzjUJ:scholar.google.com/&output=citation&scisdr=CgUMgfDyGAA:AAGBfm0AAAAAXyq_uEhjgJoDnmOIxPv42Q9NqWZcGsr8&scisig=AAGBfm0AAAAAXyq_uN5XhXdCd3AsfSm4Hq8q7hnJqMG4&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
19
-------------------------------------------------
2020-08-05 14:08:32
Got the results of the query
{'bib': {'abstract': 'We propose a single neural probabilistic model based on '
                     'variational autoencoder that can be conditioned on an '
                     'arbitrary subset of observed features and then sample '
                     'the remaining features in" one shot". The features may '
                     'be both real-valued and categorical. Training of the '
                     'model is performed by stochastic variational Bayes. The '
                     'experimental evaluation on synthetic data, as well as '
                     'feature imputation and image inpainting problems, shows '
                     'the effectiveness of the proposed approach and diversity '
                     'of the generated samples.',
         'author': ['O Ivanov', 'M Figurnov', 'D Vetrov'],
         'cites': '30',
         'eprint': 'https://arxiv.org/pdf/1806.02382',
         'gsrank': '1',
         'title': 'Variational autoencoder with arbitrary conditioning',
         'url': 'https://arxiv.org/abs/1806.02382',
         'venue': 'arXiv preprint arXiv:1806.02382',
         'year': '2018'},
 'citations_link': '/scholar?cites=10062724307854177274&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BAutoencoder%2Bwith%2BArbitrary%2BConditioning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=-jNWgnH6pYsJ&ei=Zb0qX4fVF4jHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:-jNWgnH6pYsJ:scholar.google.com/&output=citation&scisdr=CgUMgfDyGAA:AAGBfm0AAAAAXyq_wtvpt564B_HZlON2LxIFpLlYp1qX&scisig=AAGBfm0AAAAAXyq_wh3O2np5euqP-N6aBgHNFLJAWo-c&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
30
-------------------------------------------------
2020-08-05 14:08:42
Got the results of the query
{'bib': {'abstract': 'Prediction is arguably one of the most basic functions '
                     'of an intelligent system. In general, the problem of '
                     'predicting events in the future or between two waypoints '
                     'is exceedingly difficult. However, most phenomena '
                     'naturally pass through relatively predictable '
                     'bottlenecks---while we cannot predict the precise '
                     'trajectory of a robot arm between being at rest and '
                     'holding an object up, we can be certain that it must '
                     'have picked the object up. To exploit this, we decouple '
                     'visual prediction from a rigid notion of time. While '
                     'conventional approaches',
         'author': ['D Jayaraman', 'F Ebert', 'AA Efros', 'S Levine'],
         'cites': '35',
         'eprint': 'https://arxiv.org/pdf/1808.07784',
         'gsrank': '1',
         'title': 'Time-agnostic prediction: Predicting predictable video '
                  'frames',
         'url': 'https://arxiv.org/abs/1808.07784',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=5221092832811225749&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTime-Agnostic%2BPrediction:%2BPredicting%2BPredictable%2BVideo%2BFrames%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=lRIN10QMdUgJ&ei=bb0qX86zNaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:lRIN10QMdUgJ:scholar.google.com/&output=citation&scisdr=CgUMgfDyGAA:AAGBfm0AAAAAXyq_zPbCR0j760xVEsza1rF-HWTqwynG&scisig=AAGBfm0AAAAAXyq_zAdnYvm5U_6GE2U5J6DMXngylZHQ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
35
-------------------------------------------------
2020-08-05 14:08:52
Got the results of the query
{'bib': {'abstract': 'The convergence rate and final performance of common '
                     'deep learning models have significantly benefited from '
                     'heuristics such as learning rate schedules, knowledge '
                     'distillation, skip connections, and normalization '
                     'layers. In the absence of theoretical underpinnings, '
                     'controlled experiments aimed at explaining these '
                     'strategies can aid our understanding of deep learning '
                     'landscapes and the training dynamics. Existing '
                     'approaches for empirical analysis rely on tools of '
                     'linear interpolation and visualizations with',
         'author': ['A Gotmare', 'NS Keskar', 'C Xiong', 'R Socher'],
         'cites': '28',
         'eprint': 'https://arxiv.org/pdf/1810.13243',
         'gsrank': '1',
         'title': 'A closer look at deep learning heuristics: Learning rate '
                  'restarts, warmup and distillation',
         'url': 'https://arxiv.org/abs/1810.13243',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8375178060138487801&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BCloser%2BLook%2Bat%2BDeep%2BLearning%2BHeuristics:%2BLearning%2Brate%2Brestarts,%2BWarmup%2Band%2BDistillation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=-R8zdRKcOnQJ&ei=eL0qX4OXG4vrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:-R8zdRKcOnQJ:scholar.google.com/&output=citation&scisdr=CgUMgfDyGAA:AAGBfm0AAAAAXyq_1OMQErKI5vTrvQq5KYEY_Q418qBN&scisig=AAGBfm0AAAAAXyq_1G0wgnFuw3InFLGoKkDl_qcdF9w7&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
28
-------------------------------------------------
2020-08-05 14:09:00
Got the results of the query
{'bib': {'abstract': 'The Vision-and-Language Navigation (VLN) task entails an '
                     'agent following navigational instruction in '
                     'photo-realistic unknown environments. This challenging '
                     'task demands that the agent be aware of which '
                     'instruction was completed, which instruction is needed '
                     'next, which way to go, and its navigation progress '
                     'towards the goal. In this paper, we introduce a '
                     'self-monitoring agent with two complementary '
                     'components:(1) visual-textual co-grounding module to '
                     'locate the instruction completed in the past, the '
                     'instruction required for the next',
         'author': ['CY Ma', 'J Lu', 'Z Wu', 'G AlRegib', 'Z Kira', 'R Socher'],
         'cites': '52',
         'eprint': 'https://arxiv.org/pdf/1901.03035',
         'gsrank': '1',
         'title': 'Self-monitoring navigation agent via auxiliary progress '
                  'estimation',
         'url': 'https://arxiv.org/abs/1901.03035',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=5431855784757864150&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSelf-Monitoring%2BNavigation%2BAgent%2Bvia%2BAuxiliary%2BProgress%2BEstimation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=1qakmQ7UYUsJ&ei=gr0qX4-5H8KwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:1qakmQ7UYUsJ:scholar.google.com/&output=citation&scisdr=CgUMgfDyGAA:AAGBfm0AAAAAXyq_3zMu5rFFRgL7RzynLFbs-TouJCPQ&scisig=AAGBfm0AAAAAXyq_3yXBi65763zkyqHIQFz5r3c7cLWk&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
52
-------------------------------------------------
2020-08-05 14:09:11
Got the results of the query
{'bib': {'abstract': 'Detecting the emergence of abrupt property changes in '
                     'time series is a challenging problem. Kernel two-sample '
                     'test has been studied for this task which makes fewer '
                     'assumptions on the distributions than traditional '
                     'parametric approaches. However, selecting kernels is '
                     'non-trivial in practice. Although kernel selection for '
                     'two-sample test has been studied, the insufficient '
                     'samples in change point detection problem hinder the '
                     'success of those developed kernel selection algorithms. '
                     'In this paper, we propose KL-CPD, a novel',
         'author': ['WC Chang', 'CL Li', 'Y Yang', 'B Póczos'],
         'cites': '12',
         'eprint': 'https://arxiv.org/pdf/1901.06077',
         'gsrank': '1',
         'title': 'Kernel change-point detection with auxiliary deep '
                  'generative models',
         'url': 'https://arxiv.org/abs/1901.06077',
         'venue': 'arXiv preprint arXiv:1901.06077',
         'year': '2019'},
 'citations_link': '/scholar?cites=15362141737124631231&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKernel%2BChange-point%2BDetection%2Bwith%2BAuxiliary%2BDeep%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=vxKEjoRKMdUJ&ei=i70qX_21EIvrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:vxKEjoRKMdUJ:scholar.google.com/&output=citation&scisdr=CgUMgfDyGAA:AAGBfm0AAAAAXyq_6ZHaf8VHGp-8TrwSXFDuM5EPRVxx&scisig=AAGBfm0AAAAAXyq_6WLjDFPUbCyBAZuqzKUfyu6QfPJM&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 14:09:21
Got the results of the query
{'bib': {'abstract': 'A central goal of unsupervised learning is to acquire '
                     'representations from unlabeled data or experience that '
                     'can be used for more effective learning of downstream '
                     'tasks from modest amounts of labeled data. Many prior '
                     'unsupervised learning works aim to do so by developing '
                     'proxy objectives based on reconstruction, '
                     'disentanglement, prediction, and other metrics. Instead, '
                     'we develop an unsupervised meta-learning method that '
                     'explicitly optimizes for the ability to learn a variety '
                     'of tasks from small amounts of data. To do so, we',
         'author': ['K Hsu', 'S Levine', 'C Finn'],
         'cites': '55',
         'eprint': 'https://arxiv.org/pdf/1810.02334',
         'gsrank': '1',
         'title': 'Unsupervised learning via meta-learning',
         'url': 'https://arxiv.org/abs/1810.02334',
         'venue': 'arXiv preprint arXiv:1810.02334',
         'year': '2018'},
 'citations_link': '/scholar?cites=52752672237685597&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BLearning%2Bvia%2BMeta-Learning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=XQuuoUZquwAJ&ei=lL0qX8i2CsKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:XQuuoUZquwAJ:scholar.google.com/&output=citation&scisdr=CgUMgfDyGAA:AAGBfm0AAAAAXyq_8McdlKdCfE84lbKU2ulfkkDWBt2d&scisig=AAGBfm0AAAAAXyq_8BquYqRcQEjLylAIbMYAC7Mt9d89&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
55
-------------------------------------------------
2020-08-05 14:09:28
Got the results of the query
{'bib': {'abstract': 'We introduce Auxiliary Variational MCMC, a novel '
                     'framework for learning MCMC kernels that combines recent '
                     'advances in variational inference with insights drawn '
                     'from traditional auxiliary variable MCMC methods such as '
                     'Hamiltonian Monte Carlo. Our framework exploits',
         'author': ['R Habib', 'D Barber'],
         'cites': '5',
         'eprint': 'https://openreview.net/pdf?id=r1NJqsRctX',
         'gsrank': '1',
         'title': 'Auxiliary variational mcmc',
         'url': 'https://openreview.net/forum?id=r1NJqsRctX',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=7462576419802209211&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAuxiliary%2BVariational%2BMCMC%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=u1uEI7dlkGcJ&ei=nL0qX82XMKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:u1uEI7dlkGcJ:scholar.google.com/&output=citation&scisdr=CgUMgfDyGAA:AAGBfm0AAAAAXyq_-5mmhJBwTo_n_t-KjGWJR1OWd2zs&scisig=AAGBfm0AAAAAXyq_-43ZNe4ChRAG2NktwnsWZ2Ng-XYH&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 14:09:39
Got the results of the query
{'bib': {'abstract': 'Deep neural networks work well at approximating '
                     'complicated functions when provided with data and '
                     'trained by gradient descent methods. At the same time, '
                     'there is a vast amount of existing functions that '
                     'programmatically solve different tasks in a precise '
                     'manner eliminating the need for training. In many cases, '
                     'it is possible to decompose a task to a series of '
                     'functions, of which for some we may prefer to use a '
                     'neural network to learn the functionality, while for '
                     'others the preferred method would be to use existing '
                     'black-box functions. We',
         'author': ['A Jacovi', 'G Hadash', 'E Kermany', 'B Carmeli'],
         'cites': '7',
         'eprint': 'https://arxiv.org/pdf/1901.03995',
         'gsrank': '1',
         'title': 'Neural network gradient-based learning of black-box '
                  'function interfaces',
         'url': 'https://arxiv.org/abs/1901.03995',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=103487598035964441&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2Bnetwork%2Bgradient-based%2Blearning%2Bof%2Bblack-box%2Bfunction%2Binterfaces%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=GZKXD2ypbwEJ&ei=qb0qX_OCA6iBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:GZKXD2ypbwEJ:scholar.google.com/&output=citation&scisdr=CgUMgfDyGAA:AAGBfm0AAAAAXyrABqw7Lg8pdqno2O7j2gllCrdw85Jv&scisig=AAGBfm0AAAAAXyrABrlZMppB6w2VqWKJzIvtZMkaT2uP&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
7
-------------------------------------------------
2020-08-05 14:09:50
Trying new proxy
Working proxy: http://141.255.157.242:8080
Trying new proxy
Working proxy: http://84.210.183.61:3128
Got the results of the query
{'bib': {'abstract': 'Hyperparameter optimization can be formulated as a '
                     'bilevel optimization problem, where the optimal '
                     'parameters on the training set depend on the '
                     'hyperparameters. We aim to adapt regularization '
                     'hyperparameters for neural networks by fitting compact '
                     'approximations to the best-response function, which maps '
                     'hyperparameters to optimal weights and biases. We show '
                     'how to construct scalable best-response approximations '
                     'for neural networks by modeling the best-response as a '
                     'single network whose hidden units are gated '
                     'conditionally',
         'author': ['M MacKay', 'P Vicol', 'J Lorraine', 'D Duvenaud'],
         'cites': '18',
         'eprint': 'https://arxiv.org/pdf/1903.03088',
         'gsrank': '1',
         'title': 'Self-tuning networks: Bilevel optimization of '
                  'hyperparameters using structured best-response functions',
         'url': 'https://arxiv.org/abs/1903.03088',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=13746959771027006799&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSelf-Tuning%2BNetworks:%2BBilevel%2BOptimization%2Bof%2BHyperparameters%2Busing%2BStructured%2BBest-Response%2BFunctions%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=T2WmxRMDx74J&ei=670qX-aPLaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:T2WmxRMDx74J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrAScufv7DztV0BiCjNR6wrtTMJebJr&scisig=AAGBfm0AAAAAXyrASXBQkpHankJzZKM-2By-MADPe5dk&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
18
-------------------------------------------------
2020-08-05 14:10:57
Got the results of the query
{'bib': {'abstract': 'Learning to control an environment without hand-crafted '
                     'rewards or expert data remains challenging and is at the '
                     'frontier of reinforcement learning research. We present '
                     'an unsupervised learning algorithm to train agents to '
                     'achieve perceptually-specified goals using only a stream '
                     'of observations and actions. Our agent simultaneously '
                     'learns a goal-conditioned policy and a goal achievement '
                     'reward function that measures how similar a state is to '
                     'the goal state. This dual optimization leads to a '
                     'co-operative game, giving rise to a',
         'author': ['D Warde-Farley', 'T Van de Wiele', 'T Kulkarni'],
         'cites': '37',
         'eprint': 'https://arxiv.org/pdf/1811.11359',
         'gsrank': '1',
         'title': 'Unsupervised control through non-parametric discriminative '
                  'rewards',
         'url': 'https://arxiv.org/abs/1811.11359',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11006025124069493159&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BControl%2BThrough%2BNon-Parametric%2BDiscriminative%2BRewards%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=p8kvtYlBvZgJ&ei=9b0qX6qpGojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:p8kvtYlBvZgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrAUo0g76Z_q4ucJE3_kLLqqeiGSMl4&scisig=AAGBfm0AAAAAXyrAUr28YCL1O0dUAYFuIuz0GYxvdy91&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
37
-------------------------------------------------
2020-08-05 14:11:06
Got the results of the query
{'bib': {'abstract': 'In this paper, we present a new deep learning '
                     'architecture for addressing the problem of supervised '
                     'learning with sparse and irregularly sampled '
                     'multivariate time series. The architecture is based on '
                     'the use of a semi-parametric interpolation network '
                     'followed by the application of a prediction network. The '
                     'interpolation network allows for information to be '
                     'shared across multiple dimensions of a multivariate time '
                     'series during the interpolation stage, while any '
                     'standard deep learning model can be used for the '
                     'prediction network. This',
         'author': ['SN Shukla', 'BM Marlin'],
         'cites': '19',
         'eprint': 'https://arxiv.org/pdf/1909.07782',
         'gsrank': '1',
         'title': 'Interpolation-prediction networks for irregularly sampled '
                  'time series',
         'url': 'https://arxiv.org/abs/1909.07782',
         'venue': 'arXiv preprint arXiv:1909.07782',
         'year': '2019'},
 'citations_link': '/scholar?cites=15477406781147246766&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInterpolation-Prediction%2BNetworks%2Bfor%2BIrregularly%2BSampled%2BTime%2BSeries%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=rvCA8nbLytYJ&ei=_b0qX6a_N4yimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:rvCA8nbLytYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrAWb4VvaBfvycfoaVunqIfPA7gEZAd&scisig=AAGBfm0AAAAAXyrAWdq3VT_GqFguTFnrfp-PP3N5ALKd&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
19
-------------------------------------------------
2020-08-05 14:11:14
Got the results of the query
{'bib': {'abstract': 'Several first order stochastic optimization methods '
                     'commonly used in the Euclidean domain such as stochastic '
                     'gradient descent (SGD), accelerated gradient descent or '
                     'variance reduced methods have already been adapted to '
                     'certain Riemannian settings. However',
         'author': ['G Bécigneul', 'OE Ganea'],
         'cites': '41',
         'eprint': 'https://arxiv.org/pdf/1810.00760',
         'gsrank': '1',
         'title': 'Riemannian adaptive optimization methods',
         'url': 'https://arxiv.org/abs/1810.00760',
         'venue': 'arXiv preprint arXiv:1810.00760',
         'year': '2018'},
 'citations_link': '/scholar?cites=1396968712065287327&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRiemannian%2BAdaptive%2BOptimization%2BMethods%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=n4T8ZKQHYxMJ&ei=B74qX6iTIIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:n4T8ZKQHYxMJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrAZFDLQBgzsFEDU9hROQtVfJ-xVgyR&scisig=AAGBfm0AAAAAXyrAZPp1Nh4YJsceVVdf83OzDDXrAIDK&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
41
-------------------------------------------------
2020-08-05 14:11:24
Got the results of the query
{'bib': {'abstract': 'While deep neural networks are a highly successful model '
                     'class, their large memory footprint puts considerable '
                     'strain on energy consumption, communication bandwidth, '
                     'and storage requirements. Consequently, model size '
                     'reduction has become an utmost goal in deep learning. A '
                     'typical approach is to train a set of deterministic '
                     'weights, while applying certain techniques such as '
                     'pruning and quantization, in order that the empirical '
                     'weight distribution becomes amenable to Shannon-style '
                     'coding schemes. However, as shown in',
         'author': ['M Havasi', 'R Peharz', 'JM Hernández-Lobato'],
         'cites': '6',
         'eprint': 'https://arxiv.org/pdf/1810.00440',
         'gsrank': '1',
         'title': 'Minimal random code learning: Getting bits back from '
                  'compressed model parameters',
         'url': 'https://arxiv.org/abs/1810.00440',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=17962712491875468296&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMinimal%2BRandom%2BCode%2BLearning:%2BGetting%2BBits%2BBack%2Bfrom%2BCompressed%2BModel%2BParameters%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=CIS1QNdfSPkJ&ei=Er4qX6bLJ7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:CIS1QNdfSPkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrAbtqHnjjKKnVHFrcCymQSB6wLA20E&scisig=AAGBfm0AAAAAXyrAbsSrXS6CYHHKB7-AWRBrbzsqBp41&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 14:11:35
Got the results of the query
{'bib': {'abstract': 'Recent studies have highlighted adversarial examples as '
                     'a ubiquitous threat to different neural network models '
                     'and many downstream applications. Nonetheless, as unique '
                     'data properties have inspired distinct and powerful '
                     'learning principles, this paper aims to explore their '
                     'potentials towards mitigating adversarial inputs. In '
                     'particular, our results reveal the importance of using '
                     'the temporal dependency in audio data to gain '
                     'discriminate power against adversarial examples. Tested '
                     'on the automatic speech recognition (ASR) tasks and',
         'author': ['Z Yang', 'B Li', 'PY Chen', 'D Song'],
         'cites': '36',
         'eprint': 'https://arxiv.org/pdf/1809.10875',
         'gsrank': '1',
         'title': 'Characterizing audio adversarial examples using temporal '
                  'dependency',
         'url': 'https://arxiv.org/abs/1809.10875',
         'venue': 'arXiv preprint arXiv:1809.10875',
         'year': '2018'},
 'citations_link': '/scholar?cites=13178017338053441718&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCharacterizing%2BAudio%2BAdversarial%2BExamples%2BUsing%2BTemporal%2BDependency%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=tjid5vK44bYJ&ei=G74qX4n8JojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:tjid5vK44bYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrAeQ9bW1V_O3VvKg7igdZyt4PSK2q8&scisig=AAGBfm0AAAAAXyrAea4I1EsU0ZbRrH327nl-F_PGQQtT&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
36
-------------------------------------------------
2020-08-05 14:11:45
Got the results of the query
{'bib': {'abstract': 'Modern neural networks are over-parametrized. In '
                     'particular, each rectified linear hidden unit can be '
                     'modified by a multiplicative factor by adjusting input '
                     'and output weights, without changing the rest of the '
                     'network. Inspired by the Sinkhorn-Knopp algorithm, we '
                     'introduce a fast iterative method for minimizing the L2 '
                     'norm of the weights, equivalently the weight decay '
                     'regularizer. It provably converges to a unique solution. '
                     'Interleaving our algorithm with SGD during training '
                     'improves the test accuracy. For small batches, our '
                     'approach offers an',
         'author': ['P Stock', 'B Graham', 'R Gribonval', 'H Jégou'],
         'cites': '3',
         'eprint': 'https://arxiv.org/pdf/1902.10416',
         'gsrank': '1',
         'title': 'Equi-normalization of neural networks',
         'url': 'https://arxiv.org/abs/1902.10416',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=7559448072406680309&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEqui-normalization%2Bof%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=9cLPXfiN6GgJ&ei=Jb4qX7aXIYjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:9cLPXfiN6GgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrAgGddpjFTUczLeum6jNtiTiLDyp8w&scisig=AAGBfm0AAAAAXyrAgO66DDjI1HjTR80DRCY5CuhTgQ4y&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
3
-------------------------------------------------
2020-08-05 14:11:52
Got the results of the query
{'bib': {'abstract': 'Recurrent Neural Networks (RNNs) are very successful at '
                     'solving challenging problems with sequential data. '
                     'However, this observed efficiency is not yet entirely '
                     'explained by theory. It is known that a certain class of '
                     'multiplicative RNNs enjoys the property of depth '
                     'efficiency---a shallow network of exponentially large '
                     'width is necessary to realize the same score function as '
                     'computed by such an RNN. Such networks, however, are not '
                     'very often applied to real life tasks. In this work, we '
                     'attempt to reduce the gap between theory and practice by',
         'author': ['V Khrulkov', 'O Hrinchuk', 'I Oseledets'],
         'cites': '6',
         'eprint': 'https://arxiv.org/pdf/1901.10801',
         'gsrank': '1',
         'title': 'Generalized tensor models for recurrent neural networks',
         'url': 'https://arxiv.org/abs/1901.10801',
         'venue': 'arXiv preprint arXiv:1901.10801',
         'year': '2019'},
 'citations_link': '/scholar?cites=16851245394902842047&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeneralized%2BTensor%2BModels%2Bfor%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=v_oeqmem2-kJ&ei=Lb4qX76XDKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:v_oeqmem2-kJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrAjbNy89sRCi6PVP9jDn1lCRmaYMEM&scisig=AAGBfm0AAAAAXyrAjdgjshbQxs5QmVU_Id_uAuEFBm6E&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 14:12:05
Got the results of the query
{'bib': {'abstract': 'In open-domain dialogue intelligent agents should '
                     'exhibit the use of knowledge, however there are few '
                     'convincing demonstrations of this to date. The most '
                     'popular sequence to sequence models typically" generate '
                     'and hope" generic utterances that can be memorized in '
                     'the weights of the model when mapping from input '
                     'utterance (s) to output, rather than employing recalled '
                     'knowledge as context. Use of knowledge has so far proved '
                     'difficult, in part because of the lack of a supervised '
                     'learning benchmark task which exhibits',
         'author': ['E Dinan', 'S Roller', 'K Shuster', 'A Fan', 'M Auli'],
         'cites': '105',
         'eprint': 'https://arxiv.org/pdf/1811.01241.pdf?fbclid=IwAR1pKzukQ8jXk48Jbe-x4HzC4je0Pboo6Dew-CMEP7NF4C6gdGnc_yc6Xts',
         'gsrank': '1',
         'title': 'Wizard of wikipedia: Knowledge-powered conversational '
                  'agents',
         'url': 'https://arxiv.org/abs/1811.01241',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=10143779580305681961&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWizard%2Bof%2BWikipedia:%2BKnowledge-Powered%2BConversational%2BAgents%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=KS77w8jxxYwJ&ei=O74qX6ClHYjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:KS77w8jxxYwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrAl6YWsCX6xLFwN06w6BW_AOafI56G&scisig=AAGBfm0AAAAAXyrAl2xxt8sLDyYXfA0dTyjDxyxMuRLO&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
105
-------------------------------------------------
2020-08-05 14:12:15
Got the results of the query
{'bib': {'abstract': 'A wide range of defenses have been proposed to harden '
                     'neural networks against adversarial attacks. However, a '
                     'pattern has emerged in which the majority of adversarial '
                     'defenses are quickly broken by new attacks. Given the '
                     'lack of success at generating robust',
         'author': ['A Shafahi', 'WR Huang', 'C Studer', 'S Feizi'],
         'cites': '99',
         'eprint': 'https://arxiv.org/pdf/1809.02104',
         'gsrank': '1',
         'title': 'Are adversarial examples inevitable?',
         'url': 'https://arxiv.org/abs/1809.02104',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1886933684850079420&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAre%2Badversarial%2Bexamples%2Binevitable%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=vDY1YDC8LxoJ&ei=Qr4qX4S5C5qGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:vDY1YDC8LxoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrAnXUuuYDr_gshhySRX_S623nIMorx&scisig=AAGBfm0AAAAAXyrAnUzs2V5xcPueLIzVnmbNgqBl_70v&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
99
-------------------------------------------------
2020-08-05 14:12:21
Got the results of the query
{'bib': {'abstract': 'Generative adversarial networks (GANs) form a generative '
                     'modeling approach known for producing appealing samples, '
                     'but they are notably difficult to train. One common way '
                     'to tackle this issue has been to propose new '
                     'formulations of the GAN objective. Yet, surprisingly few '
                     'studies have looked at optimization methods designed for '
                     'this adversarial training. In this work, we cast GAN '
                     'optimization problems in the general variational '
                     'inequality framework. Tapping into the mathematical '
                     'programming literature, we counter',
         'author': ['G Gidel', 'H Berard', 'G Vignoud', 'P Vincent'],
         'cites': '94',
         'eprint': 'https://arxiv.org/pdf/1802.10551',
         'gsrank': '1',
         'title': 'A variational inequality perspective on generative '
                  'adversarial networks',
         'url': 'https://arxiv.org/abs/1802.10551',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=6445881932716952872&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BVariational%2BInequality%2BPerspective%2Bon%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=KM2zT3dfdFkJ&ei=Sr4qX8TvBYyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:KM2zT3dfdFkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrApNdNDP2HtKC1DpctyOboRAMgN22p&scisig=AAGBfm0AAAAAXyrApB1HDVlYeKUGYtRi_9RktVty0FHR&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
94
-------------------------------------------------
2020-08-05 14:12:28
Got the results of the query
{'bib': {'abstract': 'Estimating the frequencies of elements in a data stream '
                     'is a fundamental task in data analysis and machine '
                     'learning. The problem is typically addressed using '
                     'streaming algorithms which can process very large data '
                     "using limited storage. Today's streaming algorithms, "
                     'however, cannot exploit patterns in their input to '
                     'improve performance. We propose a new class of '
                     'algorithms that automatically learn relevant patterns in '
                     'the input data and use them to improve its frequency '
                     'estimates. The proposed algorithms combine the',
         'author': ['CY Hsu', 'P Indyk', 'D Katabi', 'A Vakilian'],
         'cites': '25',
         'eprint': 'https://openreview.net/pdf?id=r1lohoCqY7',
         'gsrank': '1',
         'title': 'Learning-based frequency estimation algorithms',
         'url': 'https://openreview.net/forum?id=r1lohoCqY7&noteId=Skl8UhSqaX',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8454116097434845567&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning-Based%2BFrequency%2BEstimation%2BAlgorithms%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=fwGEfcwNU3UJ&ei=T74qX_vDBovrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:fwGEfcwNU3UJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrAq0NaBCUCIDVEFev43DIvwfr1-off&scisig=AAGBfm0AAAAAXyrAqzm0t5jVb09KFdWa8a4BtaVHTT2G&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
25
-------------------------------------------------
2020-08-05 14:12:35
Got the results of the query
{'bib': {'abstract': 'Reinforcement learning is a promising framework for '
                     'solving control problems, but its use in practical '
                     'situations is hampered by the fact that reward functions '
                     'are often difficult to engineer. Specifying goals and '
                     'tasks for autonomous machines, such as robots, is a '
                     'significant challenge: conventionally, reward functions '
                     'and goal states have been used to communicate '
                     'objectives. But people can communicate objectives to '
                     'each other simply by describing or demonstrating them. '
                     'How can we build learning algorithms that will allow us '
                     'to',
         'author': ['J Fu', 'A Korattikara', 'S Levine', 'S Guadarrama'],
         'cites': '39',
         'eprint': 'https://arxiv.org/pdf/1902.07742',
         'gsrank': '1',
         'title': 'From language to goals: Inverse reinforcement learning for '
                  'vision-based instruction following',
         'url': 'https://arxiv.org/abs/1902.07742',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=9128320307925997063&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFrom%2BLanguage%2Bto%2BGoals:%2BInverse%2BReinforcement%2BLearning%2Bfor%2BVision-Based%2BInstruction%2BFollowing%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=B9bpxfROrn4J&ei=WL4qX_n5BrGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:B9bpxfROrn4J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrAtoCv-xDTq8ICZ6yXRkRvcw8dfW3f&scisig=AAGBfm0AAAAAXyrAtjew9GiK9sjephFVq40mtnMU7BAB&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
39
-------------------------------------------------
2020-08-05 14:12:46
Got the results of the query
{'bib': {'abstract': 'The impressive lifelong learning in animal brains is '
                     'primarily enabled by plastic changes in synaptic '
                     'connectivity. Importantly, these changes are not '
                     'passive, but are actively controlled by neuromodulation, '
                     'which is itself under the control of the brain. The '
                     'resulting self-modifying abilities of the brain play an '
                     'important role in learning and adaptation, and are a '
                     'major basis for biological reinforcement learning. Here '
                     'we show for the first time that artificial neural '
                     'networks with such neuromodulated plasticity can be '
                     'trained with gradient descent',
         'author': ['T Miconi', 'A Rawal', 'J Clune', 'KO Stanley'],
         'cites': '15',
         'eprint': 'https://arxiv.org/pdf/2002.10585',
         'gsrank': '1',
         'title': 'Backpropamine: training self-modifying neural networks with '
                  'differentiable neuromodulated plasticity',
         'url': 'https://arxiv.org/abs/2002.10585',
         'venue': 'arXiv preprint arXiv:2002.10585',
         'year': '2020'},
 'citations_link': '/scholar?cites=3700434839782890310&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBackpropamine:%2Btraining%2Bself-modifying%2Bneural%2Bnetworks%2Bwith%2Bdifferentiable%2Bneuromodulated%2Bplasticity%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Rhu9i-GVWjMJ&ei=Y74qX7vpJ8KwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Rhu9i-GVWjMJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrAwhLhf669yrCm4M3gJPBBmSleCUIj&scisig=AAGBfm0AAAAAXyrAwhmRmASIrewlk8-DYhvYJpW8PREm&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 14:12:58
Got the results of the query
{'bib': {'abstract': 'Building on the recent successes of distributed training '
                     'of RL agents, in this paper we investigate the training '
                     'of RNN-based RL agents from distributed prioritized '
                     'experience replay. We study the effects of parameter lag '
                     'resulting in representational drift and recurrent state '
                     'staleness and empirically derive an improved training '
                     'strategy. Using a single network architecture and fixed '
                     'set of hyper-parameters, the resulting agent, Recurrent '
                     'Replay Distributed DQN, quadruples the previous state of '
                     'the art on Atari-57, and matches the state',
         'author': ['S Kapturowski', 'G Ostrovski', 'J Quan'],
         'cites': '65',
         'eprint': 'https://openreview.net/pdf?id=r1lyTjAqYX',
         'gsrank': '1',
         'title': 'Recurrent experience replay in distributed reinforcement '
                  'learning',
         'url': 'https://openreview.net/forum?id=r1lyTjAqYX&noteId=r1lyTjAqYX',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=9232121321169897083&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRecurrent%2BExperience%2BReplay%2Bin%2BDistributed%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=e0akf20VH4AJ&ei=cL4qX9bcLpqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:e0akf20VH4AJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrAzjm7f-GuKYSyNiqGo5pfejQOAPBs&scisig=AAGBfm0AAAAAXyrAzjdaxq3t--TM5OiS4eJQQLZLGAgy&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
65
-------------------------------------------------
2020-08-05 14:13:10
Got the results of the query
{'bib': {'abstract': 'Chemical reactions can be described as the stepwise '
                     'redistribution of electrons in molecules. As such, '
                     "reactions are often depicted usingarrow-pushing'diagrams "
                     'which show this movement as a sequence of arrows. We '
                     'propose an electron path prediction model (ELECTRO) to '
                     'learn these sequences directly from raw reaction data. '
                     'Instead of predicting product molecules directly from '
                     'reactant molecules in one shot, learning a model of '
                     'electron movement has the benefits of (a) being easy for '
                     'chemists to interpret,(b) incorporating',
         'author': ['J Bradshaw', 'MJ Kusner', 'B Paige', 'MHS Segler'],
         'cites': '14',
         'eprint': 'https://arxiv.org/pdf/1805.10970',
         'gsrank': '1',
         'title': 'A generative model for electron paths',
         'url': 'https://arxiv.org/abs/1805.10970',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=16966077960923717171&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BGenerative%2BModel%2BFor%2BElectron%2BPaths%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Mx5x7AOec-sJ&ei=fL4qX43EIZqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Mx5x7AOec-sJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrA2K5_vF2BYu3SubuDrSi1sTEElfoT&scisig=AAGBfm0AAAAAXyrA2I3qsKHhu9O-H9eLy76cEvDJMipT&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
14
-------------------------------------------------
2020-08-05 14:13:20
Got the results of the query
{'bib': {'abstract': 'Instance embeddings are an efficient and versatile image '
                     'representation that facilitates applications like '
                     'recognition, verification, retrieval, and clustering. '
                     'Many metric learning methods represent the input as a '
                     'single point in the embedding space. Often the distance '
                     'between points is used as a proxy for match confidence. '
                     'However, this can fail to represent uncertainty arising '
                     'when the input is ambiguous, eg, due to occlusion or '
                     'blurriness. This work addresses this issue and '
                     'explicitly models the uncertainty by hedging the '
                     'location of',
         'author': ['SJ Oh', 'K Murphy', 'J Pan', 'J Roth', 'F Schroff'],
         'cites': '12',
         'eprint': 'https://arxiv.org/pdf/1810.00319',
         'gsrank': '1',
         'title': 'Modeling uncertainty with hedged instance embedding',
         'url': 'https://arxiv.org/abs/1810.00319',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15448440055420606601&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModeling%2BUncertainty%2Bwith%2BHedged%2BInstance%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=idhCjWHiY9YJ&ei=g74qX6aNCsKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:idhCjWHiY9YJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrA3svLoEa1MjdcLJSdoSCIV38VMwgm&scisig=AAGBfm0AAAAAXyrA3hAJTXm_rZgcM5qISxcZ7sz72vo_&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 14:13:26
Got the results of the query
{'bib': {'abstract': 'The conventional solution to the recommendation problem '
                     'greedily ranks individual document candidates by '
                     'prediction scores. However, this method fails to '
                     'optimize the slate as a whole, and hence, often '
                     'struggles to capture biases caused by the page layout '
                     'and document interdepedencies. The slate recommendation '
                     'problem aims to directly find the optimally ordered '
                     "subset of documents (ie slates) that best serve users' "
                     'interests. Solving this problem is hard due to the '
                     'combinatorial explosion in all combinations of document',
         'author': ['R Jiang', 'S Gowal', 'TA Mann', 'DJ Rezende'],
         'cites': '15',
         'eprint': 'https://arxiv.org/pdf/1803.01682',
         'gsrank': '1',
         'title': 'Beyond greedy ranking: Slate optimization via list-CVAE',
         'url': 'https://arxiv.org/abs/1803.01682',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8102727489122848940&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBeyond%2BGreedy%2BRanking:%2BSlate%2BOptimization%2Bvia%2BList-CVAE%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=rND1pLircnAJ&ei=ib4qX-KUIo-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:rND1pLircnAJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrA57r_e-cGOtcrgd4ZRTkDzpaxezFT&scisig=AAGBfm0AAAAAXyrA59mUStYxPjm07HR1OxAwiyKDJmxe&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 14:13:35
Got the results of the query
{'bib': {'abstract': 'We present a method that learns to integrate temporal '
                     'information, from a learned dynamics model, with '
                     'ambiguous visual information, from a learned vision '
                     'model, in the context of interacting agents. Our method '
                     'is based on a graph-structured variational recurrent '
                     'neural network (Graph-VRNN), which is trained end-to-end '
                     'to infer the current state of the (partially observed) '
                     'world, as well as to forecast future states. We show '
                     'that our method outperforms various baselines on two '
                     'sports datasets, one based on real basketball '
                     'trajectories, and one',
         'author': ['C Sun', 'P Karlsson', 'J Wu', 'JB Tenenbaum'],
         'cites': '26',
         'eprint': 'https://arxiv.org/pdf/1902.09641',
         'gsrank': '1',
         'title': 'Stochastic prediction of multi-agent interactions from '
                  'partial observations',
         'url': 'https://arxiv.org/abs/1902.09641',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=11679184736370359376&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStochastic%2BPrediction%2Bof%2BMulti-Agent%2BInteractions%2Bfrom%2BPartial%2BObservations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=UPyEhaPMFKIJ&ei=lL4qX8KmD4vrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:UPyEhaPMFKIJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrA7_1eJ34sKw6cyn-yPym1e3l7w8WL&scisig=AAGBfm0AAAAAXyrA7-Pyri8ZW9zbAnlDI8R7824OAKJD&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
26
-------------------------------------------------
2020-08-05 14:13:43
Got the results of the query
{'bib': {'abstract': 'In this paper, we introduce a system called GamePad that '
                     'can be used to explore the application of machine '
                     'learning methods to theorem proving in the Coq proof '
                     'assistant. Interactive theorem provers such as Coq '
                     'enable users to construct machine-checkable proofs in a '
                     'step-by-step manner. Hence, they provide an opportunity '
                     'to explore theorem proving with human supervision. We '
                     'use GamePad to synthesize proofs for a simple algebraic '
                     'rewrite problem and train baseline models for a '
                     'formalization of the Feit',
         'author': ['D Huang', 'P Dhariwal', 'D Song', 'I Sutskever'],
         'cites': '34',
         'eprint': 'https://arxiv.org/pdf/1806.00608',
         'gsrank': '1',
         'title': 'Gamepad: A learning environment for theorem proving',
         'url': 'https://arxiv.org/abs/1806.00608',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=10460600857870546205&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGamePad:%2BA%2BLearning%2BEnvironment%2Bfor%2BTheorem%2BProving%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=HTWFvg6FK5EJ&ei=nb4qX9K6CYjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:HTWFvg6FK5EJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrA-OoepDBGiutvAalGl641-fApuWRn&scisig=AAGBfm0AAAAAXyrA-HgpTrGNudCzxJSe2JUeh7GGp9_8&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
34
-------------------------------------------------
2020-08-05 14:13:53
Got the results of the query
{'bib': {'abstract': 'For natural language understanding (NLU) technology to '
                     'be maximally useful, both practically and as a '
                     'scientific object of study, it must be general: it must '
                     'be able to process language in a way that is not '
                     'exclusively tailored to any one specific task or '
                     'dataset. In pursuit of this objective, we introduce the '
                     'General Language Understanding Evaluation benchmark '
                     '(GLUE), a tool for evaluating and analyzing the '
                     'performance of models across a diverse range of existing '
                     'NLU tasks. GLUE is model-agnostic, but it incentivizes '
                     'sharing',
         'author': ['A Wang', 'A Singh', 'J Michael', 'F Hill', 'O Levy'],
         'cites': '696',
         'eprint': 'https://arxiv.org/pdf/1804.07461',
         'gsrank': '1',
         'title': 'Glue: A multi-task benchmark and analysis platform for '
                  'natural language understanding',
         'url': 'https://arxiv.org/abs/1804.07461',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=17443412968683100072&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGLUE:%2BA%2BMulti-Task%2BBenchmark%2Band%2BAnalysis%2BPlatform%2Bfor%2BNatural%2BLanguage%2BUnderstanding%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=qFfMBK1zE_IJ&ei=p74qX_y7DrGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:qFfMBK1zE_IJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBBLV-phzigKK_i3yAoJOqjDikM4gi&scisig=AAGBfm0AAAAAXyrBBFPHTaS0z6FThU0tZU7SfqMKybLa&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
696
-------------------------------------------------
2020-08-05 14:14:04
Got the results of the query
{'bib': {'abstract': 'Generative Adversarial Networks (GANs), though powerful, '
                     'is hard to train. Several recent works (Brock et al., '
                     '2016; Miyato et al., 2018) suggest that controlling the '
                     'spectra of weight matrices in the discriminator can '
                     'significantly improve the training of GANs. Motivated by '
                     'their discovery, we propose a new framework for training '
                     'GANs, which allows more flexible spectrum control (eg, '
                     'making the weight matrices of the discriminator have '
                     'slow singular value decays). Specifically, we propose a '
                     'new reparameterization approach',
         'author': ['H Jiang', 'Z Chen', 'M Chen', 'F Liu', 'D Wang'],
         'cites': '8',
         'eprint': 'https://par.nsf.gov/servlets/purl/10162614',
         'gsrank': '1',
         'title': 'On computation and generalization of generative adversarial '
                  'networks under spectrum control',
         'url': 'https://par.nsf.gov/biblio/10162614',
         'venue': '… Conference on Learning …',
         'year': '2019'},
 'citations_link': '/scholar?cites=11508884003947090136&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2BComputation%2Band%2BGeneralization%2Bof%2BGenerative%2BAdversarial%2BNetworks%2Bunder%2BSpectrum%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=2BBxYAbFt58J&ei=sL4qX6DRB6iBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:2BBxYAbFt58J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBDctyyCyE5AxeiUr7V9xajQD-vUOM&scisig=AAGBfm0AAAAAXyrBDbCXFqkA2LHi9r-3B-13ejr-2z5o&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
8
-------------------------------------------------
2020-08-05 14:14:13
Got the results of the query
{'bib': {'abstract': 'Reinforcement learning algorithms rely on carefully '
                     'engineering environment rewards that are extrinsic to '
                     'the agent. However, annotating each environment with '
                     'hand-designed, dense rewards is not scalable, motivating '
                     'the need for developing reward functions that are '
                     'intrinsic to the agent. Curiosity is a type of intrinsic '
                     'reward function which uses prediction error as reward '
                     'signal. In this paper:(a) We perform the first '
                     'large-scale study of purely curiosity-driven learning, '
                     'ie without any extrinsic rewards, across 54 standard '
                     'benchmark',
         'author': ['Y Burda', 'H Edwards', 'D Pathak', 'A Storkey'],
         'cites': '197',
         'eprint': 'https://arxiv.org/pdf/1808.04355',
         'gsrank': '1',
         'title': 'Large-scale study of curiosity-driven learning',
         'url': 'https://arxiv.org/abs/1808.04355',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=6931272873542879959&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLarge-Scale%2BStudy%2Bof%2BCuriosity-Driven%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=1wrTfPTTMGAJ&ei=ub4qX5CDFIvrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:1wrTfPTTMGAJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBFkXOF02PCxc2UOOgZ6LmD1ee8_cB&scisig=AAGBfm0AAAAAXyrBFoLoLBTIem3UQn6SnZef_DfJ0X70&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
197
-------------------------------------------------
2020-08-05 14:14:22
Got the results of the query
{'bib': {'abstract': 'Humans easily recognize object parts and their '
                     'hierarchical structure by watching how they move; they '
                     'can then predict how each part moves in the future. In '
                     'this paper, we propose a novel formulation that '
                     'simultaneously learns a hierarchical, disentangled '
                     'object representation and a dynamics model for object '
                     'parts from unlabeled videos. Our Parts, Structure, and '
                     'Dynamics (PSD) model learns to, first, recognize the '
                     'object parts via a layered image representation; second, '
                     'predict hierarchy via a structural descriptor that '
                     'composes',
         'author': ['Z Xu', 'Z Liu', 'C Sun', 'K Murphy', 'WT Freeman'],
         'cites': '21',
         'eprint': 'https://arxiv.org/pdf/1903.05136',
         'gsrank': '1',
         'title': 'Unsupervised discovery of parts, structure, and dynamics',
         'url': 'https://arxiv.org/abs/1903.05136',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=6600322539946703070&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BDiscovery%2Bof%2BParts,%2BStructure,%2Band%2BDynamics%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=3viGTGEOmVsJ&ei=wb4qX-DYH5qGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:3viGTGEOmVsJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBHpwcMJF9bWKVi0K7pTWApgm9VwlO&scisig=AAGBfm0AAAAAXyrBHodZ31dAa706IhvspaZYMslutvRR&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
21
-------------------------------------------------
2020-08-05 14:14:30
Got the results of the query
{'bib': {'abstract': 'Music relies heavily on repetition to build structure '
                     'and meaning. Self-reference occurs on multiple '
                     'timescales, from motifs to phrases to reusing of entire '
                     'sections of music, such as in pieces with ABA structure. '
                     'The Transformer (Vaswani et al., 2017), a sequence model '
                     'based on self-attention, has achieved compelling results '
                     'in many generation tasks that require maintaining '
                     'long-range coherence. This suggests that self-attention '
                     'might also be well-suited to modeling music. In musical '
                     'composition and performance, however, relative timing',
         'author': ['CZA Huang', 'A Vaswani', 'J Uszkoreit'],
         'cites': '42',
         'eprint': 'https://openreview.net/pdf?id=rJe4ShAcF7',
         'gsrank': '1',
         'title': 'Music transformer: Generating music with long-term '
                  'structure',
         'url': 'https://openreview.net/forum?id=rJe4ShAcF7',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=13281405787403608583&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMusic%2BTransformer:%2BGenerating%2BMusic%2Bwith%2BLong-Term%2BStructure%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=B2ICHDIIUbgJ&ei=yr4qX5X4OI-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:B2ICHDIIUbgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBJb1cDngyNuqBXIXLugKSsd2E_s2R&scisig=AAGBfm0AAAAAXyrBJXNpizHp3DtvamelA2qe2LeuSldw&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
42
-------------------------------------------------
2020-08-05 14:14:37
Got the results of the query
{'bib': {'abstract': 'Allowing humans to interactively train artificial agents '
                     'to understand language instructions is desirable for '
                     'both practical and scientific reasons. Though, given the '
                     'lack of sample efficiency in current learning methods, '
                     'reaching this goal may require substantial research '
                     'efforts. We introduce the BabyAI research platform, with '
                     'the goal of supporting investigations towards including '
                     'humans in the loop for grounded language learning. The '
                     'BabyAI platform comprises an extensible suite of 19 '
                     'levels of increasing difficulty. Each level gradually '
                     'leads',
         'author': ['M Chevalier-Boisvert', 'D Bahdanau'],
         'cites': '15',
         'eprint': 'https://openreview.net/pdf?id=rJeXCo0cYX',
         'gsrank': '1',
         'title': 'Babyai: A platform to study the sample efficiency of '
                  'grounded language learning',
         'url': 'https://openreview.net/forum?id=rJeXCo0cYX&source=post_page---------------------------',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=16615836502291630253&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBabyAI:%2BA%2BPlatform%2Bto%2BStudy%2Bthe%2BSample%2BEfficiency%2Bof%2BGrounded%2BLanguage%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=rcgPrUNPl-YJ&ei=0b4qX_a3HoyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:rcgPrUNPl-YJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBMOF1mO4UUAo8ZRv8U-ConppEheLd&scisig=AAGBfm0AAAAAXyrBMASwJLsQ9lDgGlmpQ7Hq2BRj3JZt&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 14:14:49
Got the results of the query
{'bib': {'abstract': 'In many tasks, in particular in natural science, the '
                     'goal is to determine hidden system parameters from a set '
                     'of measurements. Often, the forward process from '
                     'parameter-to measurement-space is a well-defined '
                     'function, whereas the inverse problem is ambiguous: one '
                     'measurement may map to multiple different sets of '
                     'parameters. In this setting, the posterior parameter '
                     'distribution, conditioned on an input measurement, has '
                     'to be determined. We argue that a particular class of '
                     'neural networks is well suited for this task--so',
         'author': ['L Ardizzone', 'J Kruse', 'S Wirkert', 'D Rahner'],
         'cites': '79',
         'eprint': 'https://arxiv.org/pdf/1808.04730',
         'gsrank': '1',
         'title': 'Analyzing inverse problems with invertible neural networks',
         'url': 'https://arxiv.org/abs/1808.04730',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11726167172639510448&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAnalyzing%2BInverse%2BProblems%2Bwith%2BInvertible%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=sE8GZuq2u6IJ&ei=3r4qX7WnCo-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:sE8GZuq2u6IJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBOiq0p29UdUQjw60OAjBdSZQIgq4m&scisig=AAGBfm0AAAAAXyrBOpF3x3Ax_jyROJaJzbyyPtzHJD1r&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
79
-------------------------------------------------
2020-08-05 14:14:58
Got the results of the query
{'bib': {'abstract': 'Generative adversarial networks (GANs) have achieved '
                     'great success at generating realistic images. However, '
                     'the text generation still remains a challenging task for '
                     'modern GAN architectures. In this work, we propose '
                     'RelGAN, a new GAN architecture for text generation, '
                     'consisting of three main components: a relational memory '
                     'based generator for the long-distance dependency '
                     'modeling, the Gumbel-Softmax relaxation for training '
                     'GANs on discrete data, and multiple embedded '
                     'representations in the discriminator to provide a more',
         'author': ['W Nie', 'N Narodytska', 'A Patel'],
         'cites': '32',
         'eprint': 'https://openreview.net/pdf?id=rJedV3R5tm',
         'gsrank': '1',
         'title': 'Relgan: Relational generative adversarial networks for text '
                  'generation',
         'url': 'https://openreview.net/forum?id=rJedV3R5tm',
         'venue': 'International conference on learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8523757541722331979&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRelGAN:%2BRelational%2BGenerative%2BAdversarial%2BNetworks%2Bfor%2BText%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=S5vqyVJ4SnYJ&ei=5b4qX7WEMYjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:S5vqyVJ4SnYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBQsZRBAynjQ83WOmAOeT4LmC3BZha&scisig=AAGBfm0AAAAAXyrBQgu67ypYLWszauMEHfdqCkLJs7L5&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
32
-------------------------------------------------
2020-08-05 14:15:06
Got the results of the query
{'bib': {'abstract': 'We characterize the singular values of the linear '
                     'transformation associated with a standard 2D '
                     'multi-channel convolutional layer, enabling their '
                     'efficient computation. This characterization also leads '
                     'to an algorithm for projecting a convolutional layer '
                     'onto an',
         'author': ['H Sedghi', 'V Gupta', 'PM Long'],
         'cites': '66',
         'eprint': 'https://arxiv.org/pdf/1805.10408',
         'gsrank': '1',
         'title': 'The singular values of convolutional layers',
         'url': 'https://arxiv.org/abs/1805.10408',
         'venue': 'arXiv preprint arXiv:1805.10408',
         'year': '2018'},
 'citations_link': '/scholar?cites=8194946284623254449&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BSingular%2BValues%2Bof%2BConvolutional%2BLayers%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=sVNgKDpMunEJ&ei=7L4qX4KRKIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:sVNgKDpMunEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBSIQaHZAIMfcokKqELQaV57NNNrjn&scisig=AAGBfm0AAAAAXyrBSCHvaeWBwrK52nluzFYOtdiFgwL8&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
66
-------------------------------------------------
2020-08-05 14:15:12
Got the results of the query
{'bib': {'abstract': 'Binary neural networks using the '
                     'Straight-Through-Estimator (STE) have been shown to '
                     'achieve state-of-the-art results, but their training '
                     'process is not well-founded. This is due to the '
                     'discrepancy between the evaluated function in the '
                     'forward path, and the weight updates in the '
                     'back-propagation, updates which do not correspond to '
                     'gradients of the forward path. Efficient convergence and '
                     'accuracy of binary models often rely on careful '
                     'fine-tuning and various ad-hoc techniques. In this work, '
                     'we empirically identify and study the effectiveness of',
         'author': ['M Alizadeh', 'J Fernández-Marqués'],
         'cites': '15',
         'eprint': 'https://openreview.net/pdf?id=rJfUCoR5KX',
         'gsrank': '1',
         'title': "An Empirical study of Binary Neural Networks' Optimisation",
         'url': 'https://openreview.net/forum?id=rJfUCoR5KX',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11225440842042128291&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2BEmpirical%2Bstudy%2Bof%2BBinary%2BNeural%2BNetworks%255C%2527%2BOptimisation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ozcVjfjGyJsJ&ei=9L4qX-q-NbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ozcVjfjGyJsJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBULiRu5vx2IYqC1_q2n3hEx01lAMZ&scisig=AAGBfm0AAAAAXyrBUEaJEP6uXdTNmgkliAHK8w19CMyu&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 14:15:21
Got the results of the query
{'bib': {'abstract': 'While Generative Adversarial Networks (GANs) have '
                     'empirically produced impressive results on learning '
                     'complex real-world distributions, recent work has shown '
                     'that they suffer from lack of diversity or mode '
                     'collapse. The theoretical work of Arora et al. suggests '
                     "a dilemma about GANs' statistical properties: powerful "
                     'discriminators cause overfitting, whereas weak '
                     'discriminators cannot detect mode collapse. In contrast, '
                     'we show in this paper that GANs can in principle learn '
                     'distributions in Wasserstein distance (or KL',
         'author': ['Y Bai', 'T Ma', 'A Risteski'],
         'cites': '26',
         'eprint': 'https://arxiv.org/pdf/1806.10586',
         'gsrank': '1',
         'title': 'Approximability of discriminators implies diversity in GANs',
         'url': 'https://arxiv.org/abs/1806.10586',
         'venue': 'arXiv preprint arXiv:1806.10586',
         'year': '2018'},
 'citations_link': '/scholar?cites=9758154062502373933&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DApproximability%2Bof%2BDiscriminators%2BImplies%2BDiversity%2Bin%2BGANs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=LSr9Pmvta4cJ&ei=_r4qX7jyHMKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:LSr9Pmvta4cJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBWaw_OwHcE6NOIE2ZQJKN58_FOMsB&scisig=AAGBfm0AAAAAXyrBWQ-J6w_DiR8uSv0-XqqInTli8ORy&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
26
-------------------------------------------------
2020-08-05 14:15:29
Got the results of the query
{'bib': {'abstract': 'Euclidean embeddings of data are fundamentally limited '
                     'in their ability to capture latent semantic structures, '
                     'which need not conform to Euclidean spatial assumptions. '
                     'Here we consider an alternative, which embeds data as '
                     'discrete probability distributions in a Wasserstein '
                     'space, endowed with an optimal transport metric. '
                     'Wasserstein spaces are much larger and more flexible '
                     'than Euclidean spaces, in that they can successfully '
                     'embed a wider variety of metric structures. We exploit '
                     'this flexibility by learning an embedding that',
         'author': ['C Frogner', 'F Mirzazadeh', 'J Solomon'],
         'cites': '1',
         'eprint': 'https://arxiv.org/pdf/1905.03329',
         'gsrank': '1',
         'title': 'Learning Embeddings into Entropic Wasserstein Spaces',
         'url': 'https://arxiv.org/abs/1905.03329',
         'venue': 'arXiv preprint arXiv:1905.03329',
         'year': '2019'},
 'citations_link': '/scholar?cites=3356420680246481859&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BEmbeddings%2Binto%2BEntropic%2BWasserstein%2BSpaces%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=w9PdWtNmlC4J&ei=Br8qX9z5BMKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:w9PdWtNmlC4J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBY94QDPjTiYgOAkC7zAJl_xGpG9Xm&scisig=AAGBfm0AAAAAXyrBY8UEZTM-xwQq_ST6HawUJW7Ad5Be&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
1
-------------------------------------------------
2020-08-05 14:15:39
Got the results of the query
{'bib': {'abstract': 'Because the choice and tuning of the optimizer affects '
                     'the speed, and ultimately the performance of deep '
                     'learning, there is significant past and recent research '
                     'in this area. Yet, perhaps surprisingly, there is no '
                     'generally agreed-upon protocol for the quantitative and '
                     'reproducible evaluation of optimization strategies for '
                     'deep learning. We suggest routines and benchmarks for '
                     'stochastic optimization, with special focus on the '
                     'unique aspects of deep learning, such as stochasticity, '
                     'tunability and generalization. As the primary',
         'author': ['F Schneider', 'L Balles', 'P Hennig'],
         'cites': '6',
         'eprint': 'https://arxiv.org/pdf/1903.05499',
         'gsrank': '1',
         'title': 'DeepOBS: A deep learning optimizer benchmark suite',
         'url': 'https://arxiv.org/abs/1903.05499',
         'venue': 'arXiv preprint arXiv:1903.05499',
         'year': '2019'},
 'citations_link': '/scholar?cites=10657953635405668036&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeepOBS:%2BA%2BDeep%2BLearning%2BOptimizer%2BBenchmark%2BSuite%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=xB7w9lyo6JMJ&ei=EL8qX-rOCIjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:xB7w9lyo6JMJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBbXOftpvgdNfcyUvyGKRV9rN3qzvb&scisig=AAGBfm0AAAAAXyrBbbOO7XBmakqeIrTAgSzwzWkfOUPF&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 14:15:49
Got the results of the query
{'bib': {'abstract': 'A central challenge in reinforcement learning is '
                     'discovering effective policies for tasks where rewards '
                     'are sparsely distributed. We postulate that in the '
                     'absence of useful reward signals, an effective '
                     'exploration strategy should seek out {\\it decision '
                     'states}. These states lie at critical junctions in the '
                     'state space from where the agent can transition to new, '
                     'potentially unexplored regions. We propose to learn '
                     'about decision states from prior experience. By training '
                     'a goal-conditioned policy with an information '
                     'bottleneck, we can identify decision',
         'author': ['A Goyal', 'R Islam', 'D Strouse', 'Z Ahmed'],
         'cites': '40',
         'eprint': 'https://arxiv.org/pdf/1901.10902',
         'gsrank': '1',
         'title': 'Infobot: Transfer and exploration via the information '
                  'bottleneck',
         'url': 'https://arxiv.org/abs/1901.10902',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=4130805279522394424&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInfoBot:%2BTransfer%2Band%2BExploration%2Bvia%2Bthe%2BInformation%2BBottleneck%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=OJE9CoSRUzkJ&ei=Gb8qX4T6Io-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:OJE9CoSRUzkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBeG1kQZtKGGKlxiUEOVmSXKKOqU9Q&scisig=AAGBfm0AAAAAXyrBeMvcBnefUmOrtW_iBPIJa7YpPHDX&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
40
-------------------------------------------------
2020-08-05 14:16:00
Got the results of the query
{'bib': {'abstract': 'There has been a large amount of interest, both in the '
                     'past and particularly recently, into the relative '
                     'advantage of different families of universal function '
                     'approximators, for instance neural networks, '
                     'polynomials, rational functions, etc. However, current '
                     'research has focused almost exclusively on understanding '
                     'this problem in a worst case setting: eg characterizing '
                     'the best L1 or L_ {infty} approximation in a box (or '
                     'sometimes, even under an adversarially constructed data '
                     'distribution.) In this setting many classical tools from '
                     'approximation theory',
         'author': ['F Koehler', 'A Risteski'],
         'cites': '1',
         'eprint': 'https://openreview.net/pdf?id=rJgTTjA9tX',
         'gsrank': '1',
         'title': 'The Comparative Power of ReLU Networks and Polynomial '
                  'Kernels in the Presence of Sparse Latent Structure',
         'url': 'https://openreview.net/forum?id=rJgTTjA9tX',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=4978405382971332072&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BComparative%2BPower%2Bof%2BReLU%2BNetworks%2Band%2BPolynomial%2BKernels%2Bin%2Bthe%2BPresence%2Bof%2BSparse%2BLatent%2BStructure%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=6BXmr1HZFkUJ&ei=Jb8qX4WCPaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:6BXmr1HZFkUJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBgmaFEmOLjs0jBgCerokZ-EBjUfEi&scisig=AAGBfm0AAAAAXyrBgsxwVvOkEbGb8v7vP4_-yj4e02D_&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
1
-------------------------------------------------
2020-08-05 14:16:10
Got the results of the query
{'bib': {'abstract': 'We introduce a parameter sharing scheme, in which '
                     'different layers of a convolutional neural network (CNN) '
                     'are defined by a learned linear combination of parameter '
                     'tensors from a global bank of templates. Restricting the '
                     'number of templates yields a flexible hybridization of '
                     'traditional CNNs and recurrent networks. Compared to '
                     'traditional CNNs, we demonstrate substantial parameter '
                     'savings on standard image classification tasks, while '
                     'maintaining accuracy. Our simple parameter sharing '
                     'scheme, though defined via soft',
         'author': ['P Savarese', 'M Maire'],
         'cites': '13',
         'eprint': 'https://arxiv.org/pdf/1902.09701',
         'gsrank': '1',
         'title': 'Learning implicitly recurrent CNNs through parameter '
                  'sharing',
         'url': 'https://arxiv.org/abs/1902.09701',
         'venue': 'arXiv preprint arXiv:1902.09701',
         'year': '2019'},
 'citations_link': '/scholar?cites=15123734257747528548&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BImplicitly%2BRecurrent%2BCNNs%2BThrough%2BParameter%2BSharing%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ZItyri1M4tEJ&ei=ML8qX_uHMqOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ZItyri1M4tEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBjORPskDkE9NGb2zNJVdiWWH1CP1g&scisig=AAGBfm0AAAAAXyrBjLNnLm1lYOkxAXt6_KvbkOxnvoZH&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
13
-------------------------------------------------
2020-08-05 14:16:20
Got the results of the query
{'bib': {'abstract': 'Real-life control tasks involve matters of various '
                     'substances---rigid or soft bodies, liquid, gas---each '
                     'with distinct physical behaviors. This poses challenges '
                     'to traditional rigid-body physics engines. '
                     'Particle-based simulators have been developed to model '
                     'the dynamics of these complex scenes; however, relying '
                     'on approximation techniques, their simulation often '
                     'deviates from real-world physics, especially in the long '
                     'term. In this paper, we propose to learn a '
                     'particle-based simulator for complex control tasks. '
                     'Combining learning with particle',
         'author': ['Y Li', 'J Wu', 'R Tedrake', 'JB Tenenbaum'],
         'cites': '45',
         'eprint': 'https://arxiv.org/pdf/1810.01566',
         'gsrank': '1',
         'title': 'Learning particle dynamics for manipulating rigid bodies, '
                  'deformable objects, and fluids',
         'url': 'https://arxiv.org/abs/1810.01566',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1764275287835987384&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BParticle%2BDynamics%2Bfor%2BManipulating%2BRigid%2BBodies,%2BDeformable%2BObjects,%2Band%2BFluids%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=uM122Qb3exgJ&ei=N78qX_uqM4-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:uM122Qb3exgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBl4CtWhNcGTBZf0vnHyRIUOV30NUJ&scisig=AAGBfm0AAAAAXyrBlyTpePzBI3i4E2D1iSbFA3086_Si&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
45
-------------------------------------------------
2020-08-05 14:16:31
Got the results of the query
{'bib': {'abstract': 'We propose Regularized Learning under Label shifts '
                     '(RLLS), a principled and a practical domain-adaptation '
                     'algorithm to correct for shifts in the label '
                     'distribution between a source and a target domain. We '
                     'first estimate importance weights using labeled source '
                     'data and unlabeled target data, and then train a '
                     'classifier on the weighted source samples. We derive a '
                     'generalization bound for the classifier on the target '
                     'domain which is independent of the (ambient) data '
                     'dimensions, and instead only depends on the complexity '
                     'of the function',
         'author': ['K Azizzadenesheli', 'A Liu', 'F Yang'],
         'cites': '29',
         'eprint': 'https://arxiv.org/pdf/1903.09734',
         'gsrank': '1',
         'title': 'Regularized learning for domain adaptation under label '
                  'shifts',
         'url': 'https://arxiv.org/abs/1903.09734',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=9783659999001427739&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRegularized%2BLearning%2Bfor%2BDomain%2BAdaptation%2Bunder%2BLabel%2BShifts%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=G7-t2-6KxocJ&ei=Qr8qX9_HB4yimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:G7-t2-6KxocJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBncAYl1LewYLZSyUbPdsQhANUV_pw&scisig=AAGBfm0AAAAAXyrBnW0E98lbDPaa106O3JJQarrc0xAT&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
29
-------------------------------------------------
2020-08-05 14:16:37
Got the results of the query
{'bib': {'abstract': 'The Softmax function is used in the final layer of '
                     'nearly all existing sequence-to-sequence models for '
                     'language generation. However, it is usually the slowest '
                     'layer to compute which limits the vocabulary size to a '
                     'subset of most frequent types; and it has a large memory '
                     'footprint. We propose a general technique for replacing '
                     'the softmax layer with a continuous embedding layer. Our '
                     'primary innovations are a novel probabilistic loss, and '
                     'a training and inference procedure in which we generate '
                     'a probability distribution over pre-trained word',
         'author': ['S Kumar', 'Y Tsvetkov'],
         'cites': '24',
         'eprint': 'https://arxiv.org/pdf/1812.04616',
         'gsrank': '1',
         'title': 'Von mises-fisher loss for training sequence to sequence '
                  'models with continuous outputs',
         'url': 'https://arxiv.org/abs/1812.04616',
         'venue': 'arXiv preprint arXiv:1812.04616',
         'year': '2018'},
 'citations_link': '/scholar?cites=1822338940984352644&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVon%2BMises-Fisher%2BLoss%2Bfor%2BTraining%2BSequence%2Bto%2BSequence%2BModels%2Bwith%2BContinuous%2BOutputs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=hN-7lZw_ShkJ&ei=SL8qX6ypMYjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:hN-7lZw_ShkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBpLsa5czsn8fmR9zeHonelHRjeWtl&scisig=AAGBfm0AAAAAXyrBpElIQ1vZGDqYY_zBCqaRMxNzCbTr&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
24
-------------------------------------------------
2020-08-05 14:16:44
Got the results of the query
{'bib': {'abstract': 'The behavioral dynamics of multi-agent systems have a '
                     'rich and orderly structure, which can be leveraged to '
                     'understand these systems, and to improve how artificial '
                     'agents learn to operate in them. Here we introduce '
                     'Relational Forward Models (RFM) for multi-agent '
                     'learning, networks that can learn to make accurate '
                     "predictions of agents' future behavior in multi-agent "
                     'environments. Because these models operate on the '
                     'discrete entities and relations present in the '
                     'environment, they produce interpretable intermediate',
         'author': ['A Tacchetti', 'HF Song', 'PAM Mediano'],
         'cites': '25',
         'eprint': 'https://arxiv.org/pdf/1809.11044',
         'gsrank': '1',
         'title': 'Relational forward models for multi-agent learning',
         'url': 'https://arxiv.org/abs/1809.11044',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1450759969074634127&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRelational%2BForward%2BModels%2Bfor%2BMulti-Agent%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=j90MY4EiIhQJ&ei=T78qX6nTBIjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:j90MY4EiIhQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBrIP4Fu5KrTLWq5ZB03ck2vK_aios&scisig=AAGBfm0AAAAAXyrBrK82YSoWon9b0-NO5i9vnGPm6vuK&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
25
-------------------------------------------------
2020-08-05 14:16:52
Got the results of the query
{'bib': {'abstract': 'We present a novel method to precisely impose '
                     'tree-structured category information onto '
                     'word-embeddings, resulting in ball embeddings in higher '
                     'dimensional spaces (N-balls for short). Inclusion '
                     'relations among N-balls implicitly encode subordinate '
                     'relations among categories. The similarity measurement '
                     'in terms of the cosine function is enriched by category '
                     'information. Using a geometric construction method '
                     'instead of back-propagation, we create large N-ball '
                     'embeddings that satisfy two conditions:(1) category '
                     'trees are',
         'author': ['T Dong', 'C Bauckhage', 'H Jin', 'J Li'],
         'cites': '10',
         'eprint': 'https://openreview.net/pdf?id=rJlWOj0qF7',
         'gsrank': '1',
         'title': 'Imposing category trees onto word-embeddings using a '
                  'geometric construction',
         'url': 'https://openreview.net/forum?id=rJlWOj0qF7&noteId=HkguZ1dlG4',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=13347660437088281744&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImposing%2BCategory%2BTrees%2BOnto%2BWord-Embeddings%2BUsing%2BA%2BGeometric%2BConstruction%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=kKQu13JqPLkJ&ei=XL8qX5rpJbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:kKQu13JqPLkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBumJxZOotmuszEbK7Q2mVlq4aPA7z&scisig=AAGBfm0AAAAAXyrBuuV3R-esCIvbWynSC4WYK5bNiZ9V&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
10
-------------------------------------------------
2020-08-05 14:17:06
Got the results of the query
{'bib': {'abstract': 'A key component for many reinforcement learning agents '
                     'is to learn a value function, either for policy '
                     'evaluation or control. Many of the algorithms for '
                     'learning values, however, are designed for linear '
                     'function approximation---with a fixed basis or fixed '
                     'representation. Though there have been a few sound '
                     'extensions to nonlinear function approximation, such as '
                     'nonlinear gradient temporal difference learning, these '
                     'methods have largely not been adopted, eschewed in '
                     'favour of simpler but not sound methods like temporal '
                     'difference',
         'author': ['W Chung', 'S Nath', 'A Joseph', 'M White'],
         'cites': '9',
         'eprint': 'https://openreview.net/pdf?id=rJleN20qK7',
         'gsrank': '1',
         'title': 'Two-timescale networks for nonlinear value function '
                  'approximation',
         'url': 'https://openreview.net/forum?id=rJleN20qK7&noteId=rJleN20qK7',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=17946656214102686695&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTwo-Timescale%2BNetworks%2Bfor%2BNonlinear%2BValue%2BFunction%2BApproximation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=57ulw71UD_kJ&ei=Zb8qX7T4KpqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:57ulw71UD_kJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBw5W81niOMAahHR71lY7FLaVECSdw&scisig=AAGBfm0AAAAAXyrBw6Thk98rAodyLtdslwjBpfL82QZ-&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 14:17:15
Got the results of the query
{'bib': {'abstract': 'We propose a simple yet highly effective method that '
                     'addresses the mode-collapse problem in the Conditional '
                     'Generative Adversarial Network (cGAN). Although '
                     'conditional distributions are multi-modal (ie, having '
                     'many modes) in practice, most cGAN approaches tend to '
                     'learn an overly simplified distribution where an input '
                     'is always mapped to a single output regardless of '
                     'variations in latent code. To address such issue, we '
                     'propose to explicitly regularize the generator to '
                     'produce diverse outputs depending on latent codes',
         'author': ['D Yang', 'S Hong', 'Y Jang', 'T Zhao', 'H Lee'],
         'cites': '36',
         'eprint': 'https://arxiv.org/pdf/1901.09024',
         'gsrank': '1',
         'title': 'Diversity-sensitive conditional generative adversarial '
                  'networks',
         'url': 'https://arxiv.org/abs/1901.09024',
         'venue': 'arXiv preprint arXiv:1901.09024',
         'year': '2019'},
 'citations_link': '/scholar?cites=154904573992759731&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiversity-Sensitive%2BConditional%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=s_HrquNUJgIJ&ei=br8qX_eAFbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:s_HrquNUJgIJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrBy1PwrXs0TMyD3ibWNBPC-BEjUsaw&scisig=AAGBfm0AAAAAXyrBy222h9PPCZyWCeMjnzAYzK34rlbB&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
36
-------------------------------------------------
2020-08-05 14:17:23
Got the results of the query
{'bib': {'abstract': 'We study the problem of attacking a machine learning '
                     'model in the hard-label black-box setting, where no '
                     'model information is revealed except that the attacker '
                     'can make queries to probe the corresponding hard-label '
                     'decisions. This is a very challenging problem since the '
                     'direct extension of state-of-the-art white-box attacks '
                     '(eg, CW or PGD) to the hard-label black-box setting will '
                     'require minimizing a non-continuous step function, which '
                     'is combinatorial and cannot be solved by a '
                     'gradient-based optimizer. The only current approach is '
                     'based on',
         'author': ['M Cheng', 'T Le', 'PY Chen', 'J Yi', 'H Zhang'],
         'cites': '90',
         'eprint': 'https://arxiv.org/pdf/1807.04457.pdf).',
         'gsrank': '1',
         'title': 'Query-efficient hard-label black-box attack: An '
                  'optimization-based approach',
         'url': 'https://arxiv.org/abs/1807.04457',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=5116459169179417425&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQuery-Efficient%2BHard-label%2BBlack-box%2BAttack:%2BAn%2BOptimization-based%2BApproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Ubu6loFQAUcJ&ei=d78qX-S3GYjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Ubu6loFQAUcJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrB1ZCesfGCRPpoQSly8X3uPqR8YSUC&scisig=AAGBfm0AAAAAXyrB1VW03W2FQqxS1s-8IJD-jE5rrf0f&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
90
-------------------------------------------------
2020-08-05 14:17:33
Got the results of the query
{'bib': {'abstract': 'Network pruning is widely used for reducing the heavy '
                     'inference cost of deep models in low-resource settings. '
                     'A typical pruning algorithm is a three-stage pipeline, '
                     'ie, training (a large model), pruning and fine-tuning. '
                     'During pruning, according to a certain criterion, '
                     'redundant weights are pruned and important weights are '
                     'kept to best preserve the accuracy. In this work, we '
                     'make several surprising observations which contradict '
                     'common beliefs. For all state-of-the-art structured '
                     'pruning algorithms we examined, fine-tuning a pruned '
                     'model only',
         'author': ['Z Liu', 'M Sun', 'T Zhou', 'G Huang', 'T Darrell'],
         'cites': '237',
         'eprint': 'https://arxiv.org/pdf/1810.05270',
         'gsrank': '1',
         'title': 'Rethinking the value of network pruning',
         'url': 'https://arxiv.org/abs/1810.05270',
         'venue': 'arXiv preprint arXiv:1810.05270',
         'year': '2018'},
 'citations_link': '/scholar?cites=3601827758437367761&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRethinking%2Bthe%2BValue%2Bof%2BNetwork%2BPruning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=0f_MU0ND_DEJ&ei=gb8qX_yXJYyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:0f_MU0ND_DEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrB3Zs06ivX3t5-mE4LMR0R1AmKV8Hq&scisig=AAGBfm0AAAAAXyrB3ZoAKjJgnNXlyn0OfDrKnnxBtSxd&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
237
-------------------------------------------------
2020-08-05 14:17:41
Got the results of the query
{'bib': {'abstract': 'We introduce hyperbolic attention networks to endow '
                     'neural networks with enough capacity to match the '
                     'complexity of data with hierarchical and power-law '
                     'structure. A few recent approaches have successfully '
                     'demonstrated the benefits of imposing hyperbolic '
                     'geometry',
         'author': ['C Gulcehre', 'M Denil', 'M Malinowski', 'A Razavi'],
         'cites': '43',
         'eprint': 'https://arxiv.org/pdf/1805.09786',
         'gsrank': '1',
         'title': 'Hyperbolic attention networks',
         'url': 'https://arxiv.org/abs/1805.09786',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1595119013035173525&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHyperbolic%2BAttention%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=leZugkoAIxYJ&ei=ir8qX_DBK6OGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:leZugkoAIxYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrB5A5UXcuAABCgAtjdCspQU1dj0FdP&scisig=AAGBfm0AAAAAXyrB5PwFHH2cUHbmnny0yUPyaQqrcimX&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
43
-------------------------------------------------
2020-08-05 14:17:48
Got the results of the query
{'bib': {'abstract': 'We consider the problem of learning a binary classifier '
                     'only from positive data and unlabeled data (PU '
                     'learning). Recent methods of PU learning commonly assume '
                     'that the labeled positive data are identically '
                     'distributed as the unlabeled positive data. However, '
                     'this assumption is unrealistic in many instances of PU '
                     'learning because it fails to capture the existence of a '
                     'selection bias in the labeling process. When the data '
                     'has a selection bias, it is difficult to learn the Bayes '
                     'optimal classifier by conventional methods of PU '
                     'learning. In this',
         'author': ['M Kato', 'T Teshima', 'J Honda'],
         'cites': '13',
         'eprint': 'https://openreview.net/pdf?id=rJzLciCqKm',
         'gsrank': '1',
         'title': 'Learning from positive and unlabeled data with a selection '
                  'bias',
         'url': 'https://openreview.net/forum?id=rJzLciCqKm',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=349495997136611157&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bfrom%2BPositive%2Band%2BUnlabeled%2BData%2Bwith%2Ba%2BSelection%2BBias%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Vae_98Go2QQJ&ei=j78qX6rDKrGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Vae_98Go2QQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrB62WX6XFAbh1zXgS6JmmTDksSfzNs&scisig=AAGBfm0AAAAAXyrB6zmZcfpbVQ61a8u0m0lykUT33TPr&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
13
-------------------------------------------------
2020-08-05 14:17:55
Got the results of the query
{'bib': {'abstract': 'We present a new algorithm to train a robust neural '
                     'network against adversarial attacks. Our algorithm is '
                     'motivated by the following two ideas. First, although '
                     'recent work has demonstrated that fusing randomness can '
                     'improve the robustness of neural networks (Liu 2017), we '
                     'noticed that adding noise blindly to all the layers is '
                     'not the optimal way to incorporate randomness. Instead, '
                     'we model randomness under the framework of Bayesian '
                     'Neural Network (BNN) to formally learn the posterior '
                     'distribution of models in a scalable',
         'author': ['X Liu', 'Y Li', 'C Wu', 'CJ Hsieh'],
         'cites': '33',
         'eprint': 'https://arxiv.org/pdf/1810.01279',
         'gsrank': '1',
         'title': 'Adv-bnn: Improved adversarial defense through robust '
                  'bayesian neural network',
         'url': 'https://arxiv.org/abs/1810.01279',
         'venue': 'arXiv preprint arXiv:1810.01279',
         'year': '2018'},
 'citations_link': '/scholar?cites=16111397550296660225&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdv-BNN:%2BImproved%2BAdversarial%2BDefense%2Bthrough%2BRobust%2BBayesian%2BNeural%2BNetwork%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=AfmHzLUul98J&ei=l78qX4GvLIjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:AfmHzLUul98J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrB8yWA9QbrgUpYVsrNaLrbZua9qgaP&scisig=AAGBfm0AAAAAXyrB88OlO01jqZdLeXC5jVfxZMuQPM5J&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
33
-------------------------------------------------
2020-08-05 14:18:04
Got the results of the query
{'bib': {'abstract': 'We present Optimal Completion Distillation (OCD), a '
                     'training procedure for optimizing sequence to sequence '
                     'models based on edit distance. OCD is efficient, has no '
                     'hyper-parameters of its own, and does not require '
                     'pretraining or joint optimization with conditional '
                     'log-likelihood. Given a partial sequence generated by '
                     'the model, we first identify the set of optimal suffixes '
                     'that minimize the total edit distance, using an '
                     'efficient dynamic programming algorithm. Then, for each '
                     'position of the generated sequence, we use a target '
                     'distribution',
         'author': ['S Sabour', 'W Chan', 'M Norouzi'],
         'cites': '24',
         'eprint': 'https://arxiv.org/pdf/1810.01398',
         'gsrank': '1',
         'title': 'Optimal completion distillation for sequence learning',
         'url': 'https://arxiv.org/abs/1810.01398',
         'venue': 'arXiv preprint arXiv:1810.01398',
         'year': '2018'},
 'citations_link': '/scholar?cites=1067727282240510663&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOptimal%2BCompletion%2BDistillation%2Bfor%2BSequence%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=x1rwXk5U0Q4J&ei=or8qX9mHFrGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:x1rwXk5U0Q4J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrB_VXGn1uI3W4QBnggHLXMlMFRVK-8&scisig=AAGBfm0AAAAAXyrB_WQ6kmJy5SxoJKHmsOyMxPc2zmT2&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
24
-------------------------------------------------
2020-08-05 14:18:13
Got the results of the query
{'bib': {'abstract': 'Information bottleneck (IB) is a method for extracting '
                     'information from one random variable $ X $ that is '
                     'relevant for predicting another random variable $ Y $. '
                     'To do so, IB identifies an intermediate" bottleneck" '
                     'variable $ T $ that has low mutual information $ I (X; '
                     'T) $ and high mutual information $ I (Y; T) $. The" IB '
                     'curve" characterizes the set of bottleneck variables '
                     'that achieve maximal $ I (Y; T) $ for a given $ I (X; T) '
                     '$, and is typically explored by maximizing the" IB '
                     'Lagrangian", $ I (Y; T)-\\beta I (X; T) $. In some '
                     'cases, $ Y $ is a',
         'author': ['A Kolchinsky', 'BD Tracey', 'S Van Kuyk'],
         'cites': '21',
         'eprint': 'https://arxiv.org/pdf/1808.07593',
         'gsrank': '1',
         'title': 'Caveats for information bottleneck in deterministic '
                  'scenarios',
         'url': 'https://arxiv.org/abs/1808.07593',
         'venue': 'arXiv preprint arXiv:1808.07593',
         'year': '2018'},
 'citations_link': '/scholar?cites=8561375002982335569&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCaveats%2Bfor%2Binformation%2Bbottleneck%2Bin%2Bdeterministic%2Bscenarios%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=UcSrgTQd0HYJ&ei=qr8qX7H6H4vrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:UcSrgTQd0HYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCCAH_8vxf-rQn3ExoxSYjZWM3o1mh&scisig=AAGBfm0AAAAAXyrCCOlkkSuDx8JY4rp3K_gPZoJap0x2&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
21
-------------------------------------------------
2020-08-05 14:18:24
Got the results of the query
{'bib': {'abstract': 'The ground-breaking performance obtained by deep '
                     'convolutional neural networks (CNNs) for image '
                     'processing tasks is inspiring research efforts '
                     'attempting to extend it for 3D geometric tasks. One of '
                     'the main challenge in applying CNNs to 3D shape analysis '
                     'is how to define a natural convolution operator on '
                     'non-euclidean surfaces. In this paper, we present a '
                     'method for applying deep learning to 3D surfaces using '
                     'their spherical descriptors and alt-az anisotropic '
                     'convolution on 2-sphere. A cascade set of geodesic disk '
                     'filters rotate on the 2',
         'author': ['M Liu', 'F Yao', 'C Choi', 'A Sinha'],
         'cites': '7',
         'eprint': 'https://openreview.net/pdf?id=rkeSiiA5Fm',
         'gsrank': '1',
         'title': 'Deep learning 3d shapes using alt-az anisotropic 2-sphere '
                  'convolution',
         'url': 'https://openreview.net/forum?id=rkeSiiA5Fm',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=5655071054318144878&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLearning%2B3D%2BShapes%2BUsing%2BAlt-az%2BAnisotropic%2B2-Sphere%2BConvolution%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=bg06fCnZek4J&ei=tL8qX-TmDqiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:bg06fCnZek4J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCD9fimL1aTlpbpi6qztB0VVARybzA&scisig=AAGBfm0AAAAAXyrCDxrO4xIn_yfa1GHTRIiEiBv4HHMR&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
7
-------------------------------------------------
2020-08-05 14:18:31
Got the results of the query
{'bib': {'abstract': 'We investigate the loss surface of neural networks. We '
                     'prove that even for one-hidden-layer networks with" '
                     'slightest" nonlinearity, the empirical risks have '
                     'spurious local minima in most cases. Our results thus '
                     'indicate that in general" no spurious local minima" is a '
                     'property limited to deep linear networks, and insights '
                     'obtained from linear networks may not be robust. '
                     'Specifically, for ReLU (-like) networks we '
                     'constructively prove that for almost all practical '
                     'datasets there exist infinitely many local minima. We '
                     'also present a',
         'author': ['C Yun', 'S Sra', 'A Jadbabaie'],
         'cites': '33',
         'eprint': 'https://arxiv.org/pdf/1802.03487',
         'gsrank': '1',
         'title': 'Small nonlinearities in activation functions create bad '
                  'local minima in neural networks',
         'url': 'https://arxiv.org/abs/1802.03487',
         'venue': 'arXiv preprint arXiv:1802.03487',
         'year': '2018'},
 'citations_link': '/scholar?cites=12267478949647511567&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSmall%2Bnonlinearities%2Bin%2Bactivation%2Bfunctions%2Bcreate%2Bbad%2Blocal%2Bminima%2Bin%2Bneural%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=D3BS2xvXPqoJ&ei=vr8qX-TlGaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:D3BS2xvXPqoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCGnHqLPl1J19_tFVV9OQqm5GSGX9Y&scisig=AAGBfm0AAAAAXyrCGmmTo4lGxHgHzUqD7xS2q8RTkopc&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
33
-------------------------------------------------
2020-08-05 14:18:42
Got the results of the query
{'bib': {'abstract': 'In this article we use rate-distortion theory, a branch '
                     'of information theory devoted to the problem of lossy '
                     'compression, to shed light on an important problem in '
                     'latent variable modeling of data: is there room to '
                     'improve the model? One way to address this question is '
                     'to find an upper bound on the probability (equivalently '
                     'a lower bound on the negative log likelihood) that the '
                     'model can assign to some data as one varies the prior '
                     'and/or the likelihood function in a latent variable '
                     'model. The core of our contribution is to formally show',
         'author': ['LA Lastras'],
         'cites': '2',
         'eprint': 'https://arxiv.org/pdf/1904.06395',
         'gsrank': '1',
         'title': 'Information theoretic lower bounds on negative log '
                  'likelihood',
         'url': 'https://arxiv.org/abs/1904.06395',
         'venue': 'arXiv preprint arXiv:1904.06395',
         'year': '2019'},
 'citations_link': '/scholar?cites=13031039824552411051&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInformation%2BTheoretic%2Blower%2Bbounds%2Bon%2Bnegative%2Blog%2Blikelihood%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=qz9q1K2N17QJ&ei=yL8qX87XEMKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:qz9q1K2N17QJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCJsTfwZ9Vb2qxuDFTo9mABxRWo_OK&scisig=AAGBfm0AAAAAXyrCJge7F36WiAudX3C7R1XWsOntdG-g&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
2
-------------------------------------------------
2020-08-05 14:18:54
Got the results of the query
{'bib': {'abstract': 'Reinforcement learning (RL) agents optimize only the '
                     'features specified in a reward function and are '
                     'indifferent to anything left out inadvertently. This '
                     'means that we must not only specify what to do, but also '
                     'the much larger space of what not to do. It is easy to '
                     'forget these preferences, since these preferences are '
                     'already satisfied in our environment. This motivates our '
                     'key insight: when a robot is deployed in an environment '
                     'that humans act in, the state of the environment is '
                     'already optimized for what humans want. We can therefore',
         'author': ['R Shah', 'D Krasheninnikov', 'J Alexander'],
         'cites': '6',
         'eprint': 'https://arxiv.org/pdf/1902.04198',
         'gsrank': '1',
         'title': 'Preferences Implicit in the State of the World',
         'url': 'https://arxiv.org/abs/1902.04198',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=9659325123261489202&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPreferences%2BImplicit%2Bin%2Bthe%2BState%2Bof%2Bthe%2BWorld%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=MuABtgXRDIYJ&ei=1b8qX72kBojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:MuABtgXRDIYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCMmFHSzm5o55c8ZwVXJ5xtXFkyqL5&scisig=AAGBfm0AAAAAXyrCMvnuVG-7nJ8WSN4wBoC5ABIbWgKW&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 14:19:07
Got the results of the query
{'bib': {'abstract': 'In this paper, we present a random matrix approach to '
                     'recover sparse principal components from n p-dimensional '
                     'vectors. Specifically, considering the large dimensional '
                     'setting where n, p→∞ with p/n→ c∈(0,∞) and under '
                     'Gaussian vector observations, we study kernel random '
                     'matrices of the type f (Ĉ), where f is a three-times '
                     'continuously differentiable function applied entry-wise '
                     'to the sample covariance matrix Ĉ of the data. Then, '
                     'assuming that the principal components are sparse, we '
                     "show that taking f in such a way that f'(0)= f''(0)= 0",
         'author': ['MEA Seddik', 'M Tamaazousti'],
         'cites': '2',
         'eprint': 'https://openreview.net/pdf?id=rkgBHoCqYX',
         'gsrank': '1',
         'title': 'A kernel random matrix-based approach for sparse PCA',
         'url': 'https://openreview.net/forum?id=rkgBHoCqYX',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=4157067689430485336&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BKernel%2BRandom%2BMatrix-Based%2BApproach%2Bfor%2BSparse%2BPCA%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=WAnC1wnfsDkJ&ei=3r8qX6qhNI-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:WAnC1wnfsDkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCPEqsomF9G_Vko2uT2SXzHrY8T6pD&scisig=AAGBfm0AAAAAXyrCPGPECcNrOFvL1De30gXvwTcNGHGk&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
2
-------------------------------------------------
2020-08-05 14:19:16
Got the results of the query
{'bib': {'abstract': 'For autonomous agents to successfully operate in the '
                     'real world, the ability to anticipate future scene '
                     'states is a key competence. In real-world scenarios, '
                     'future states become increasingly uncertain and '
                     'multi-modal, particularly on long time horizons. Dropout '
                     'based Bayesian inference provides a computationally '
                     'tractable, theoretically well grounded approach to learn '
                     'likely hypotheses/models to deal with uncertain futures '
                     'and make predictions that correspond well to '
                     'observations--are well calibrated. However, it turns out',
         'author': ['A Bhattacharyya', 'M Fritz', 'B Schiele'],
         'cites': '20',
         'eprint': 'https://arxiv.org/pdf/1810.00746',
         'gsrank': '1',
         'title': 'Bayesian prediction of future street scenes using synthetic '
                  'likelihoods',
         'url': 'https://arxiv.org/abs/1810.00746',
         'venue': 'arXiv preprint arXiv:1810.00746',
         'year': '2018'},
 'citations_link': '/scholar?cites=8839058001244993401&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBayesian%2BPrediction%2Bof%2BFuture%2BStreet%2BScenes%2Busing%2BSynthetic%2BLikelihoods%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=eTdIuWukqnoJ&ei=578qX822J4-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:eTdIuWukqnoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCRryY0Cir3CULWa0X8YEuyfKxa4Uo&scisig=AAGBfm0AAAAAXyrCRqqnxAKACVbhRHu7sgG7x_hjbw8t&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
20
-------------------------------------------------
2020-08-05 14:19:26
Got the results of the query
{'bib': {'abstract': 'Presently the most successful approaches to '
                     'semi-supervised learning are based on consistency '
                     'regularization, whereby a model is trained to be robust '
                     'to small perturbations of its inputs and parameters. To '
                     'understand consistency regularization, we conceptually '
                     'explore how loss geometry interacts with training '
                     'procedures. The consistency loss dramatically improves '
                     'generalization performance over supervised-only '
                     'training; however, we show that SGD struggles to '
                     'converge on the consistency loss and continues to make',
         'author': ['B Athiwaratkun', 'M Finzi', 'P Izmailov'],
         'cites': '56',
         'eprint': 'https://arxiv.org/pdf/1806.05594',
         'gsrank': '1',
         'title': 'There are many consistent explanations of unlabeled data: '
                  'Why you should average',
         'url': 'https://arxiv.org/abs/1806.05594',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=16133183473908875555&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThere%2BAre%2BMany%2BConsistent%2BExplanations%2Bof%2BUnlabeled%2BData:%2BWhy%2BYou%2BShould%2BAverage%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=I4GKauSU5N8J&ei=878qX_qDIojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:I4GKauSU5N8J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCUijSaKVNkErw1a1b8IBRPAY9thHC&scisig=AAGBfm0AAAAAXyrCUvtNe0xsw-cD8oVi0KS_61wHI83M&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
56
-------------------------------------------------
2020-08-05 14:19:38
Got the results of the query
{'bib': {'abstract': "Answerer in Questioner's Mind (AQM) is an "
                     'information-theoretic framework that has been recently '
                     'proposed for task-oriented dialog systems. AQM benefits '
                     'from asking a question that would maximize the '
                     'information gain when it is asked. However, due to its '
                     'intrinsic nature of explicitly calculating the '
                     'information gain, AQM has a limitation when the solution '
                     'space is very large. To address this, we propose AQM+ '
                     'that can deal with a large-scale problem and ask a '
                     'question that is more coherent to the current context of '
                     'the dialog. We',
         'author': ['SW Lee', 'T Gao', 'S Yang', 'J Yoo', 'JW Ha'],
         'cites': '2',
         'eprint': 'https://arxiv.org/pdf/1902.08355',
         'gsrank': '1',
         'title': "Large-Scale Answerer in Questioner's Mind for Visual Dialog "
                  'Question Generation',
         'url': 'https://arxiv.org/abs/1902.08355',
         'venue': 'arXiv preprint arXiv:1902.08355',
         'year': '2019'},
 'citations_link': '/scholar?cites=7353352535802475325&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLarge-Scale%2BAnswerer%2Bin%2BQuestioner%255C%2527s%2BMind%2Bfor%2BVisual%2BDialog%2BQuestion%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=PXO57CtbDGYJ&ei=_b8qX8v8J4jHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:PXO57CtbDGYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCWuV735qtQNn2eP21vKuFGQXChrO-&scisig=AAGBfm0AAAAAXyrCWp3EhELz7LFSISu5ogd4AaiwhwiA&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
2
-------------------------------------------------
2020-08-05 14:19:46
Got the results of the query
{'bib': {'abstract': 'Neural architecture search (NAS) automatically finds the '
                     'best task-specific neural network topology, '
                     'outperforming many manual architecture designs. However, '
                     'it can be prohibitively expensive as the search requires '
                     'training thousands of different networks, while each can '
                     'last for hours. In this work, we propose the Graph '
                     'HyperNetwork (GHN) to amortize the search cost: given an '
                     'architecture, it directly generates the weights by '
                     'running inference on a graph neural network. GHNs model '
                     'the topology of an architecture and therefore can',
         'author': ['C Zhang', 'M Ren', 'R Urtasun'],
         'cites': '61',
         'eprint': 'https://arxiv.org/pdf/1810.05749',
         'gsrank': '1',
         'title': 'Graph hypernetworks for neural architecture search',
         'url': 'https://arxiv.org/abs/1810.05749',
         'venue': 'arXiv preprint arXiv:1810.05749',
         'year': '2018'},
 'citations_link': '/scholar?cites=18192411628962893749&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGraph%2BHyperNetworks%2Bfor%2BNeural%2BArchitecture%2BSearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=tc8EMP1tePwJ&ei=BcAqX_PiBKiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:tc8EMP1tePwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCYW_2HHNfihNFEmS85Hjtm6ljBPTg&scisig=AAGBfm0AAAAAXyrCYUU2_-VUlYaz9VqmJo1ufn4ZwxqO&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
61
-------------------------------------------------
2020-08-05 14:19:53
Got the results of the query
{'bib': {'abstract': 'Transfer learning through fine-tuning a pre-trained '
                     'neural network with an extremely large dataset, such as '
                     'ImageNet, can significantly accelerate training while '
                     'the accuracy is frequently bottlenecked by the limited '
                     'dataset size of the new target task. To solve the '
                     'problem, some regularization methods, constraining the '
                     'outer layer weights of the target network using the '
                     'starting point as references (SPAR), have been studied. '
                     'In this paper, we propose a novel regularized transfer '
                     'learning framework DELTA, namely DEep Learning',
         'author': ['X Li', 'H Xiong', 'H Wang', 'Y Rao', 'L Liu', 'J Huan'],
         'cites': '18',
         'eprint': 'https://arxiv.org/pdf/1901.09229',
         'gsrank': '1',
         'title': 'Delta: Deep learning transfer using feature map with '
                  'attention for convolutional networks',
         'url': 'https://arxiv.org/abs/1901.09229',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=1065820725505324380&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDelta:%2BDeep%2BLearning%2BTransfer%2Busing%2BFeature%2BMap%2Bwith%2BAttention%2Bfor%2BConvolutional%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=XHX-iU2Oyg4J&ei=C8AqX8TjIY-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:XHX-iU2Oyg4J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCaDqKItThQxVa_MRARHQWUCERD35a&scisig=AAGBfm0AAAAAXyrCaM2dXQA0jehYm-thkIdKdYAAxPAv&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
18
-------------------------------------------------
2020-08-05 14:20:00
Got the results of the query
{'bib': {'abstract': 'We address two challenges of probabilistic topic '
                     'modelling in order to better estimate the probability of '
                     'a word in a given context, ie, P (word| context):(1) No '
                     'Language Structure in Context: Probabilistic topic '
                     'models ignore word order by summarizing a given context '
                     'as a" bag-of-word" and consequently the semantics of '
                     'words in the context is lost. The LSTM-LM learns a '
                     'vector-space representation of each word by accounting '
                     'for word order in local collocation patterns and models '
                     'complex characteristics of language (eg, syntax and',
         'author': ['P Gupta', 'Y Chaudhary', 'F Buettner'],
         'cites': '4',
         'eprint': 'https://arxiv.org/pdf/1810.03947',
         'gsrank': '1',
         'title': 'Texttovec: Deep contextualized neural autoregressive topic '
                  'models of language with distributed compositional prior',
         'url': 'https://arxiv.org/abs/1810.03947',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=16604775897027080889&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTexttovec:%2BDeep%2BContextualized%2BNeural%2Bautoregressive%2BTopic%2BModels%2Bof%2BLanguage%2Bwith%2BDistributed%2BCompositional%2BPrior%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=uXJE1bMDcOYJ&ei=FcAqX9bfKaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:uXJE1bMDcOYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCcBfYQzdIg8i2Aeiqxyw1DOCVdGW6&scisig=AAGBfm0AAAAAXyrCcBwMVpT6sGjhdnfekPGvZz-_th5U&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
4
-------------------------------------------------
2020-08-05 14:20:08
Got the results of the query
{'bib': {'abstract': 'Meta-learning, or learning-to-learn, has proven to be a '
                     'successful strategy in attacking problems in supervised '
                     'learning and reinforcement learning that involve small '
                     'amounts of data. State-of-the-art solutions involve '
                     'learning an initialization and/or learning algorithm',
         'author': ['S Ravi', 'A Beatson'],
         'cites': '23',
         'eprint': 'https://openreview.net/pdf?id=rkgpy3C5tX',
         'gsrank': '1',
         'title': 'Amortized bayesian meta-learning',
         'url': 'https://openreview.net/forum?id=rkgpy3C5tX&noteId=rkgpy3C5tX',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1798581693339493143&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAmortized%2BBayesian%2BMeta-Learning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=F2MFU4XY9RgJ&ei=HcAqX7O6M6OGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:F2MFU4XY9RgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCe7tSiLCnIDbysWSr3l-6jULGAxLz&scisig=AAGBfm0AAAAAXyrCe7dOrYpQaG9PYxmryxumnKkAKzow&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
23
-------------------------------------------------
2020-08-05 14:20:19
Got the results of the query
{'bib': {'abstract': 'Humans are capable of attributing latent mental contents '
                     'such as beliefs or intentions to others. The social '
                     'skill is critical in daily life for reasoning about the '
                     "potential consequences of others' behaviors so as to "
                     'plan ahead. It is known that humans use such reasoning '
                     'ability recursively by considering what others believe '
                     'about their own beliefs. In this paper, we start from '
                     'level-$1 $ recursion and introduce a probabilistic '
                     'recursive reasoning (PR2) framework for multi-agent '
                     'reinforcement learning. Our hypothesis is that it is '
                     'beneficial for each agent to',
         'author': ['Y Wen', 'Y Yang', 'R Luo', 'J Wang', 'W Pan'],
         'cites': '26',
         'eprint': 'https://arxiv.org/pdf/1901.09207',
         'gsrank': '1',
         'title': 'Probabilistic recursive reasoning for multi-agent '
                  'reinforcement learning',
         'url': 'https://arxiv.org/abs/1901.09207',
         'venue': 'arXiv preprint arXiv:1901.09207',
         'year': '2019'},
 'citations_link': '/scholar?cites=12761322458577853549&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProbabilistic%2BRecursive%2BReasoning%2Bfor%2BMulti-Agent%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=bVy_5ylTGbEJ&ei=KMAqX4ehFoyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:bVy_5ylTGbEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrChkLdMMaymcw9187tkKA7nviqv4mJ&scisig=AAGBfm0AAAAAXyrChvF5xpRS81Pn2CG_aCYa-na0aO0m&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
26
-------------------------------------------------
2020-08-05 14:20:31
Got the results of the query
{'bib': {'abstract': 'Partial differential equations (PDEs) are widely used '
                     'across the physical and computational sciences. Decades '
                     'of research and engineering went into designing fast '
                     'iterative solution methods. Existing solvers are general '
                     'purpose, but may be sub-optimal for specific classes of '
                     'problems. In contrast to existing hand-crafted '
                     'solutions, we propose an approach to learn a fast '
                     'iterative solver tailored to a specific domain. We '
                     'achieve this goal by learning to modify the updates of '
                     'an existing solver using a deep neural network. '
                     'Crucially, our',
         'author': ['JT Hsieh', 'S Zhao', 'S Eismann', 'L Mirabella'],
         'cites': '25',
         'eprint': 'https://arxiv.org/pdf/1906.01200',
         'gsrank': '1',
         'title': 'Learning neural PDE solvers with convergence guarantees',
         'url': 'https://arxiv.org/abs/1906.01200',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=12683471981515520201&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BNeural%2BPDE%2BSolvers%2Bwith%2BConvergence%2BGuarantees%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=yTjaOJG-BLAJ&ei=OMAqX9abE6OGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:yTjaOJG-BLAJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrClmM3dKPYpiedIadJvsye5ZDairCh&scisig=AAGBfm0AAAAAXyrClm1OgJikF8RtxzPB6yTsyi2zNoKh&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
25
-------------------------------------------------
2020-08-05 14:20:46
Got the results of the query
{'bib': {'abstract': 'This paper introduces a novel framework for learning '
                     'algorithms to solve online combinatorial optimization '
                     'problems. Towards this goal, we introduce a number of '
                     'key ideas from traditional algorithms and complexity '
                     'theory. First, we draw a new connection between '
                     'primal-dual methods and reinforcement learning. Next, we '
                     'introduce the concept of adversarial distributions '
                     '(universal and high-entropy training sets), which are '
                     'distributions that encourage the learner to find '
                     'algorithms that work well in the worst case. We test our',
         'author': ['W Kong', 'C Liaw', 'A Mehta'],
         'cites': '5',
         'eprint': 'https://openreview.net/pdf?id=rkluJ2R9KQ',
         'gsrank': '1',
         'title': 'A new dog learns old tricks: RL finds classic optimization '
                  'algorithms',
         'url': 'https://openreview.net/forum?id=rkluJ2R9KQ',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=12269274176119395887&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2Bnew%2Bdog%2Blearns%2Bold%2Btricks:%2BRL%2Bfinds%2Bclassic%2Boptimization%2Balgorithms%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=L1Z4l9s3RaoJ&ei=QcAqX4qAEpqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:L1Z4l9s3RaoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCnTUUtUhxYL1yEMzv4xwo9aiu2Rmd&scisig=AAGBfm0AAAAAXyrCnVCCnM9rgt_qCQYFn8yec8Aauc-Q&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 14:20:53
Got the results of the query
{'bib': {'abstract': 'We adapt the approach of Deep Infomax (DIM), which '
                     'relies on maxi- mizing mutual information between patch '
                     'representations and high-level summaries, to the graph '
                     'domain. The resulting technique, Deep Graph Infomax '
                     '(DGI), demonstrates com- petitive performance on node',
         'author': ['P Velickovic', 'W Fedus', 'WL Hamilton'],
         'cites': '122',
         'gsrank': '1',
         'title': 'Deep Graph Infomax.',
         'url': 'http://postersession.ai.s3.amazonaws.com/27d3e2e5-6a65-4a1b-8f57-ce3b35df3e08.pdf',
         'venue': 'ICLR …',
         'year': '2019'},
 'citations_link': '/scholar?cites=6675561854020696633&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BGraph%2BInfomax%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=OToNOSNcpFwJ&ei=SMAqX7LALoyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:OToNOSNcpFwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCqc1sdyZPTasOEsj7J8QPDmZnkdGg&scisig=AAGBfm0AAAAAXyrCqWN2YyYzzdxWk7xCKBdUB1HlUonZ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
122
-------------------------------------------------
2020-08-05 14:21:05
Got the results of the query
{'bib': {'abstract': 'Batch Normalization (BN) has become a cornerstone of '
                     'deep learning across diverse architectures, appearing to '
                     'help optimization as well as generalization. While the '
                     'idea makes intuitive sense, theoretical analysis of its '
                     'effectiveness has been lacking. Here theoretical support '
                     'is provided for one of its conjectured properties, '
                     'namely, the ability to allow gradient descent to succeed '
                     'with less tuning of learning rates. It is shown that '
                     'even if we fix the learning rate of scale-invariant '
                     'parameters (eg, weights of each layer with BN) to a',
         'author': ['S Arora', 'Z Li', 'K Lyu'],
         'cites': '24',
         'eprint': 'https://arxiv.org/pdf/1812.03981',
         'gsrank': '1',
         'title': 'Theoretical analysis of auto rate-tuning by batch '
                  'normalization',
         'url': 'https://arxiv.org/abs/1812.03981',
         'venue': 'arXiv preprint arXiv:1812.03981',
         'year': '2018'},
 'citations_link': '/scholar?cites=12820662183792985320&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTheoretical%2BAnalysis%2Bof%2BAuto%2BRate-Tuning%2Bby%2BBatch%2BNormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=6MRDUlQk7LEJ&ei=WMAqX9W0OIvrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:6MRDUlQk7LEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCtZxTmzwTpwjCXzEe2jixF2ohhvhg&scisig=AAGBfm0AAAAAXyrCtSHafz1ghX80HiRIghxK3P7Kioal&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
24
-------------------------------------------------
2020-08-05 14:21:17
Got the results of the query
{'bib': {'abstract': 'The high computational and parameter complexity of '
                     'neural networks makes their training very slow and '
                     'difficult to deploy on energy and storage-constrained '
                     'computing systems. Many network complexity reduction '
                     'techniques have been proposed including fixed-point '
                     'implementation. However, a systematic approach for '
                     'designing full fixed-point training and inference of '
                     'deep neural networks remains elusive. We describe a '
                     'precision assignment methodology for neural network '
                     'training in which all network parameters, ie, '
                     'activations and',
         'author': ['C Sakr', 'N Shanbhag'],
         'cites': '16',
         'eprint': 'https://arxiv.org/pdf/1812.11732',
         'gsrank': '1',
         'title': 'Per-tensor fixed-point quantization of the back-propagation '
                  'algorithm',
         'url': 'https://arxiv.org/abs/1812.11732',
         'venue': 'arXiv preprint arXiv:1812.11732',
         'year': '2018'},
 'citations_link': '/scholar?cites=14777632566024300455&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPer-Tensor%2BFixed-Point%2BQuantization%2Bof%2Bthe%2BBack-Propagation%2BAlgorithm%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=p2cfB4ayFM0J&ei=Y8AqX_i6E5qGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:p2cfB4ayFM0J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCv5Z-dEm4Who0tGW4-Ryy9AliAIUk&scisig=AAGBfm0AAAAAXyrCvx63nnLQoLteEtLAQXPYko2dxkGg&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
16
-------------------------------------------------
2020-08-05 14:21:28
Got the results of the query
{'bib': {'abstract': 'Variational Bayesian neural networks (BNNs) perform '
                     'variational inference over weights, but it is difficult '
                     'to specify meaningful priors and approximate posteriors '
                     'in a high-dimensional weight space. We introduce '
                     'functional variational Bayesian neural networks (fBNNs), '
                     'which maximize an Evidence Lower BOund (ELBO) defined '
                     'directly on stochastic processes, ie distributions over '
                     'functions. We prove that the KL divergence between '
                     'stochastic processes equals the supremum of marginal KL '
                     'divergences over all finite sets of inputs. Based on '
                     'this',
         'author': ['S Sun', 'G Zhang', 'J Shi', 'R Grosse'],
         'cites': '51',
         'eprint': 'https://arxiv.org/pdf/1903.05779',
         'gsrank': '1',
         'title': 'Functional variational bayesian neural networks',
         'url': 'https://arxiv.org/abs/1903.05779',
         'venue': 'arXiv preprint arXiv:1903.05779',
         'year': '2019'},
 'citations_link': '/scholar?cites=11345668122445712961&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFunctional%2Bvariational%2BBayesian%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=QY7ZoQvpc50J&ei=bcAqX-eRLYyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:QY7ZoQvpc50J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrCy4rVkj1qtp2Y4ydu4NsUA2oVb_Rm&scisig=AAGBfm0AAAAAXyrCywoTtf-3KtNPWNwzpRFiuM-SUiiF&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
51
-------------------------------------------------
2020-08-05 14:21:39
Got the results of the query
{'bib': {'abstract': 'Reinforcement learning agents need exploratory behaviors '
                     'to escape from local optima. These behaviors may include '
                     'both immediate dithering perturbation and temporally '
                     'consistent exploration. To achieve these, a stochastic '
                     'policy model that is inherently consistent through a '
                     'period of time is in desire, especially for tasks with '
                     'either sparse rewards or long term information. In this '
                     'work, we introduce a novel on-policy temporally '
                     'consistent exploration strategy-Neural Adaptive Dropout '
                     'Policy Exploration (NADPEx)-for',
         'author': ['S Xie', 'J Huang', 'L Lei', 'C Liu', 'Z Ma', 'W Zhang'],
         'cites': '2',
         'eprint': 'https://arxiv.org/pdf/1812.09028',
         'gsrank': '1',
         'title': 'NADPEx: An on-policy temporally consistent exploration '
                  'method for deep reinforcement learning',
         'url': 'https://arxiv.org/abs/1812.09028',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=16547616825713719585&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNADPEx:%2BAn%2Bon-policy%2Btemporally%2Bconsistent%2Bexploration%2Bmethod%2Bfor%2Bdeep%2Breinforcement%2Blearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=IXnxd9TxpOUJ&ei=esAqX9bYBKiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:IXnxd9TxpOUJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrC14bXnuQwWPuZV-USww_BYfw9sJtH&scisig=AAGBfm0AAAAAXyrC1y1p6tfG2HoDLYCtw1kr27pxcJHB&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
2
-------------------------------------------------
2020-08-05 14:21:51
Got the results of the query
{'bib': {'abstract': 'Deep Learning for Computer Vision depends mainly on the '
                     'source of supervision. Photo-realistic simulators can '
                     'generate large-scale automatically labeled '
                     'syntheticdata, but introduce a domain gap negatively '
                     'impacting performance. We propose anew unsupervised '
                     'domain adaptation algorithm, called SPIGAN, relying on '
                     'Sim-ulator Privileged Information (PI) and Generative '
                     'Adversarial Networks (GAN). We use internal data from '
                     'the simulator as PI during the training of a target '
                     'tasknetwork. We experimentally evaluate our approach on',
         'author': ['KH Lee', 'G Ros', 'J Li', 'A Gaidon'],
         'cites': '23',
         'eprint': 'https://arxiv.org/pdf/1810.03756',
         'gsrank': '1',
         'title': 'Spigan: Privileged adversarial learning from simulation',
         'url': 'https://arxiv.org/abs/1810.03756',
         'venue': 'arXiv preprint arXiv:1810.03756',
         'year': '2018'},
 'citations_link': '/scholar?cites=4636284905704356497&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSPIGAN:%2BPrivileged%2BAdversarial%2BLearning%2Bfrom%2BSimulation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=kXoDwo5kV0AJ&ei=g8AqX9_hNI-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:kXoDwo5kV0AJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrC4oHVjz1x_btv3aQ88Awg6Gp6lLy4&scisig=AAGBfm0AAAAAXyrC4lWpnnxwsGX5JU-wBVz9P4TUO3HZ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
23
-------------------------------------------------
2020-08-05 14:22:02
Got the results of the query
{'bib': {'abstract': 'We study the problem of training sequential generative '
                     'models for capturing coordinated multi-agent trajectory '
                     'behavior, such as offensive basketball gameplay. When '
                     'modeling such settings, it is often beneficial to design '
                     'hierarchical models that can capture long-term '
                     'coordination using intermediate variables. Furthermore, '
                     'these intermediate variables should capture interesting '
                     'high-level behavioral semantics in an interpretable and '
                     'manipulatable way. We present a hierarchical framework '
                     'that can effectively learn such sequential',
         'author': ['E Zhan', 'S Zheng', 'Y Yue', 'L Sha', 'P Lucey'],
         'cites': '15',
         'eprint': 'https://arxiv.org/pdf/1803.07612',
         'gsrank': '1',
         'title': 'Generating multi-agent trajectories using programmatic weak '
                  'supervision',
         'url': 'https://arxiv.org/abs/1803.07612',
         'venue': 'arXiv preprint arXiv:1803.07612',
         'year': '2018'},
 'citations_link': '/scholar?cites=18317571777385401636&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerating%2BMulti-Agent%2BTrajectories%2Busing%2BProgrammatic%2BWeak%2BSupervision%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=JA3HGnsWNf4J&ei=kMAqX53PH8KwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:JA3HGnsWNf4J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrC7SwkaiQHEvPw8Yod1BEXZbg8QZnd&scisig=AAGBfm0AAAAAXyrC7ZBcPJtLUrCvZ0w3hZEJOeDyO-rJ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 14:22:13
Got the results of the query
{'bib': {'abstract': 'We present a deep learning-based method for '
                     'super-resolving coarse (low-resolution) labels assigned '
                     'to groups of image pixels into pixel-level '
                     '(high-resolution) labels, given the joint distribution '
                     'between those low-and high-resolution labels. This '
                     'method involves a',
         'author': ['K Malkin', 'C Robinson', 'L Hou', 'R Soobitsky'],
         'cites': '8',
         'gsrank': '1',
         'title': 'Label super-resolution networks',
         'url': 'https://openreview.net/forum?id=rkxwShA9Ym',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=632480761848779697&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLabel%2Bsuper-resolution%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=sdND4eYFxwgJ&ei=mcAqX7rhFojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:sdND4eYFxwgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrC-XZy5UgwDnItWfqafaSG8xn-Bdry&scisig=AAGBfm0AAAAAXyrC-T2jFGYQn1yeEk9vGynZ4LhjKXab&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
8
-------------------------------------------------
2020-08-05 14:22:25
Got the results of the query
{'bib': {'abstract': 'Distributed optimization is vital in solving large-scale '
                     'machine learning problems. A widely-shared feature of '
                     'distributed optimization techniques is the requirement '
                     'that all nodes complete their assigned tasks in each '
                     'computational epoch before the system can proceed to the '
                     'next epoch. In such settings, slow nodes, called '
                     'stragglers, can greatly slow progress. To mitigate the '
                     'impact of stragglers, we propose an online distributed '
                     'optimization method called Anytime Minibatch. In this '
                     'approach, all nodes are given a fixed time to compute '
                     'the',
         'author': ['N Ferdinand', 'H Al-Lawati', 'SC Draper'],
         'cites': '8',
         'eprint': 'https://arxiv.org/pdf/2006.05752',
         'gsrank': '1',
         'title': 'Anytime minibatch: Exploiting stragglers in online '
                  'distributed optimization',
         'url': 'https://arxiv.org/abs/2006.05752',
         'venue': 'arXiv preprint arXiv …',
         'year': '2020'},
 'citations_link': '/scholar?cites=1554485746213937113&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAnytime%2BMinibatch:%2BExploiting%2BStragglers%2Bin%2BOnline%2BDistributed%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=2V9914ykkhUJ&ei=psAqX5DSB4yimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:2V9914ykkhUJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrDAUF58jArQkN7bXLQR8_gl1pQB5Ol&scisig=AAGBfm0AAAAAXyrDAZxEzIvO_w5BjrMAD37ZLo_upNWs&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
8
-------------------------------------------------
2020-08-05 14:22:33
Got the results of the query
{'bib': {'abstract': 'We present a meta-learning approach for adaptive '
                     'text-to-speech (TTS) with few data. During training, we '
                     'learn a multi-speaker model using a shared conditional '
                     'WaveNet core and independent learned embeddings for each '
                     'speaker. The aim of training is not to',
         'author': ['Y Chen', 'Y Assael', 'B Shillingford', 'D Budden'],
         'cites': '35',
         'eprint': 'https://arxiv.org/pdf/1809.10460',
         'gsrank': '1',
         'title': 'Sample efficient adaptive text-to-speech',
         'url': 'https://arxiv.org/abs/1809.10460',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=14180263255450614943&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSample%2BEfficient%2BAdaptive%2BText-to-Speech%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=n3yo8kxqysQJ&ei=scAqX66gDrGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:n3yo8kxqysQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrDD-IcKo_6zFK2cwZgVuXg8JY6Kb3V&scisig=AAGBfm0AAAAAXyrDD2DVgubg42mAvmHiwhmFyzozYicN&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
35
-------------------------------------------------
2020-08-05 14:22:48
Got the results of the query
{'bib': {'abstract': 'Deep latent variable models have seen recent success in '
                     'many data domains. Lossless compression is an '
                     'application of these models which, despite having the '
                     'potential to be highly useful, has yet to be implemented '
                     'in a practical manner. We presentBits Back with '
                     "ANS'(BB-ANS), a scheme to perform lossless compression "
                     'with latent variable models at a near optimal rate. We '
                     'demonstrate this scheme by using it to compress the '
                     'MNIST dataset with a variational auto-encoder model '
                     '(VAE), achieving compression rates superior to',
         'author': ['J Townsend', 'T Bird', 'D Barber'],
         'cites': '17',
         'eprint': 'https://arxiv.org/pdf/1901.04866',
         'gsrank': '1',
         'title': 'Practical lossless compression with latent variables using '
                  'bits back coding',
         'url': 'https://arxiv.org/abs/1901.04866',
         'venue': 'arXiv preprint arXiv:1901.04866',
         'year': '2019'},
 'citations_link': '/scholar?cites=1443052248345328520&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPractical%2Blossless%2Bcompression%2Bwith%2Blatent%2Bvariables%2Busing%2Bbits%2Bback%2Bcoding%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=iLvzzl_ABhQJ&ei=vsAqX7uZC6iBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:iLvzzl_ABhQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrDG8RTPJ931Xcot2laakJvyfhpl6mC&scisig=AAGBfm0AAAAAXyrDG4_-zqMmNl3MDJ9TuKZith1_NO5-&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
17
-------------------------------------------------
2020-08-05 14:22:59
Got the results of the query
-------------------------------------------------
2020-08-05 14:23:59
Trying new proxy
Working proxy: http://85.47.31.179:3128
Trying new proxy
Working proxy: http://85.47.31.179:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://34.217.107.252:8888
Trying new proxy
Working proxy: http://84.210.183.61:3128
Got the results of the query
{'bib': {'abstract': 'In order to choose a neural network architecture that '
                     'will be effective for a particular modeling problem, one '
                     'must understand the limitations imposed by each of the '
                     'potential options. These limitations are typically '
                     'described in terms of information theoretic bounds, or '
                     'by comparing the relative complexity needed to '
                     'approximate example functions between different '
                     'architectures. In this paper, we examine the topological '
                     'constraints that the architecture of a neural network '
                     'imposes on the level sets of all the functions that it '
                     'is able to',
         'author': ['J Johnson'],
         'cites': '7',
         'eprint': 'https://arxiv.org/pdf/1810.00393',
         'gsrank': '1',
         'title': 'Deep, skinny neural networks are not universal '
                  'approximators',
         'url': 'https://arxiv.org/abs/1810.00393',
         'venue': 'arXiv preprint arXiv:1810.00393',
         'year': '2018'},
 'citations_link': '/scholar?cites=12423471448848584321&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep,%2BSkinny%2BNeural%2BNetworks%2Bare%2Bnot%2BUniversal%2BApproximators%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=gc7T2XUJaawJ&ei=08QqX6bDIMKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:gc7T2XUJaawJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrHL3zun2jW6ohk0jahVVx69KJmMwGd&scisig=AAGBfm0AAAAAXyrHL_io3P3VtC_QEWN44YJ9GNy9U_ed&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
7
-------------------------------------------------
2020-08-05 14:40:23
Got the results of the query
{'bib': {'abstract': 'Graphs are a prevalent tool in data science, as they '
                     'model the inherent structure of the data. They have been '
                     'used successfully in unsupervised and semi-supervised '
                     'learning. Typically they are constructed either by '
                     'connecting nearest samples, or by learning them from '
                     'data, solving an optimization problem. While graph '
                     'learning does achieve a better quality, it also comes '
                     'with a higher computational cost. In particular, the '
                     'current state-of-the-art model cost is $\\mathcal {O}(n^ '
                     '2) $ for $ n $ samples. In this paper, we show how to '
                     'scale it, obtaining',
         'author': ['V Kalofolias', 'N Perraudin'],
         'cites': '16',
         'eprint': 'https://arxiv.org/pdf/1710.05654',
         'gsrank': '1',
         'title': 'Large scale graph learning from smooth signals',
         'url': 'https://arxiv.org/abs/1710.05654',
         'venue': 'arXiv preprint arXiv:1710.05654',
         'year': '2017'},
 'citations_link': '/scholar?cites=300846958242643752&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLarge%2BScale%2BGraph%2BLearning%2BFrom%2BSmooth%2BSignals%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=KD_P7bbSLAQJ&ei=2sQqX8y5KIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:KD_P7bbSLAQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrHN4dq-5lzl1ySVGc9PkwLalh2LVTp&scisig=AAGBfm0AAAAAXyrHNyLm8wfyWUmIncErcT2HsQwofJtR&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
16
-------------------------------------------------
2020-08-05 14:40:32
Got the results of the query
{'bib': {'abstract': 'Learning multiple tasks sequentially is important for '
                     'the development of AI and lifelong learning systems. '
                     'However, standard neural network architectures suffer '
                     'from catastrophic forgetting which makes it difficult '
                     'for them to learn a sequence of tasks. Several continual '
                     'learning methods have been proposed to address the '
                     'problem. In this paper, we propose a very different '
                     'approach, called Parameter Generation and Model '
                     'Adaptation (PGMA), to dealing with the problem. The '
                     'proposed approach learns to build a model, called the '
                     'solver',
         'author': ['W Hu', 'Z Lin', 'B Liu', 'C Tao', 'Z Tao', 'J Ma'],
         'cites': '20',
         'eprint': 'https://openreview.net/pdf?id=ryGvcoA5YX',
         'gsrank': '1',
         'title': 'Overcoming catastrophic forgetting for continual learning '
                  'via model adaptation',
         'url': 'https://openreview.net/forum?id=ryGvcoA5YX',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=17192867455621921103&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOvercoming%2BCatastrophic%2BForgetting%2Bfor%2BContinual%2BLearning%2Bvia%2BModel%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=T_m38ttVme4J&ei=4sQqX6XIJIvrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:T_m38ttVme4J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrHPlCLuIjRxFM_Vqu9syU--8JoHhlT&scisig=AAGBfm0AAAAAXyrHPh-J54FnnSitVLGuA2GNzDO_3L2_&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
20
-------------------------------------------------
2020-08-05 14:40:38
Got the results of the query
{'bib': {'abstract': 'Pyruvate dehydrogenase complex (PDC) is one of the '
                     'largest multienzyme complexes known and consists of a '
                     'dodecahedral E2 core to which other components are '
                     'attached. We report the results of applying a new '
                     'computational method, quantized elastic deformational',
         'author': ['Y Kong', 'D Ming', 'Y Wu', 'JK Stoops', 'ZH Zhou'],
         'cites': '36',
         'gsrank': '1',
         'title': 'Conformational flexibility of pyruvate dehydrogenase '
                  'complexes: a computational analysis by quantized elastic '
                  'deformational model',
         'url': 'https://www.sciencedirect.com/science/article/pii/S0022283603005552',
         'venue': 'Journal of molecular …',
         'year': '2003'},
 'citations_link': '/scholar?cites=4554748476350687757&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAnalysis%2Bof%2BQuantized%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=DWoibpu3NT8J&ei=68QqX8jfHrGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:DWoibpu3NT8J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrHSuKswhRrJWl8k6BRBzQVaOYA1JP3&scisig=AAGBfm0AAAAAXyrHStCsfu_Qp7bifBr55MpNcn9V6_wD&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
36
-------------------------------------------------
2020-08-05 14:40:50
Got the results of the query
-------------------------------------------------
2020-08-05 14:42:00
Got the results of the query
{'bib': {'abstract': 'Learning when to communicate and doing that effectively '
                     'is essential in multi-agent tasks. Recent works show '
                     'that continuous communication allows efficient training '
                     'with back-propagation in multi-agent scenarios, but have '
                     'been restricted to fully-cooperative tasks. In this '
                     'paper, we present Individualized Controlled Continuous '
                     'Communication Model (IC3Net) which has better training '
                     'efficiency than simple continuous communication model, '
                     'and can be applied to semi-cooperative and competitive '
                     'settings along with the cooperative settings',
         'author': ['A Singh', 'T Jain', 'S Sukhbaatar'],
         'cites': '35',
         'eprint': 'https://arxiv.org/pdf/1812.09755',
         'gsrank': '1',
         'title': 'Learning when to communicate at scale in multiagent '
                  'cooperative and competitive tasks',
         'url': 'https://arxiv.org/abs/1812.09755',
         'venue': 'arXiv preprint arXiv:1812.09755',
         'year': '2018'},
 'citations_link': '/scholar?cites=12298395236200633957&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bwhen%2Bto%2BCommunicate%2Bat%2BScale%2Bin%2BMultiagent%2BCooperative%2Band%2BCompetitive%2BTasks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ZVYYwU6trKoJ&ei=P8UqX4amOsKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ZVYYwU6trKoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrHnjhRA6JeNymtLtaYXEDTpy5jAY1O&scisig=AAGBfm0AAAAAXyrHnonUS-HMyy9XbRAwe6mbOaP1k-22&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
35
-------------------------------------------------
2020-08-05 14:42:15
Got the results of the query
{'bib': {'abstract': 'The goal of program synthesis is to automatically '
                     'generate programs in a particular language from '
                     'corresponding specifications, eg input-output behavior. '
                     'Many current approaches achieve impressive results after '
                     'training on randomly generated I/O examples in limited '
                     'domain-specific languages (DSLs), as with string '
                     'transformations in RobustFill. However, we empirically '
                     'discover that applying test input generation techniques '
                     'for languages with control flow and rich input space '
                     'causes deep networks to generalize poorly',
         'author': ['R Shin', 'N Kant', 'K Gupta', 'C Bender', 'B Trabucco'],
         'cites': '6',
         'eprint': 'https://arxiv.org/pdf/1912.12345',
         'gsrank': '1',
         'title': 'Synthetic datasets for neural program synthesis',
         'url': 'https://arxiv.org/abs/1912.12345',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=7991169696227265299&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSynthetic%2BDatasets%2Bfor%2BNeural%2BProgram%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Ewt-639W5m4J&ei=TcUqX-zGKIjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Ewt-639W5m4J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrHq0_JyHvBHkkp0gdD9jPMpF99-Hp2&scisig=AAGBfm0AAAAAXyrHq9QImu87_F0R-IalC6YiV3n16QcU&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 14:42:27
Got the results of the query
{'bib': {'abstract': 'Multiview stereo aims to reconstruct scene depth from '
                     'images acquired by a camera under arbitrary motion. '
                     'Recent methods address this problem through deep '
                     'learning, which can utilize semantic cues to deal with '
                     'challenges such as textureless and reflective regions. '
                     'In this paper, we present a convolutional neural network '
                     'called DPSNet (Deep Plane Sweep Network) whose design is '
                     'inspired by best practices of traditional geometry-based '
                     'approaches for dense depth reconstruction. Rather than '
                     'directly estimating depth and/or',
         'author': ['S Im', 'HG Jeon', 'S Lin', 'IS Kweon'],
         'cites': '36',
         'eprint': 'https://arxiv.org/pdf/1905.00538',
         'gsrank': '1',
         'title': 'DPSNet: End-to-end deep plane sweep stereo',
         'url': 'https://arxiv.org/abs/1905.00538',
         'venue': 'arXiv preprint arXiv:1905.00538',
         'year': '2019'},
 'citations_link': '/scholar?cites=11110225942792064313&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDPSNet:%2BEnd-to-end%2BDeep%2BPlane%2BSweep%2BStereo%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=OU1zZqFzL5oJ&ei=V8UqX6vSHKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:OU1zZqFzL5oJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrHtd8Zrt2hf4mNH7T6peZb6Xmc1uIt&scisig=AAGBfm0AAAAAXyrHtdkUmPnPvHLTkGn0W_zY3DQT2-cI&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
36
-------------------------------------------------
2020-08-05 14:42:38
Got the results of the query
{'bib': {'abstract': 'Network Embeddings (NEs) map the nodes of a given '
                     'network into $ d $-dimensional Euclidean space $\\mathbb '
                     '{R}^ d $. Ideally, this mapping is such '
                     "thatsimilar'nodes are mapped onto nearby points, such "
                     'that the NE can be used for purposes such as link',
         'author': ['B Kang', 'J Lijffijt', 'T De Bie'],
         'cites': '9',
         'eprint': 'https://arxiv.org/pdf/1805.07544',
         'gsrank': '1',
         'title': 'Conditional network embeddings',
         'url': 'https://arxiv.org/abs/1805.07544',
         'venue': 'arXiv preprint arXiv:1805.07544',
         'year': '2018'},
 'citations_link': '/scholar?cites=11198262175221248724&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DConditional%2BNetwork%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=1HLBDx44aJsJ&ei=Y8UqX4qAII-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:1HLBDx44aJsJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrHywQL6rcwJhAf1SgBlQ_OSszylxIT&scisig=AAGBfm0AAAAAXyrHywpZk-bEabcLLB6UUbQvtzKGBe4o&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 14:43:00
Got the results of the query
{'bib': {'abstract': 'Neural network quantization is becoming an industry '
                     'standard to efficiently deploy deep learning models on '
                     'hardware platforms, such as CPU, GPU, TPU, and FPGAs. '
                     'However, we observe that the conventional quantization '
                     'approaches are vulnerable to adversarial attacks. This '
                     "paper aims to raise people's awareness about the "
                     'security of the quantized models, and we designed a '
                     'novel quantization methodology to jointly optimize the '
                     'efficiency and robustness of deep learning models. We '
                     'first conduct an empirical study to show that',
         'author': ['J Lin', 'C Gan', 'S Han'],
         'cites': '50',
         'eprint': 'https://arxiv.org/pdf/1904.08444',
         'gsrank': '1',
         'title': 'Defensive quantization: When efficiency meets robustness',
         'url': 'https://arxiv.org/abs/1904.08444',
         'venue': 'arXiv preprint arXiv:1904.08444',
         'year': '2019'},
 'citations_link': '/scholar?cites=341973308993338144&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDefensive%2BQuantization:%2BWhen%2BEfficiency%2BMeets%2BRobustness%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=IHs2penuvgQJ&ei=ecUqX4P3NYvrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:IHs2penuvgQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrH2iE4T2hOIQ_BZXgXrHW6rsmA_K0P&scisig=AAGBfm0AAAAAXyrH2s1CtGYqtixJdqHqRsa6TFVt4KVX&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
50
-------------------------------------------------
2020-08-05 14:43:14
Got the results of the query
{'bib': {'abstract': 'Within many machine learning algorithms, a fundamental '
                     'problem concerns efficient calculation of an unbiased '
                     'gradient wrt parameters $\\gammav $ for '
                     'expectation-based objectives $\\Ebb_ {q_ '
                     '{\\gammav}(\\yv)}[f (\\yv)] $. Most existing methods '
                     'either (i) suffer from high variance, seeking help from '
                     '(often) complicated variance-reduction techniques; or '
                     '(ii) they only apply to reparameterizable continuous '
                     'random variables and employ a reparameterization trick. '
                     'To address these limitations, we propose a General and '
                     'One',
         'author': ['Y Cong', 'M Zhao', 'K Bai', 'L Carin'],
         'cites': '7',
         'eprint': 'https://arxiv.org/pdf/1901.06020',
         'gsrank': '1',
         'title': 'GO gradient for expectation-based objectives',
         'url': 'https://arxiv.org/abs/1901.06020',
         'venue': 'arXiv preprint arXiv:1901.06020',
         'year': '2019'},
 'citations_link': '/scholar?cites=13295613950307692271&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGO%2BGradient%2Bfor%2BExpectation-Based%2BObjectives%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=727R0nGCg7gJ&ei=h8UqX9KpOZqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:727R0nGCg7gJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrH4kOmGoJ8bRHIvOSgZQecSWSXhxXD&scisig=AAGBfm0AAAAAXyrH4kc76-fT-EnY043W1YLXK5qCyNU-&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
7
-------------------------------------------------
2020-08-05 14:43:22
Got the results of the query
{'bib': {'abstract': 'Recurrent neural networks are known for their notorious '
                     'exploding and vanishing gradient problem (EVGP). This '
                     'problem becomes more evident in tasks where the '
                     'information needed to correctly solve them exist over '
                     'long time scales, because EVGP prevents',
         'author': ['D Arpit', 'B Kanuparthi', 'G Kerg', 'NR Ke'],
         'cites': '6',
         'eprint': 'https://arxiv.org/pdf/1810.03023',
         'gsrank': '1',
         'title': 'h-detach: Modifying the lstm gradient towards better '
                  'optimization',
         'url': 'https://arxiv.org/abs/1810.03023',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=762520068872474914&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3Dh-detach:%2BModifying%2Bthe%2BLSTM%2BGradient%2BTowards%2BBetter%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=IhFAb_YDlQoJ&ei=jMUqX8GRMovrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:IhFAb_YDlQoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrH6J0OuHBEseOrO2vKPa9CR6LOW039&scisig=AAGBfm0AAAAAXyrH6IrGDPDGP0ifSipGBRGcn9NKKWs6&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 14:43:28
Got the results of the query
{'bib': {'abstract': 'Much attention has been devoted recently to the '
                     'generalization puzzle in deep learning: large, deep '
                     'networks can generalize well, but existing theories '
                     'bounding generalization error are exceedingly loose, and '
                     'thus cannot explain this striking performance. '
                     'Furthermore, a major hope is that knowledge may transfer '
                     'across tasks, so that multi-task learning can improve '
                     'generalization on individual tasks. However we lack '
                     'analytic theories that can quantitatively predict how '
                     'the degree of knowledge transfer depends on the '
                     'relationship',
         'author': ['AK Lampinen', 'S Ganguli'],
         'cites': '30',
         'eprint': 'https://arxiv.org/pdf/1809.10374',
         'gsrank': '1',
         'title': 'An analytic theory of generalization dynamics and transfer '
                  'learning in deep linear networks',
         'url': 'https://arxiv.org/abs/1809.10374',
         'venue': 'arXiv preprint arXiv:1809.10374',
         'year': '2018'},
 'citations_link': '/scholar?cites=358108946105258258&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2Banalytic%2Btheory%2Bof%2Bgeneralization%2Bdynamics%2Band%2Btransfer%2Blearning%2Bin%2Bdeep%2Blinear%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ElF-azBC-AQJ&ei=l8UqX_T_LYyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ElF-azBC-AQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrH9G0V8ygWL16UvCvNOgYP2mmzrLp5&scisig=AAGBfm0AAAAAXyrH9BnsKUOATQrBvDrFXEtfC4kE3_uA&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
30
-------------------------------------------------
2020-08-05 14:43:40
Got the results of the query
{'bib': {'abstract': 'We address a learning-to-normalize problem by proposing '
                     'Switchable Normalization (SN), which learns to select '
                     'different normalizers for different normalization layers '
                     'of a deep neural network. SN employs three distinct '
                     'scopes to compute statistics (means and variances) '
                     'including a channel, a layer, and a minibatch. SN '
                     'switches between them by learning their importance '
                     'weights in an end-to-end manner. It has several good '
                     'properties. First, it adapts to various network '
                     'architectures and tasks (see Fig. 1). Second, it is '
                     'robust to a wide range of',
         'author': ['P Luo', 'J Ren', 'Z Peng', 'R Zhang', 'J Li'],
         'cites': '60',
         'eprint': 'https://arxiv.org/pdf/1806.10779',
         'gsrank': '1',
         'title': 'Differentiable learning-to-normalize via switchable '
                  'normalization',
         'url': 'https://arxiv.org/abs/1806.10779',
         'venue': 'arXiv preprint arXiv:1806.10779',
         'year': '2018'},
 'citations_link': '/scholar?cites=16272948606490934413&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDifferentiable%2BLearning-to-Normalize%2Bvia%2BSwitchable%2BNormalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=jXwUMoog1eEJ&ei=ocUqX72SIqOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:jXwUMoog1eEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIAMPRYTVTCBdOAnuAho4drX_NODQz&scisig=AAGBfm0AAAAAXyrIAFcg4Ll1rmyqw3B0sceZIrIfoIZI&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
60
-------------------------------------------------
2020-08-05 14:43:52
Got the results of the query
{'bib': {'abstract': 'High-dimensional time series are common in many domains. '
                     'Since human cognition is not optimized to work well in '
                     'high-dimensional spaces, these areas could benefit from '
                     'interpretable low-dimensional representations. However, '
                     'most representation learning algorithms for time series '
                     'data are difficult to interpret. This is due to '
                     'non-intuitive mappings from data features to salient '
                     'properties of the representation and non-smoothness over '
                     'time. To address this problem, we propose a new '
                     'representation learning framework building on',
         'author': ['V Fortuin', 'M Hüser', 'F Locatello', 'H Strathmann'],
         'cites': '27',
         'eprint': 'https://arxiv.org/pdf/1806.02199',
         'gsrank': '1',
         'title': 'Som-vae: Interpretable discrete representation learning on '
                  'time series',
         'url': 'https://arxiv.org/abs/1806.02199',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=9836294528958312436&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSOM-VAE:%2BInterpretable%2BDiscrete%2BRepresentation%2BLearning%2Bon%2BTime%2BSeries%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=9K-tWMKJgYgJ&ei=r8UqX7abL7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:9K-tWMKJgYgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIDRHk8FU4jtcjbUbHlEKIlsfhPezy&scisig=AAGBfm0AAAAAXyrIDZz6998fAdTPfM6O455VbGjh--pt&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
27
-------------------------------------------------
2020-08-05 14:44:05
Got the results of the query
{'bib': {'abstract': 'This paper proposes a neural sequence-to-sequence '
                     'text-to-speech (TTS) model which can control latent '
                     'attributes in the generated speech that are rarely '
                     'annotated in the training data, such as speaking style, '
                     'accent, background noise, and recording conditions. The '
                     'model is formulated as a conditional generative model '
                     'based on the variational autoencoder (VAE) framework, '
                     'with two levels of hierarchical latent variables. The '
                     'first level is a categorical variable, which represents '
                     'attribute groups (eg clean/noisy) and provides',
         'author': ['WN Hsu', 'Y Zhang', 'RJ Weiss', 'H Zen', 'Y Wu'],
         'cites': '53',
         'eprint': 'https://arxiv.org/pdf/1810.07217',
         'gsrank': '1',
         'title': 'Hierarchical generative modeling for controllable speech '
                  'synthesis',
         'url': 'https://arxiv.org/abs/1810.07217',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=7736857481159574881&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BGenerative%2BModeling%2Bfor%2BControllable%2BSpeech%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=YQVXMuPWXmsJ&ei=uMUqX7n4KLGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:YQVXMuPWXmsJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIFHuM5sWC7llCJ9db39l5wOC2Ue0S&scisig=AAGBfm0AAAAAXyrIFEOkod3PcKt_6LNaZ4SUrJW9Rawr&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
53
-------------------------------------------------
2020-08-05 14:44:12
Got the results of the query
{'bib': {'abstract': 'Learning multimodal representations is a fundamentally '
                     'complex research problem due to the presence of multiple '
                     'heterogeneous sources of information. Although the '
                     'presence of multiple modalities provides additional '
                     'valuable information, there are two key challenges to '
                     'address when learning from multimodal data: 1) models '
                     'must learn the complex intra-modal and cross-modal '
                     'interactions for prediction and 2) models must be robust '
                     'to unexpected missing or noisy modalities during '
                     'testing. In this paper, we propose to optimize for a '
                     'joint',
         'author': ['YHH Tsai', 'PP Liang', 'A Zadeh', 'LP Morency'],
         'cites': '39',
         'eprint': 'https://arxiv.org/pdf/1806.06176',
         'gsrank': '1',
         'title': 'Learning factorized multimodal representations',
         'url': 'https://arxiv.org/abs/1806.06176',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=2626823666054989533&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BFactorized%2BMultimodal%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=3QbM7jRadCQJ&ei=v8UqX6P2FIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:3QbM7jRadCQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIG5BHozvX5sZfw6i5hnl6wbRcBaAu&scisig=AAGBfm0AAAAAXyrIG2shGfd7x7qMrY5cxRkhbXaCVznJ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
39
-------------------------------------------------
2020-08-05 14:44:19
Got the results of the query
{'bib': {'abstract': 'Humans acquire complex skills by exploiting previously '
                     'learned skills and making transitions between them. To '
                     'empower machines with this ability, we propose a method '
                     'that can learn transition policies which effectively '
                     'connect primitive skills to perform sequential tasks '
                     'without handcrafted rewards. To efficiently train our '
                     'transition policies, we introduce proximity predictors '
                     'which induce rewards gauging proximity to suitable '
                     'initial states for the next skill. The proposed method '
                     'is evaluated on a set of complex continuous control '
                     'tasks in',
         'author': ['Y Lee', 'SH Sun', 'S Somasundaram', 'ES Hu'],
         'cites': '14',
         'gsrank': '1',
         'title': 'Composing complex skills by learning transition policies',
         'url': 'https://openreview.net/forum?id=rygrBhC5tQ',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11124502787308100010&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DComposing%2BComplex%2BSkills%2Bby%2BLearning%2BTransition%2BPolicies%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=qplXTFgsYpoJ&ei=ycUqX6exFoyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:qplXTFgsYpoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIJRnHmnHjRKwwjqs34qCnemRkF0_D&scisig=AAGBfm0AAAAAXyrIJXtkL_K40LiROYVWGsBva8Xhkut_&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
14
-------------------------------------------------
2020-08-05 14:44:29
Got the results of the query
{'bib': {'abstract': 'Localizing a specific protein in a human cell is '
                     'essential for understanding cellular functions and '
                     'biological processes of underlying diseases. A '
                     'promising, low-cost, and time-efficient biotechnology '
                     'for localizing proteins is high-throughput fluorescence '
                     'microscopy imaging (HTI). This imaging technique stains '
                     'the protein of interest in a cell with fluorescent '
                     'antibodies and subsequently takes a microscopic image. '
                     'Together with images of other stained proteins or cell '
                     'organelles and the annotation by the Human Protein Atlas '
                     'project',
         'author': ['E Rumetshofer', 'M Hofmarcher', 'C Röhrl'],
         'cites': '5',
         'eprint': 'https://openreview.net/pdf?id=ryl5khRcKm',
         'gsrank': '1',
         'title': 'Human-level protein localization with convolutional neural '
                  'networks',
         'url': 'https://openreview.net/forum?id=ryl5khRcKm&source=post_page---------------------------',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1213567118521779599&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHuman-level%2BProtein%2BLocalization%2Bwith%2BConvolutional%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=j8WVUt101xAJ&ei=0MUqX_D-EIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:j8WVUt101xAJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrILS-5xsf8ujwlth4HWwYbiTvpC_yh&scisig=AAGBfm0AAAAAXyrILZO3FXKHZpq_zzF3nv_kk55qUMDL&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 14:44:37
Got the results of the query
{'bib': {'abstract': 'A key challenge in reinforcement learning (RL) is '
                     'environment generalization: a policy trained to solve a '
                     'task in one environment often fails to solve the same '
                     'task in a slightly different test environment. A common '
                     'approach to improve inter-environment transfer is to',
         'author': ['W Zhou', 'L Pinto', 'A Gupta'],
         'cites': '9',
         'eprint': 'https://arxiv.org/pdf/1907.11740',
         'gsrank': '1',
         'title': 'Environment probing interaction policies',
         'url': 'https://arxiv.org/abs/1907.11740',
         'venue': 'arXiv preprint arXiv:1907.11740',
         'year': '2019'},
 'citations_link': '/scholar?cites=2903789960714905866&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnvironment%2BProbing%2BInteraction%2BPolicies%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=CunBlpVVTCgJ&ei=2MUqX4WGLqiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:CunBlpVVTCgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrINVa6COfSqbe8yW7vnxXgzq7FCpfI&scisig=AAGBfm0AAAAAXyrINWgpiNbX5dC1SUa1oA4JZuMmGn0R&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 14:44:45
Got the results of the query
{'bib': {'abstract': 'The variational autoencoder (VAE) is a popular '
                     'combination of deep latent variable model and '
                     'accompanying variational learning technique. By using a '
                     "neural inference network to approximate the model's "
                     'posterior on latent variables, VAEs efficiently '
                     'parameterize a lower bound on marginal data likelihood '
                     'that can be optimized directly via gradient methods. In '
                     'practice, however, VAE training often results in a '
                     'degenerate local optimum known as" posterior collapse" '
                     'where the model learns to ignore the latent variable and '
                     'the approximate',
         'author': ['J He', 'D Spokoyny', 'G Neubig'],
         'cites': '75',
         'eprint': 'https://arxiv.org/pdf/1901.05534',
         'gsrank': '1',
         'title': 'Lagging inference networks and posterior collapse in '
                  'variational autoencoders',
         'url': 'https://arxiv.org/abs/1901.05534',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=5286759698670808442&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLagging%2BInference%2BNetworks%2Band%2BPosterior%2BCollapse%2Bin%2BVariational%2BAutoencoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=eu0ffO9XXkkJ&ei=4MUqX62TN7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:eu0ffO9XXkkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIPd6gSEJQ-CJR19VDBOcBrleQ-I7t&scisig=AAGBfm0AAAAAXyrIPTCCnlAxqPD9b-UYvGosH2-wGAC8&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
75
-------------------------------------------------
2020-08-05 14:44:53
Got the results of the query
{'bib': {'abstract': 'In this paper, we propose the Asynchronous Accelerated '
                     'Nonuniform Randomized Block Coordinate Descent algorithm '
                     '(A2BCD). We prove A2BCD converges linearly to a solution '
                     'of the convex minimization problem at the same rate as '
                     'NU_ACDM, so long as the maximum delay is not too large. '
                     'This is the first asynchronous Nesterov-accelerated '
                     'algorithm that attains any provable speedup. Moreover, '
                     'we then prove that these algorithms both have optimal '
                     'complexity. Asynchronous algorithms complete much faster '
                     'iterations, and A2BCD',
         'author': ['R Hannah', 'F Feng', 'W Yin'],
         'cites': '6',
         'eprint': 'https://openreview.net/pdf?id=rylIAsCqYm',
         'gsrank': '1',
         'title': 'A2BCD: Asynchronous acceleration with optimal complexity',
         'url': 'https://openreview.net/forum?id=rylIAsCqYm',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=13768856975610039063&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA2BCD:%2BAsynchronous%2BAcceleration%2Bwith%2BOptimal%2BComplexity%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=F0tlAnjOFL8J&ei=68UqX5yJFpqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:F0tlAnjOFL8J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrISgPfo4SATsPyjdybVjs8pyPVYcRU&scisig=AAGBfm0AAAAAXyrISiWrtmG6kvC5QhKUQxF0Js3RXGR4&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 14:45:06
Got the results of the query
{'bib': {'abstract': 'Human perception of 3D shapes goes beyond reconstructing '
                     'them as a set of points or a composition of geometric '
                     'primitives: we also effortlessly understand higher-level '
                     'shape structure such as the repetition and reflective '
                     'symmetry of object parts. In contrast, recent advances '
                     'in 3D shape sensing focus more on low-level geometry but '
                     'less on these higher-level relationships. In this paper, '
                     'we propose 3D shape programs, integrating bottom-up '
                     'recognition systems with top-down, symbolic program '
                     'structure to capture both low-level',
         'author': ['Y Tian', 'A Luo', 'X Sun', 'K Ellis', 'WT Freeman'],
         'cites': '33',
         'eprint': 'https://arxiv.org/pdf/1901.02875',
         'gsrank': '1',
         'title': 'Learning to infer and execute 3d shape programs',
         'url': 'https://arxiv.org/abs/1901.02875',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=12000176727118199358&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BInfer%2Band%2BExecute%2B3D%2BShape%2BPrograms%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=PjrKVScxiaYJ&ei=98UqX7mgBcKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:PjrKVScxiaYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIUpxryQa6E4aSV7SJAvyfehP7zmpC&scisig=AAGBfm0AAAAAXyrIUrN_3jCD-nosOG2BaoaHsrA9lah0&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
33
-------------------------------------------------
2020-08-05 14:45:14
Got the results of the query
{'bib': {'abstract': 'Deep neural networks, in particular convolutional neural '
                     'networks, have become highly effective tools for '
                     'compressing images and solving inverse problems '
                     'including denoising, inpainting, and reconstruction from '
                     'few and noisy measurements. This success can be '
                     'attributed in part to their ability to represent and '
                     'generate natural images well. Contrary to classical '
                     'tools such as wavelets, image-generating deep neural '
                     'networks have a large number of parameters---typically a '
                     'multiple of their output dimension---and need to be',
         'author': ['R Heckel', 'P Hand'],
         'cites': '54',
         'eprint': 'https://arxiv.org/pdf/1810.03982',
         'gsrank': '1',
         'title': 'Deep decoder: Concise image representations from untrained '
                  'non-convolutional networks',
         'url': 'https://arxiv.org/abs/1810.03982',
         'venue': 'arXiv preprint arXiv:1810.03982',
         'year': '2018'},
 'citations_link': '/scholar?cites=5031846359818705791&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BDecoder:%2BConcise%2BImage%2BRepresentations%2Bfrom%2BUntrained%2BNon-convolutional%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=fxtWuZq11EUJ&ei=_8UqX4-wHrGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:fxtWuZq11EUJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIXDkGC8lHtAc9JcOCXweSnHlxtnb3&scisig=AAGBfm0AAAAAXyrIXHCUzTHfYf-7ukTNKM8yJbgA2Gnd&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
54
-------------------------------------------------
2020-08-05 14:45:24
Got the results of the query
{'bib': {'abstract': 'We propose Stochastic Neural Architecture Search (SNAS), '
                     'an economical end-to-end solution to Neural Architecture '
                     'Search (NAS) that trains neural operation parameters and '
                     'architecture distribution parameters in same round of '
                     'back-propagation, while maintaining the completeness and '
                     'differentiability of the NAS pipeline. In this work, NAS '
                     'is reformulated as an optimization problem on parameters '
                     'of a joint distribution for the search space in a cell. '
                     'To leverage the gradient information in generic '
                     'differentiable loss for architecture',
         'author': ['S Xie', 'H Zheng', 'C Liu', 'L Lin'],
         'cites': '227',
         'eprint': 'https://arxiv.org/pdf/1812.09926',
         'gsrank': '1',
         'title': 'SNAS: stochastic neural architecture search',
         'url': 'https://arxiv.org/abs/1812.09926',
         'venue': 'arXiv preprint arXiv:1812.09926',
         'year': '2018'},
 'citations_link': '/scholar?cites=13328811299154907405&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSNAS:%2Bstochastic%2Bneural%2Barchitecture%2Bsearch%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=DVFX7EFz-bgJ&ei=BsYqX8rKL4-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:DVFX7EFz-bgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIYabKZcqxwfhgxqccoRiunMkaRjBw&scisig=AAGBfm0AAAAAXyrIYdgxURSL9FF1DeEiNxcVUKhzvHSb&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
227
-------------------------------------------------
2020-08-05 14:45:29
Got the results of the query
{'bib': {'abstract': 'To study how mental object representations are related '
                     'to behavior, we estimated sparse, non-negative '
                     'representations of objects using human behavioral '
                     'judgments on images representative of 1,854 object '
                     'categories. These representations predicted a latent '
                     'similarity structure between objects, which captured '
                     'most of the explainable variance in human behavioral '
                     'judgments. Individual dimensions in the low-dimensional '
                     'embedding were found to be highly reproducible and '
                     'interpretable as conveying degrees of taxonomic '
                     'membership',
         'author': ['CY Zheng', 'F Pereira', 'CI Baker', 'MN Hebart'],
         'cites': '7',
         'eprint': 'https://arxiv.org/pdf/1901.02915',
         'gsrank': '1',
         'title': 'Revealing interpretable object representations from human '
                  'behavior',
         'url': 'https://arxiv.org/abs/1901.02915',
         'venue': 'arXiv preprint arXiv …',
         'year': '2019'},
 'citations_link': '/scholar?cites=16835293067180738408&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRevealing%2Binterpretable%2Bobject%2Brepresentations%2Bfrom%2Bhuman%2Bbehavior%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=aIfn7tj5oukJ&ei=EMYqX9vHHbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:aIfn7tj5oukJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIbj85AgM4a-AbGiD7ts9qAzu5BWTl&scisig=AAGBfm0AAAAAXyrIbmljuVLtnw0_KBU2GhOglqBGC5Ks&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
7
-------------------------------------------------
2020-08-05 14:45:42
Got the results of the query
{'bib': {'abstract': 'Recurrent neural networks have gained widespread use in '
                     'modeling sequential data. Learning long-term '
                     'dependencies using these models remains difficult '
                     'though, due to exploding or vanishing gradients. In this '
                     'paper, we draw connections between recurrent networks '
                     'and ordinary differential equations. A special form of '
                     'recurrent networks called the AntisymmetricRNN is '
                     'proposed under this theoretical framework, which is able '
                     'to capture long-term dependencies thanks to the '
                     'stability property of its underlying differential '
                     'equation',
         'author': ['B Chang', 'M Chen', 'E Haber', 'EH Chi'],
         'cites': '36',
         'eprint': 'https://arxiv.org/pdf/1902.09689',
         'gsrank': '1',
         'title': 'AntisymmetricRNN: A dynamical system view on recurrent '
                  'neural networks',
         'url': 'https://arxiv.org/abs/1902.09689',
         'venue': 'arXiv preprint arXiv:1902.09689',
         'year': '2019'},
 'citations_link': '/scholar?cites=5419518435083318687&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAntisymmetricRNN:%2BA%2BDynamical%2BSystem%2BView%2Bon%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=n3XSiE3_NUsJ&ei=HMYqX6eeGo-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:n3XSiE3_NUsJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIeKNYplVpdhhJ7zdq99eJtZKcpjmw&scisig=AAGBfm0AAAAAXyrIeLaWU5032gTUIRnWI0CnlwJfpKlN&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
36
-------------------------------------------------
2020-08-05 14:45:53
Got the results of the query
{'bib': {'abstract': 'End-to-end task-oriented dialogue is challenging since '
                     'knowledge bases are usually large, dynamic and hard to '
                     'incorporate into a learning framework. We propose the '
                     'global-to-local memory pointer (GLMP) networks to '
                     'address this issue. In our model, a global memory '
                     'encoder and a local memory decoder are proposed to share '
                     'external knowledge. The encoder encodes dialogue '
                     'history, modifies global contextual representation, and '
                     'generates a global memory pointer. The decoder first '
                     'generates a sketch response with unfilled slots',
         'author': ['CS Wu', 'R Socher', 'C Xiong'],
         'cites': '33',
         'eprint': 'https://arxiv.org/pdf/1901.04713',
         'gsrank': '1',
         'title': 'Global-to-local memory pointer networks for task-oriented '
                  'dialogue',
         'url': 'https://arxiv.org/abs/1901.04713',
         'venue': 'arXiv preprint arXiv:1901.04713',
         'year': '2019'},
 'citations_link': '/scholar?cites=8042905846859720405&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGlobal-to-local%2BMemory%2BPointer%2BNetworks%2Bfor%2BTask-Oriented%2BDialogue%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=1Wb8K0Eknm8J&ei=J8YqX6WYK6OGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:1Wb8K0Eknm8J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIhTzUxff42GEpgaprnkwrCTL9_T1M&scisig=AAGBfm0AAAAAXyrIhaVeMV3ZSgczjOaue6xpLXlO-Mu9&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
33
-------------------------------------------------
2020-08-05 14:46:05
Got the results of the query
{'bib': {'abstract': 'Unsupervised image-to-image translation has gained '
                     'considerable attention due to the recent impressive '
                     'progress based on generative adversarial networks '
                     '(GANs). However, previous methods often fail in '
                     'challenging cases, in particular, when an image has '
                     'multiple target instances and a translation task '
                     'involves significant changes in shape, eg, translating '
                     'pants to skirts in fashion images. To tackle the issues, '
                     'we propose a novel method, coined instance-aware GAN '
                     '(InstaGAN), that incorporates the instance information '
                     '(eg, object',
         'author': ['S Mo', 'M Cho', 'J Shin'],
         'cites': '54',
         'eprint': 'https://arxiv.org/pdf/1812.10889.pdf&xid=17259,15700023,15700186,15700191,15700256,15700259,15700262,15700264',
         'gsrank': '1',
         'title': 'Instagan: Instance-aware image-to-image translation',
         'url': 'https://arxiv.org/abs/1812.10889',
         'venue': 'arXiv preprint arXiv:1812.10889',
         'year': '2018'},
 'citations_link': '/scholar?cites=14041898124180765737&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInstaGAN:%2BInstance-aware%2BImage-to-Image%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=KbwEYPLX3sIJ&ei=MMYqX6KhKaiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:KbwEYPLX3sIJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIkj5jwuCIlMapeHM_nwHQ_ulHSQIv&scisig=AAGBfm0AAAAAXyrIkr08H0-nk4OZwhUg-_Q70tceErWU&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
54
-------------------------------------------------
2020-08-05 14:46:18
Got the results of the query
{'bib': {'abstract': 'We provide a novel perspective on the forward pass '
                     'through a block of layers in a deep network. In '
                     'particular, we show that a forward pass through a '
                     'standard dropout layer followed by a linear layer and a '
                     'non-linear activation is equivalent to optimizing a '
                     'convex',
         'author': ['A Bibi', 'B Ghanem', 'V Koltun', 'R Ranftl'],
         'cites': '8',
         'eprint': 'https://repository.kaust.edu.sa/bitstream/handle/10754/662270/deep_layers_as_stochastic_solvers.pdf?sequence=1&isAllowed=y',
         'gsrank': '1',
         'title': 'Deep layers as stochastic solvers',
         'url': 'https://repository.kaust.edu.sa/handle/10754/662270',
         'venue': '',
         'year': '2019'},
 'citations_link': '/scholar?cites=14411629229022892325&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLayers%2Bas%2BStochastic%2BSolvers%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=JdGLfm1kAMgJ&ei=P8YqX4T5CI-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:JdGLfm1kAMgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrInO58piegdIiqxslCjXjaYhMMNBpY&scisig=AAGBfm0AAAAAXyrInLCs4xKkSUkuyMe2cZx_8vcCw7xm&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
8
-------------------------------------------------
2020-08-05 14:46:28
Got the results of the query
{'bib': {'abstract': 'Hierarchical agents have the potential to solve '
                     'sequential decision making tasks with greater sample '
                     'efficiency than their non-hierarchical counterparts '
                     'because hierarchical agents can break down tasks into '
                     'sets of subtasks that only require short sequences of '
                     'decisions. In order to realize this potential of faster '
                     'learning, hierarchical agents need to be able to learn '
                     'their multiple levels of policies in parallel so these '
                     'simpler subproblems can be solved simultaneously. Yet, '
                     'learning multiple levels of policies in parallel is hard '
                     'because it',
         'author': ['A Levy', 'G Konidaris', 'R Platt', 'K Saenko'],
         'cites': '32',
         'eprint': 'https://arxiv.org/pdf/1712.00948',
         'gsrank': '1',
         'title': 'Learning multi-level hierarchies with hindsight',
         'url': 'https://arxiv.org/abs/1712.00948',
         'venue': 'arXiv preprint arXiv:1712.00948',
         'year': '2017'},
 'citations_link': '/scholar?cites=11558193958091287134&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BMulti-Level%2BHierarchies%2Bwith%2BHindsight%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=XuoYvSr0ZqAJ&ei=SMYqX5jkE8KwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:XuoYvSr0ZqAJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIpdPrD_UDfKAeGQq9maNs2eg-A51l&scisig=AAGBfm0AAAAAXyrIpZBajnRU6zWsHQbfyEszHwbPKQWD&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
32
-------------------------------------------------
2020-08-05 14:46:37
Got the results of the query
{'bib': {'abstract': 'Several recently proposed stochastic optimization '
                     'methods that have been successfully used in training '
                     'deep networks such as RMSProp, Adam, Adadelta, Nadam are '
                     'based on using gradient updates scaled by square roots '
                     'of exponential moving averages of squared',
         'author': ['SJ Reddi', 'S Kale', 'S Kumar'],
         'cites': '781',
         'eprint': 'https://arxiv.org/pdf/1904.09237',
         'gsrank': '1',
         'title': 'On the convergence of adam and beyond',
         'url': 'https://arxiv.org/abs/1904.09237',
         'venue': 'arXiv preprint arXiv:1904.09237',
         'year': '2019'},
 'citations_link': '/scholar?cites=7572152545124305671&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BConvergence%2Bof%2BAdam%2Band%2BBeyond%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=B0s07Z6wFWkJ&ei=UMYqX9eNBKiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:B0s07Z6wFWkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIro2jbU6CtwN1DNP8d86aTIO7dwAQ&scisig=AAGBfm0AAAAAXyrIrgAP7Xi-uvjlVnokHfnTbV_v9EEh&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
781
-------------------------------------------------
2020-08-05 14:46:46
Got the results of the query
{'bib': {'abstract': 'Character-based neural machine translation (NMT) models '
                     'alleviate out-of-vocabulary issues, learn morphology, '
                     'and move us closer to completely end-to-end translation '
                     'systems. Unfortunately, they are also very brittle and '
                     'easily falter when presented with noisy data. In this '
                     'paper, we confront NMT models with synthetic and natural '
                     'sources of noise. We find that state-of-the-art models '
                     'fail to translate even moderately noisy texts that '
                     'humans have no trouble comprehending. We explore two '
                     'approaches to increase model robustness',
         'author': ['Y Belinkov', 'Y Bisk'],
         'cites': '199',
         'eprint': 'https://arxiv.org/pdf/1711.02173',
         'gsrank': '1',
         'title': 'Synthetic and natural noise both break neural machine '
                  'translation',
         'url': 'https://arxiv.org/abs/1711.02173',
         'venue': 'arXiv preprint arXiv:1711.02173',
         'year': '2017'},
 'citations_link': '/scholar?cites=10493132199224079445&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSynthetic%2Band%2BNatural%2BNoise%2BBoth%2BBreak%2BNeural%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=VfT24yMYn5EJ&ei=W8YqX8yMLaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:VfT24yMYn5EJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIv15hB487Cxf89gE4fSEzUsesLJhY&scisig=AAGBfm0AAAAAXyrIv3L-F7JldAkLmNEj92o5ZCRuZm26&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
199
-------------------------------------------------
2020-08-05 14:47:03
Got the results of the query
{'bib': {'abstract': 'In this paper we investigate image classification with '
                     'computational resource limits at test time. Two such '
                     'settings are: 1. anytime classification, where the '
                     "network's prediction for a test example is progressively "
                     'updated, facilitating the output of a prediction at any '
                     'time; and 2. budgeted batch classification, where a '
                     'fixed amount of computation is available to classify a '
                     'set of examples that can be spent unevenly across" '
                     'easier" and" harder" inputs. In contrast to most prior '
                     'work, such as the popular Viola and Jones algorithm, our '
                     'approach is based on',
         'author': ['G Huang', 'D Chen', 'T Li', 'F Wu', 'L van der Maaten'],
         'cites': '139',
         'eprint': 'https://arxiv.org/pdf/1703.09844',
         'gsrank': '1',
         'title': 'Multi-scale dense networks for resource efficient image '
                  'classification',
         'url': 'https://arxiv.org/abs/1703.09844',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=8749554166283747056&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-Scale%2BDense%2BNetworks%2Bfor%2BResource%2BEfficient%2BImage%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=8L5hOCipbHkJ&ei=bMYqX4nJFKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:8L5hOCipbHkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrIy3hmEW-VOva0XaqhujBTeQ2ocK4w&scisig=AAGBfm0AAAAAXyrIy6PpIUTzfvIoDIKEeHaknuX3CkPG&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
139
-------------------------------------------------
2020-08-05 14:47:15
Got the results of the query
{'bib': {'abstract': 'Researches on deep neural networks with discrete '
                     'parameters and their deployment in embedded systems have '
                     'been active and promising topics. Although previous '
                     'works have successfully reduced precision in inference, '
                     'transferring both training and inference processes to '
                     'low-bitwidth integers has not been demonstrated '
                     'simultaneously. In this work, we develop a new method '
                     'termed as" WAGE" to discretize both training and '
                     'inference, where weights (W), activations (A), gradients '
                     '(G) and errors (E) among layers are shifted and',
         'author': ['S Wu', 'G Li', 'F Chen', 'L Shi'],
         'cites': '155',
         'eprint': 'https://arxiv.org/pdf/1802.04680',
         'gsrank': '1',
         'title': 'Training and inference with integers in deep neural '
                  'networks',
         'url': 'https://arxiv.org/abs/1802.04680',
         'venue': 'arXiv preprint arXiv:1802.04680',
         'year': '2018'},
 'citations_link': '/scholar?cites=15215054387477750278&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2Band%2BInference%2Bwith%2BIntegers%2Bin%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=BiLcQ1q7JtMJ&ei=eMYqX8fKPIjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:BiLcQ1q7JtMJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrI1qjP59Us80NApwOw7Plnr3N6x0iN&scisig=AAGBfm0AAAAAXyrI1gXsr_maLZ8PtAEaOH2kJ9DoQVIe&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
155
-------------------------------------------------
2020-08-05 14:47:26
Got the results of the query
{'bib': {'abstract': 'The ability of algorithms to evolve or learn '
                     '(compositional) communication protocols has '
                     'traditionally been studied in the language evolution '
                     'literature through the use of emergent communication '
                     'tasks. Here we scale up this research by using '
                     'contemporary deep learning methods and by training '
                     'reinforcement-learning neural network agents on '
                     'referential communication games. We extend previous '
                     'work, in which agents were trained in symbolic '
                     'environments, by developing agents which are able to '
                     'learn from raw pixel data, a more',
         'author': ['A Lazaridou', 'KM Hermann', 'K Tuyls', 'S Clark'],
         'cites': '75',
         'eprint': 'https://arxiv.org/pdf/1804.03984',
         'gsrank': '1',
         'title': 'Emergence of linguistic communication from referential '
                  'games with symbolic and pixel input',
         'url': 'https://arxiv.org/abs/1804.03984',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=12707373577928936905&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergence%2Bof%2BLinguistic%2BCommunication%2Bfrom%2BReferential%2BGames%2Bwith%2BSymbolic%2Band%2BPixel%2BInput%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=yY2jTPGoWbAJ&ei=hsYqX7mEBcKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:yY2jTPGoWbAJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrI45aqn1lDR2AB5rXeOP-iy3bH7p8U&scisig=AAGBfm0AAAAAXyrI44OMHhEONkvF0CEyXU30CDTjhWgz&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
75
-------------------------------------------------
2020-08-05 14:47:39
Got the results of the query
{'bib': {'abstract': 'Convolutional Neural Networks (CNNs) have become the '
                     'method of choice for learning problems involving 2D '
                     'planar images. However, a number of problems of recent '
                     'interest have created a demand for models that can '
                     'analyze spherical images. Examples include',
         'author': ['TS Cohen', 'M Geiger', 'J Köhler', 'M Welling'],
         'cites': '258',
         'eprint': 'https://arxiv.org/pdf/1801.10130.pdf%20http://arxiv.org/abs/1801.10130',
         'gsrank': '1',
         'title': 'Spherical cnns',
         'url': 'https://arxiv.org/abs/1801.10130',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=6361332838540502667&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpherical%2BCNNs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=i-Y7S4P-R1gJ&ei=j8YqX-KLLsKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:i-Y7S4P-R1gJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrI7XOTg56mrF1P6PvVP2MAcnif0b-q&scisig=AAGBfm0AAAAAXyrI7QqLykR0L5JymrgPGftUtu3PhEC7&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
258
-------------------------------------------------
2020-08-05 14:47:49
Got the results of the query
{'bib': {'abstract': 'We frame Question Answering (QA) as a Reinforcement '
                     'Learning task, an approach that we call Active Question '
                     'Answering. We propose an agent that sits between the '
                     'user and a black box QA system and learns to reformulate '
                     'questions to elicit the best possible answers. The agent '
                     'probes the system with, potentially many, natural '
                     'language reformulations of an initial question and '
                     'aggregates the returned evidence to yield the best '
                     'answer. The reformulation system is trained end-to-end '
                     'to maximize answer quality using policy gradient. We '
                     'evaluate',
         'author': ['C Buck', 'J Bulian', 'M Ciaramita', 'W Gajewski'],
         'cites': '78',
         'eprint': 'https://arxiv.org/pdf/1705.07830',
         'gsrank': '1',
         'title': 'Ask the right questions: Active question reformulation with '
                  'reinforcement learning',
         'url': 'https://arxiv.org/abs/1705.07830',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=4524039328183548455&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAsk%2Bthe%2BRight%2BQuestions:%2BActive%2BQuestion%2BReformulation%2Bwith%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=J_pGssydyD4J&ei=m8YqX7-oCYyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:J_pGssydyD4J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrI96EtZJQ1FOKWaDFtLYMAFVSvbyPB&scisig=AAGBfm0AAAAAXyrI9_a6PqKiprPiNUz4Bq6-a8Pk1rjp&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
78
-------------------------------------------------
2020-08-05 14:47:59
Got the results of the query
{'bib': {'abstract': 'Momentum based stochastic gradient methods such as heavy '
                     "ball (HB) and Nesterov's accelerated gradient descent "
                     '(NAG) method are widely used in practice for training '
                     'deep networks and other supervised learning models, as '
                     'they often provide significant improvements over '
                     'stochastic gradient descent (SGD). In general,“fast '
                     'gradient” methods have provable improvements over '
                     'gradient descent only for the deterministic case, where '
                     'the gradients are exact. In the stochastic case, the '
                     'popular explanations for their wide',
         'author': ['R Kidambi', 'P Netrapalli', 'P Jain'],
         'cites': '42',
         'eprint': 'https://arxiv.org/pdf/1803.05591',
         'gsrank': '1',
         'title': 'On the insufficiency of existing momentum schemes for '
                  'stochastic optimization',
         'url': 'https://ieeexplore.ieee.org/abstract/document/8503173/',
         'venue': '2018 Information Theory …',
         'year': '2018'},
 'citations_link': '/scholar?cites=6907311906014063619&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2Binsufficiency%2Bof%2Bexisting%2Bmomentum%2Bschemes%2Bfor%2BStochastic%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=A2zL_pSz218J&ei=pMYqX_yqJ4-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:A2zL_pSz218J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJAivSkaG40WG0hXwtR7vdmiRef4aa&scisig=AAGBfm0AAAAAXyrJAoJ_mb00Z35Yl94zLDTYkuly_6Oo&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
42
-------------------------------------------------
2020-08-05 14:48:10
Got the results of the query
{'bib': {'abstract': 'Neural networks are vulnerable to adversarial examples '
                     'and researchers have proposed many heuristic attack and '
                     'defense mechanisms. We address this problem through the '
                     'principled lens of distributionally robust optimization, '
                     'which guarantees performance under adversarial input '
                     'perturbations. By considering a Lagrangian penalty '
                     'formulation of perturbing the underlying data '
                     'distribution in a Wasserstein ball, we provide a '
                     'training procedure that augments model parameter updates '
                     'with worst-case perturbations of training',
         'author': ['A Sinha', 'H Namkoong', 'J Duchi'],
         'cites': '251',
         'eprint': 'https://arxiv.org/pdf/1710.10571.pdf]',
         'gsrank': '1',
         'title': 'Certifying some distributional robustness with principled '
                  'adversarial training',
         'url': 'https://arxiv.org/abs/1710.10571',
         'venue': 'arXiv preprint arXiv:1710.10571',
         'year': '2017'},
 'citations_link': '/scholar?cites=5504610656672947417&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCertifying%2BSome%2BDistributional%2BRobustness%2Bwith%2BPrincipled%2BAdversarial%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=2RzSQTpOZEwJ&ei=r8YqX5vkN4jHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:2RzSQTpOZEwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJDst8LSS4OCyKXj1UCxR3GPRPZdjJ&scisig=AAGBfm0AAAAAXyrJDmgtb4zOoJmSAiGnoo1SqBiYfdMx&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
251
-------------------------------------------------
2020-08-05 14:48:22
Got the results of the query
{'bib': {'abstract': 'We consider the problem of representing collective '
                     'behavior of large populations and predicting the '
                     'evolution of a population distribution over a discrete '
                     'state space. A discrete time mean field game (MFG) is '
                     'motivated as an interpretable model founded on game '
                     'theory for understanding the aggregate effect of '
                     'individual actions and predicting the temporal evolution '
                     'of population distributions. We achieve a synthesis of '
                     'MFG and Markov decision processes (MDP) by showing that '
                     'a special MFG is reducible to an MDP. This enables us to',
         'author': ['J Yang', 'X Ye', 'R Trivedi', 'H Xu', 'H Zha'],
         'cites': '7',
         'eprint': 'https://arxiv.org/pdf/1711.03156',
         'gsrank': '1',
         'title': 'Learning deep mean field games for modeling large '
                  'population behavior',
         'url': 'https://arxiv.org/abs/1711.03156',
         'venue': 'arXiv preprint arXiv:1711.03156',
         'year': '2017'},
 'citations_link': '/scholar?cites=17328831972958604970&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BDeep%2BMean%2BField%2BGames%2Bfor%2BModeling%2BLarge%2BPopulation%2BBehavior%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=qg6hCN5gfPAJ&ei=vMYqX6P-KpqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:qg6hCN5gfPAJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJG0KvyZdQB6UiiBDmsG9pl2OIHK-k&scisig=AAGBfm0AAAAAXyrJGyZjX6yQhabHVzL83fq1A0AbW-Gm&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
7
-------------------------------------------------
2020-08-05 14:48:35
Got the results of the query
{'bib': {'abstract': 'We propose the Wasserstein Auto-Encoder (WAE)---a new '
                     'algorithm for building a generative model of the data '
                     'distribution. WAE minimizes a penalized form of the '
                     'Wasserstein distance between the model distribution and '
                     'the target distribution, which leads',
         'author': ['I Tolstikhin', 'O Bousquet', 'S Gelly'],
         'cites': '390',
         'eprint': 'https://arxiv.org/pdf/1711.01558.pdf,',
         'gsrank': '1',
         'title': 'Wasserstein auto-encoders',
         'url': 'https://arxiv.org/abs/1711.01558',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=1669877132293977025&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWasserstein%2BAuto-Encoders%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=wTPuiGeYLBcJ&ei=yMYqX_zxIcKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:wTPuiGeYLBcJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJJ1IUyMNh0KSGvgKWqWbwIK2vfWDS&scisig=AAGBfm0AAAAAXyrJJ0J03uvxvv7CP4KkK-Tw-ZD8ecqo&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
390
-------------------------------------------------
2020-08-05 14:48:48
Got the results of the query
{'bib': {'abstract': 'One of the challenges in the study of generative '
                     'adversarial networks is the instability of its training. '
                     'In this paper, we propose a novel weight normalization '
                     'technique called spectral normalization to stabilize the '
                     'training of the discriminator. Our new normalization '
                     'technique is computationally light and easy to '
                     'incorporate into existing implementations. We tested the '
                     'efficacy of spectral normalization on CIFAR10, STL-10, '
                     'and ILSVRC2012 dataset, and we experimentally confirmed '
                     'that spectrally normalized GANs (SN-GANs) is capable of',
         'author': ['T Miyato', 'T Kataoka', 'M Koyama', 'Y Yoshida'],
         'cites': '1265',
         'eprint': 'https://arxiv.org/pdf/1802.05957',
         'gsrank': '1',
         'title': 'Spectral normalization for generative adversarial networks',
         'url': 'https://arxiv.org/abs/1802.05957',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=973410365172845184&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSpectral%2BNormalization%2Bfor%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=gIrP3JE_gg0J&ei=1sYqX7n0LaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:gIrP3JE_gg0J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJNt1z6eC0xN3NkNl1vFW-1WyuIEli&scisig=AAGBfm0AAAAAXyrJNnFsuuTlXll1UN-ZswcqVNns1Ucy&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
1265
-------------------------------------------------
2020-08-05 14:49:02
Got the results of the query
{'bib': {'abstract': 'Learning tasks on source code (ie, formal languages) '
                     'have been considered recently, but most work has tried '
                     'to transfer natural language methods and does not '
                     "capitalize on the unique opportunities offered by code's "
                     'known syntax. For example, long-range dependencies '
                     'induced by using the same variable or function in '
                     'distant locations are often not considered. We propose '
                     'to use graphs to represent both the syntactic and '
                     'semantic structure of code and use graph-based deep '
                     'learning methods to learn to reason over',
         'author': ['M Allamanis', 'M Brockschmidt', 'M Khademi'],
         'cites': '194',
         'eprint': 'https://arxiv.org/pdf/1711.00740',
         'gsrank': '1',
         'title': 'Learning to represent programs with graphs',
         'url': 'https://arxiv.org/abs/1711.00740',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=9342740598325165289&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BRepresent%2BPrograms%2Bwith%2BGraphs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=6bzK_xIVqIEJ&ei=5cYqX5twj5iYAeaVrKgM',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:6bzK_xIVqIEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJRXvZ905WpGeht9slqJvjfKR1VZ_X&scisig=AAGBfm0AAAAAXyrJRQ9Zcl6Ly2Vi8msBTMdxkwg-E_zb&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
194
-------------------------------------------------
2020-08-05 14:49:17
Got the results of the query
{'bib': {'abstract': 'Deep Neural Networks (DNNs) have recently been shown to '
                     'be vulnerable against adversarial examples, which are '
                     'carefully crafted instances that can mislead DNNs to '
                     'make errors during prediction. To better understand such '
                     'attacks, a characterization is needed of the properties '
                     "of regions (the so-called'adversarial subspaces') in "
                     'which adversarial examples lie. We tackle this challenge '
                     'by characterizing the dimensional properties of '
                     'adversarial regions, via the use of Local Intrinsic '
                     'Dimensionality (LID). LID assesses the',
         'author': ['X Ma', 'B Li', 'Y Wang', 'SM Erfani', 'S Wijewickrema'],
         'cites': '200',
         'eprint': 'https://arxiv.org/pdf/1801.02613',
         'gsrank': '1',
         'title': 'Characterizing adversarial subspaces using local intrinsic '
                  'dimensionality',
         'url': 'https://arxiv.org/abs/1801.02613',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=17134144151462669065&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCharacterizing%2BAdversarial%2BSubspaces%2BUsing%2BLocal%2BIntrinsic%2BDimensionality%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=CetEPlO1yO0J&ei=8sYqX8DgDLGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:CetEPlO1yO0J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJT7xYU9m1j-_shQ_m4dt-SUTgzYuV&scisig=AAGBfm0AAAAAXyrJTwWU8NwNyDByRRnGrRgdl6-HO4Hm&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
200
-------------------------------------------------
2020-08-05 14:49:27
Got the results of the query
{'bib': {'abstract': 'We formulate language modeling as a matrix factorization '
                     'problem, and show that the expressiveness of '
                     'Softmax-based models (including the majority of neural '
                     'language models) is limited by a Softmax bottleneck. '
                     'Given that natural language is highly context-dependent, '
                     'this further implies that in practice Softmax with '
                     'distributed word embeddings does not have enough '
                     'capacity to model natural language. We propose a simple '
                     'and effective method to address this issue, and improve '
                     'the state-of-the-art perplexities on Penn',
         'author': ['Z Yang', 'Z Dai', 'R Salakhutdinov', 'WW Cohen'],
         'cites': '210',
         'eprint': 'https://arxiv.org/pdf/1711.03953.pdf%22%20%5Ct%20%22https://www.analyticsvidhya.com/blog/2018/03/comprehensive-collection-deep-learning-datasets/_blank',
         'gsrank': '1',
         'title': 'Breaking the softmax bottleneck: A high-rank RNN language '
                  'model',
         'url': 'https://arxiv.org/abs/1711.03953',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=15538946355362697879&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBreaking%2Bthe%2BSoftmax%2BBottleneck:%2BA%2BHigh-Rank%2BRNN%2BLanguage%2BModel%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=l5I-n2FtpdcJ&ei=-8YqX9HNIIvrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:l5I-n2FtpdcJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJWPeOqYKsXUMbhENMXKtGiB8EuWio&scisig=AAGBfm0AAAAAXyrJWHqaCqnSkD9RRMFTmhWjE_rosQl2&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
210
-------------------------------------------------
2020-08-05 14:49:36
Got the results of the query
{'bib': {'abstract': 'Ability to continuously learn and adapt from limited '
                     'experience in nonstationary environments is an important '
                     'milestone on the path towards general intelligence. In '
                     'this paper, we cast the problem of continuous adaptation '
                     'into the learning-to-learn framework. We develop a '
                     'simple gradient-based meta-learning algorithm suitable '
                     'for adaptation in dynamically changing and adversarial '
                     'scenarios. Additionally, we design a new multi-agent '
                     'competitive environment, RoboSumo, and define iterated '
                     'adaptation games for testing',
         'author': ['M Al-Shedivat', 'T Bansal', 'Y Burda', 'I Sutskever'],
         'cites': '145',
         'eprint': 'https://arxiv.org/pdf/1710.03641.pdf?source=post_page---------------------------',
         'gsrank': '1',
         'title': 'Continuous adaptation via meta-learning in nonstationary '
                  'and competitive environments',
         'url': 'https://arxiv.org/abs/1710.03641',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=10800934967753473866&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DContinuous%2BAdaptation%2Bvia%2BMeta-Learning%2Bin%2BNonstationary%2Band%2BCompetitive%2BEnvironments%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Sn8KmyCh5JUJ&ei=BccqX6qOKIjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Sn8KmyCh5JUJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJY_bBzQcck8T_9JU7L5Cy6GU-RXAp&scisig=AAGBfm0AAAAAXyrJY8XAoh6BYYhsdDcCzl3UdFxSV1BF&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
145
-------------------------------------------------
2020-08-05 14:49:47
Got the results of the query
{'bib': {'abstract': 'The driving force behind deep networks is their ability '
                     'to compactly represent rich classes of functions. The '
                     'primary notion for formally reasoning about this '
                     'phenomenon is expressive efficiency, which refers to a '
                     'situation where one network must grow unfeasibly large '
                     'in order to realize (or approximate) functions of '
                     'another. To date, expressive efficiency analyses focused '
                     'on the architectural feature of depth, showing that deep '
                     'networks are representationally superior to shallow '
                     'ones. In this paper we study the expressive efficiency',
         'author': ['N Cohen', 'R Tamari', 'A Shashua'],
         'cites': '12',
         'eprint': 'https://arxiv.org/pdf/1703.06846',
         'gsrank': '1',
         'title': 'Boosting dilated convolutional networks with mixed tensor '
                  'decompositions',
         'url': 'https://arxiv.org/abs/1703.06846',
         'venue': 'arXiv preprint arXiv:1703.06846',
         'year': '2017'},
 'citations_link': '/scholar?cites=5878589884999737901&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBoosting%2BDilated%2BConvolutional%2BNetworks%2Bwith%2BMixed%2BTensor%2BDecompositions%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=LSKds1rylFEJ&ei=E8cqX7b2BqOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:LSKds1rylFEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJb3A_y0f2M3Ei2F7g_R9l19yGDU2I&scisig=AAGBfm0AAAAAXyrJbyuVglngv-alxo0qXTUYTOg18uRP&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 14:49:59
Got the results of the query
{'bib': {'abstract': 'We study the problem of generating source code in a '
                     'strongly typed, Java-like programming language, given a '
                     'label (for example a set of API calls or types) carrying '
                     'a small amount of information about the code that is '
                     'desired. The generated programs are expected to respect '
                     'a" realistic" relationship between programs and labels, '
                     'as exemplified by a corpus of labeled programs available '
                     'during training. Two challenges in such conditional '
                     'program generation are that the generated programs must '
                     'satisfy a rich set of syntactic and semantic constraints',
         'author': ['V Murali', 'L Qi', 'S Chaudhuri', 'C Jermaine'],
         'cites': '47',
         'eprint': 'https://arxiv.org/pdf/1703.05698',
         'gsrank': '1',
         'title': 'Neural sketch learning for conditional program generation',
         'url': 'https://arxiv.org/abs/1703.05698',
         'venue': 'arXiv preprint arXiv:1703.05698',
         'year': '2017'},
 'citations_link': '/scholar?cites=11134234129920472875&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BSketch%2BLearning%2Bfor%2BConditional%2BProgram%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=K7fwD_O-hJoJ&ei=GscqX9_CMKiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:K7fwD_O-hJoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJeT-xax83MwsydIKDARxLhPEEFCVK&scisig=AAGBfm0AAAAAXyrJeV1uWo9a1V6Gw3YfD1y9OKD97E1C&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
47
-------------------------------------------------
2020-08-05 14:50:09
Got the results of the query
{'bib': {'abstract': 'We describe a new training methodology for generative '
                     'adversarial networks. The key idea is to grow both the '
                     'generator and discriminator progressively: starting from '
                     'a low resolution, we add new layers that model '
                     'increasingly fine details as training progresses. This '
                     'both speeds the training up and greatly stabilizes it, '
                     'allowing us to produce images of unprecedented quality, '
                     'eg, CelebA images at 1024^ 2. We also propose a simple '
                     'way to increase the variation in generated images, and '
                     'achieve a record inception score of 8.80 in',
         'author': ['T Karras', 'T Aila', 'S Laine', 'J Lehtinen'],
         'cites': '1963',
         'eprint': 'https://arxiv.org/pdf/1710.10196.pdf?__hstc=200028081.1bb630f9cde2cb5f07430159d50a3c91.1524009600081.1524009600082.1524009600083.1&__hssc=200028081.1.1524009600084&__hsfp=1773666937',
         'gsrank': '1',
         'title': 'Progressive growing of gans for improved quality, '
                  'stability, and variation',
         'url': 'https://arxiv.org/abs/1710.10196',
         'venue': 'arXiv preprint arXiv:1710.10196',
         'year': '2017'},
 'citations_link': '/scholar?cites=11486098150916361186&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DProgressive%2BGrowing%2Bof%2BGANs%2Bfor%2BImproved%2BQuality,%2BStability,%2Band%2BVariation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=4gukjGnRZp8J&ei=JccqX4CsM6OGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:4gukjGnRZp8J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJg8wLsaLLLAYqZJgvXMwLCVm1cnRa&scisig=AAGBfm0AAAAAXyrJg_6RfypIo8tOG88DhFR_FOwVF_9v&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
1963
-------------------------------------------------
2020-08-05 14:50:19
Got the results of the query
{'bib': {'abstract': 'Policy gradient methods have enjoyed great success in '
                     'deep reinforcement learning but suffer from high '
                     'variance of gradient estimates. The high variance '
                     'problem is particularly exasperated in problems with '
                     'long horizons or high-dimensional action spaces. To '
                     'mitigate this issue, we derive a bias-free '
                     'action-dependent baseline for variance reduction which '
                     'fully exploits the structural form of the stochastic '
                     'policy itself and does not make any additional '
                     'assumptions about the MDP. We demonstrate and quantify '
                     'the benefit of the',
         'author': ['C Wu', 'A Rajeswaran', 'Y Duan', 'V Kumar'],
         'cites': '60',
         'eprint': 'https://arxiv.org/pdf/1803.07246',
         'gsrank': '1',
         'title': 'Variance reduction for policy gradient with '
                  'action-dependent factorized baselines',
         'url': 'https://arxiv.org/abs/1803.07246',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11001316948042198974&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariance%2BReduction%2Bfor%2BPolicy%2BGradient%2Bwith%2BAction-Dependent%2BFactorized%2BBaselines%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=vi-KDXqHrJgJ&ei=MMcqX6-sOovrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:vi-KDXqHrJgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJje4t7-vy6oTx1bDIeytC4Vj2h6so&scisig=AAGBfm0AAAAAXyrJjXfaGI_rQFNGGOWiDKxx7GDeZwPb&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
60
-------------------------------------------------
2020-08-05 14:50:29
Got the results of the query
{'bib': {'abstract': 'The current dominant paradigm for imitation learning '
                     'relies on strong supervision of expert actions to learn '
                     "both'what'and'how'to imitate. We pursue an alternative "
                     'paradigm wherein an agent first explores the world '
                     'without any expert supervision and then distills its '
                     'experience',
         'author': ['D Pathak', 'P Mahmoudieh', 'G Luo'],
         'cites': '111',
         'eprint': 'http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w40/Pathak_Zero-Shot_Visual_Imitation_CVPR_2018_paper.pdf',
         'gsrank': '1',
         'title': 'Zero-shot visual imitation',
         'url': 'http://openaccess.thecvf.com/content_cvpr_2018_workshops/w40/html/Pathak_Zero-Shot_Visual_Imitation_CVPR_2018_paper.html',
         'venue': 'Proceedings of the …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15276541363750863723&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DZero-Shot%2BVisual%2BImitation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=a4Puhm4tAdQJ&ei=OscqX83hM7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:a4Puhm4tAdQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJllzOb8A8kMVV37tr-DTfM0WNfgWR&scisig=AAGBfm0AAAAAXyrJlr1TOZbmI7dY885JxgBHVkApkLSl&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
111
-------------------------------------------------
2020-08-05 14:50:38
Got the results of the query
{'bib': {'abstract': 'The driving force behind the recent success of LSTMs has '
                     'been their ability to learn complex and non-linear '
                     'relationships. Consequently, our inability to describe '
                     'these relationships has led to LSTMs being characterized '
                     'as black boxes. To this end, we introduce contextual '
                     'decomposition (CD), an interpretation algorithm for '
                     'analysing individual predictions made by standard LSTMs, '
                     'without any changes to the underlying model. By '
                     'decomposing the output of a LSTM, CD captures the '
                     'contributions of combinations of words or variables to '
                     'the final',
         'author': ['WJ Murdoch', 'PJ Liu', 'B Yu'],
         'cites': '86',
         'eprint': 'https://arxiv.org/pdf/1801.05453',
         'gsrank': '1',
         'title': 'Beyond word importance: Contextual decomposition to extract '
                  'interactions from LSTMs',
         'url': 'https://arxiv.org/abs/1801.05453',
         'venue': 'arXiv preprint arXiv:1801.05453',
         'year': '2018'},
 'citations_link': '/scholar?cites=9223539489272553209&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DBeyond%2BWord%2BImportance:%2BContextual%2BDecomposition%2Bto%2BExtract%2BInteractions%2Bfrom%2BLSTMs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=-d7gDUyYAIAJ&ei=QscqX__8CsKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:-d7gDUyYAIAJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJnDWsE3fb-1nndNrL482-6SOkwM0o&scisig=AAGBfm0AAAAAXyrJnP_dmTME21sLtg6C6SWt3T_Fvalo&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
86
-------------------------------------------------
2020-08-05 14:50:45
Got the results of the query
{'bib': {'abstract': 'Generative models provide a way to model structure in '
                     'complex distributions and have been shown to be useful '
                     'for many tasks of practical interest. However, current '
                     'techniques for training generative models require access '
                     'to fully-observed samples. In many settings, it is '
                     'expensive or even impossible to obtain fullyobserved '
                     'samples, but economical to obtain partial, noisy '
                     'observations. We consider the task of learning an '
                     'implicit generative model given only lossy measurements '
                     'of samples from the distribution of interest. We show '
                     'that the',
         'author': ['A Bora', 'E Price', 'AG Dimakis'],
         'cites': '65',
         'eprint': 'https://pdfs.semanticscholar.org/b959/d5655a3b2f92c2c1a8a7896fecafafea979d.pdf',
         'gsrank': '1',
         'title': 'AmbientGAN: Generative models from lossy measurements.',
         'url': 'https://pdfs.semanticscholar.org/b959/d5655a3b2f92c2c1a8a7896fecafafea979d.pdf',
         'venue': 'ICLR',
         'year': '2018'},
 'citations_link': '/scholar?cites=16888678630950428815&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAmbientGAN:%2BGenerative%2Bmodels%2Bfrom%2Blossy%2Bmeasurements%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=j-CHG7yjYOoJ&ei=SccqX6aqHI-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:j-CHG7yjYOoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJp39uWr-7b66JCp1kBr75vtugTvhG&scisig=AAGBfm0AAAAAXyrJpwY07_vdWKP0--kPZIXpS_0ivJMN&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
65
-------------------------------------------------
2020-08-05 14:50:56
Got the results of the query
{'bib': {'abstract': 'In this work, we face the problem of unsupervised domain '
                     'adaptation with a novel deep learning approach which '
                     'leverages on our finding that entropy minimization is '
                     'induced by the optimal alignment of second order '
                     'statistics between source and target domains. We '
                     'formally demonstrate this hypothesis and, aiming at '
                     'achieving an optimal alignment in practical cases, we '
                     'adopt a more principled strategy which, differently from '
                     'the current Euclidean approaches, deploys alignment '
                     'along geodesics. Our pipeline can be',
         'author': ['P Morerio', 'J Cavazza', 'V Murino'],
         'cites': '48',
         'eprint': 'https://arxiv.org/pdf/1711.10288',
         'gsrank': '1',
         'title': 'Minimal-entropy correlation alignment for unsupervised deep '
                  'domain adaptation',
         'url': 'https://arxiv.org/abs/1711.10288',
         'venue': 'arXiv preprint arXiv:1711.10288',
         'year': '2017'},
 'citations_link': '/scholar?cites=8549844875925427078&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMinimal-Entropy%2BCorrelation%2BAlignment%2Bfor%2BUnsupervised%2BDeep%2BDomain%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=hpM6mp0mp3YJ&ei=VscqX9nYBIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:hpM6mp0mp3YJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJs-VF3o4q_l8h6MjMtm9k3i0vYj7c&scisig=AAGBfm0AAAAAXyrJszo8SAeeZWSuy8eiZ54kYtZZp8I2&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
48
-------------------------------------------------
2020-08-05 14:51:07
Got the results of the query
{'bib': {'abstract': 'This paper presents a novel two-step approach for the '
                     'fundamental problem of learning an optimal map from one '
                     'distribution to another. First, we learn an optimal '
                     'transport (OT) plan, which can be thought as a '
                     'one-to-many map between the two distributions. To that '
                     'end, we propose a stochastic dual approach of '
                     'regularized OT, and show empirically that it scales '
                     'better than a recent related approach when the amount of '
                     'samples is very large. Second, we estimate a\\textit '
                     '{Monge map} as a deep neural network learned by '
                     'approximating the',
         'author': ['V Seguy', 'BB Damodaran', 'R Flamary', 'N Courty'],
         'cites': '61',
         'eprint': 'https://arxiv.org/pdf/1711.02283',
         'gsrank': '1',
         'title': 'Large-scale optimal transport and mapping estimation',
         'url': 'https://arxiv.org/abs/1711.02283',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=513370095015629698&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLarge%2BScale%2BOptimal%2BTransport%2Band%2BMapping%2BEstimation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ghO3p2HbHwcJ&ei=YccqX5LlLsKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ghO3p2HbHwcJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJvmAl3RFy9CZPKOjqwYhBOV9Z_d1J&scisig=AAGBfm0AAAAAXyrJvl8lxZSLoRhKhLX9H3ZMZS44vboR&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
61
-------------------------------------------------
2020-08-05 14:51:18
Got the results of the query
{'bib': {'abstract': 'In this paper, we propose to combine imitation and '
                     'reinforcement learning via the idea of reward shaping '
                     'using an oracle. We study the effectiveness of the '
                     'near-optimal cost-to-go oracle on the planning horizon '
                     'and demonstrate that the cost-to-go oracle shortens the '
                     "learner's planning horizon as function of its accuracy: "
                     'a globally optimal oracle can shorten the planning '
                     'horizon to one, leading to a one-step greedy Markov '
                     'Decision Process which is much easier to optimize, while '
                     'an oracle that is far away from the optimality requires',
         'author': ['W Sun', 'JA Bagnell', 'B Boots'],
         'cites': '32',
         'eprint': 'https://arxiv.org/pdf/1805.11240',
         'gsrank': '1',
         'title': 'Truncated horizon policy search: Combining reinforcement '
                  'learning & imitation learning',
         'url': 'https://arxiv.org/abs/1805.11240',
         'venue': 'arXiv preprint arXiv:1805.11240',
         'year': '2018'},
 'citations_link': '/scholar?cites=782954345548959677&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTruncated%2Bhorizon%2BPolicy%2BSearch:%2BCombining%2BReinforcement%2BLearning%2B%2526%252338%253B%2BImitation%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=vU_LOdSc3QoJ&ei=bMcqX8PdOJqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:vU_LOdSc3QoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJz03pUN0hLmJTeYyW74XMuPE4CxkX&scisig=AAGBfm0AAAAAXyrJzxTGx7TTCViuU3D-6rrAKl3nwyxc&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
32
-------------------------------------------------
2020-08-05 14:51:35
Got the results of the query
{'bib': {'abstract': 'Model-free reinforcement learning (RL) methods are '
                     'succeeding in a growing number of tasks, aided by recent '
                     'advances in deep learning. However, they tend to suffer '
                     'from high sample complexity, which hinders their use in '
                     'real-world domains. Alternatively, model-based '
                     'reinforcement learning promises to reduce sample '
                     'complexity, but tends to require careful tuning and to '
                     'date have succeeded mainly in restrictive domains where '
                     'simple models are sufficient for learning. In this '
                     'paper, we analyze the behavior of vanilla model',
         'author': ['T Kurutach', 'I Clavera', 'Y Duan', 'A Tamar'],
         'cites': '131',
         'eprint': 'https://arxiv.org/pdf/1802.10592',
         'gsrank': '1',
         'title': 'Model-ensemble trust-region policy optimization',
         'url': 'https://arxiv.org/abs/1802.10592',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=5763230631763342838&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModel-Ensemble%2BTrust-Region%2BPolicy%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=9nXnhLkb-08J&ei=fccqX7LdKojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:9nXnhLkb-08J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJ2kR2_LZjcgMwXfM8GLra3xhzv3Ri&scisig=AAGBfm0AAAAAXyrJ2m_VHO-rcIOSW0UEiv2cvTs8kksR&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
131
-------------------------------------------------
2020-08-05 14:51:46
Got the results of the query
{'bib': {'abstract': 'We present sketch-rnn, a recurrent neural network (RNN) '
                     'able to construct stroke-based drawings of common '
                     'objects. The model is trained on thousands of crude '
                     'human-drawn images representing hundreds of classes. We '
                     'outline a framework for conditional and unconditional '
                     'sketch generation, and describe new robust training '
                     'methods for generating coherent sketch drawings in a '
                     'vector format. Subjects: Neural and Evolutionary '
                     'Computing (cs. NE); Machine Learning (cs. LG); Machine '
                     'Learning (stat. ML)',
         'author': ['D Ha', 'D Eck'],
         'cites': '311',
         'eprint': 'https://arxiv.org/pdf/1704.03477.pdf?source=post_page---------------------------',
         'gsrank': '1',
         'title': 'A neural representation of sketch drawings',
         'url': 'https://arxiv.org/abs/1704.03477',
         'venue': 'arXiv preprint arXiv:1704.03477',
         'year': '2017'},
 'citations_link': '/scholar?cites=2337146750300919321&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BNeural%2BRepresentation%2Bof%2BSketch%2BDrawings%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=GRalL5Y2byAJ&ei=h8cqX5iQLsKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:GRalL5Y2byAJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJ4onhQ5qxZ63Uw4juqiizWOGU4GVq&scisig=AAGBfm0AAAAAXyrJ4tMWwgg9DXx9hAgfPuCuPz9F7XUb&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
311
-------------------------------------------------
2020-08-05 14:51:54
Got the results of the query
{'bib': {'abstract': 'We propose a new output layer for deep neural networks '
                     'that permits the use of logged contextual bandit '
                     'feedback for training. Such contextual bandit feedback '
                     'can be available in huge quantities (eg, logs of search '
                     'engines, recommender systems) at little cost, opening up',
         'author': ['T Joachims', 'A Swaminathan'],
         'cites': '44',
         'eprint': 'https://openreview.net/pdf?id=SJaP_-xAb',
         'gsrank': '1',
         'title': 'Deep learning with logged bandit feedback',
         'url': 'https://openreview.net/forum?id=SJaP_-xAb&noteId=SJaP_-xAb',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8863477731568200162&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLearning%2Bwith%2BLogged%2BBandit%2BFeedback%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=4im1QAlmAXsJ&ei=kscqX-zfDaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:4im1QAlmAXsJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJ8aK5bN189Wfpd7xOkA6XhZDYcjTC&scisig=AAGBfm0AAAAAXyrJ8dG9ItmpdNX3uC5smcT-JPE8AN1F&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
44
-------------------------------------------------
2020-08-05 14:52:09
Got the results of the query
{'bib': {'abstract': 'Permutations and matchings are core building blocks in a '
                     'variety of latent variable models, as they allow us to '
                     'align, canonicalize, and sort data. Learning in such '
                     'models is difficult, however, because exact '
                     'marginalization over these combinatorial objects is '
                     'intractable. In response, this paper introduces a '
                     'collection of new methods for end-to-end learning in '
                     'such models that approximate discrete maximum-weight '
                     'matching using the continuous Sinkhorn operator. '
                     'Sinkhorn iteration is attractive because it functions as '
                     'a simple, easy-to-implement',
         'author': ['G Mena', 'D Belanger', 'S Linderman', 'J Snoek'],
         'cites': '60',
         'eprint': 'https://arxiv.org/pdf/1802.08665',
         'gsrank': '1',
         'title': 'Learning latent permutations with gumbel-sinkhorn networks',
         'url': 'https://arxiv.org/abs/1802.08665',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=17995429437153045101&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BLatent%2BPermutations%2Bwith%2BGumbel-Sinkhorn%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=bWodrrqbvPkJ&ei=nccqX6y9EoyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:bWodrrqbvPkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrJ_5Law-7p3ndAHHLvWwdljW-xmc3_&scisig=AAGBfm0AAAAAXyrJ_0PitLjU_me7IXKJ9cfnmcCGL4gV&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
60
-------------------------------------------------
2020-08-05 14:52:23
Got the results of the query
{'bib': {'abstract': 'We present a method for reinforcement learning of '
                     'closely related skills that are parameterized via a '
                     'skill embedding space. We learn such skills by taking '
                     'advantage of latent variables and exploiting a '
                     'connection between reinforcement learning and '
                     'variational inference. The main contribution of our work '
                     'is an entropy-regularized policy gradient formulation '
                     'for hierarchical policies, and an associated, '
                     'data-efficient and robust off-policy gradient algorithm '
                     'based on stochastic value gradients. We demonstrate the '
                     'effectiveness of',
         'author': ['K Hausman', 'JT Springenberg', 'Z Wang'],
         'cites': '90',
         'eprint': 'https://openreview.net/pdf?id=rk07ZXZRb',
         'gsrank': '1',
         'title': 'Learning an embedding space for transferable robot skills',
         'url': 'https://openreview.net/forum?id=rk07ZXZRb',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1816310193271208067&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Ban%2BEmbedding%2BSpace%2Bfor%2BTransferable%2BRobot%2BSkills%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=g_jZX3_UNBkJ&ei=rscqX5bPG4yimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:g_jZX3_UNBkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrKDQUX-RPOHjEpk-tnCt4U2Bnpn3jf&scisig=AAGBfm0AAAAAXyrKDYNdPVrxQMRvyV9cJrdIWNVgmr9G&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
90
-------------------------------------------------
2020-08-05 14:52:37
Got the results of the query
{'bib': {'abstract': 'Intrinsically motivated goal exploration algorithms '
                     'enable machines to discover repertoires of policies that '
                     'produce a diversity of effects in complex environments. '
                     'These exploration algorithms have been shown to allow '
                     'real world robots to acquire skills such as tool use in '
                     'high-dimensional continuous state and action spaces. '
                     'However, they have so far assumed that self-generated '
                     'goals are sampled in a specifically engineered feature '
                     'space, limiting their autonomy. In this work, we propose '
                     'to use deep representation learning algorithms to',
         'author': ['A Péré', 'S Forestier', 'O Sigaud', 'PY Oudeyer'],
         'cites': '42',
         'eprint': 'https://arxiv.org/pdf/1803.00781',
         'gsrank': '1',
         'title': 'Unsupervised learning of goal spaces for intrinsically '
                  'motivated goal exploration',
         'url': 'https://arxiv.org/abs/1803.00781',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=17844977813077230695&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BLearning%2Bof%2BGoal%2BSpaces%2Bfor%2BIntrinsically%2BMotivated%2BGoal%2BExploration%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ZwQyNcYYpvcJ&ei=u8cqX76rCcKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ZwQyNcYYpvcJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrKHIGuyvsIkGTv9DXOCSEJE_uhCy8x&scisig=AAGBfm0AAAAAXyrKHOCm_zseZv0eAb7OreUeN7U-1kPY&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
42
-------------------------------------------------
2020-08-05 14:52:52
Got the results of the query
{'bib': {'abstract': 'The development of high-dimensional generative models '
                     'has recently gained a great surge of interest with the '
                     'introduction of variational auto-encoders and generative '
                     'adversarial neural networks. Different variants have '
                     'been proposed where the underlying latent space is '
                     'structured, for example, based on attributes describing '
                     'the data to generate. We focus on a particular problem '
                     'where one aims at generating samples corresponding to a '
                     'number of objects under various views. We assume that '
                     'the distribution of the data is driven by two',
         'author': ['M Chen', 'L Denoyer', 'T Artières'],
         'cites': '14',
         'eprint': 'https://arxiv.org/pdf/1711.00305',
         'gsrank': '1',
         'title': 'Multi-view data generation without view supervision',
         'url': 'https://arxiv.org/abs/1711.00305',
         'venue': 'arXiv preprint arXiv:1711.00305',
         'year': '2017'},
 'citations_link': '/scholar?cites=15286827840377806140&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-View%2BData%2BGeneration%2BWithout%2BView%2BSupervision%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=PF1FgO24JdQJ&ei=0ccqX_XyKbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:PF1FgO24JdQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrKLsfldiLVKskqAIeF5LbDPtXbxYjL&scisig=AAGBfm0AAAAAXyrKLiD-XolFeMYngyKbn63GzQplUAIu&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
14
-------------------------------------------------
2020-08-05 14:53:10
Got the results of the query
{'bib': {'abstract': 'Recent advances in deep reinforcement learning have made '
                     'significant strides in performance on applications such '
                     'as Go and Atari games. However, developing practical '
                     'methods to balance exploration and exploitation in '
                     'complex domains remains largely unsolved. Thompson '
                     'Sampling and its extension to reinforcement learning '
                     'provide an elegant approach to exploration that only '
                     'requires access to posterior samples of the model. At '
                     'the same time, advances in approximate Bayesian methods '
                     'have made posterior',
         'author': ['C Riquelme', 'G Tucker', 'J Snoek'],
         'cites': '73',
         'eprint': 'https://arxiv.org/pdf/1802.09127',
         'gsrank': '1',
         'title': 'Deep bayesian bandits showdown: An empirical comparison of '
                  'bayesian deep networks for thompson sampling',
         'url': 'https://arxiv.org/abs/1802.09127',
         'venue': 'arXiv preprint arXiv:1802.09127',
         'year': '2018'},
 'citations_link': '/scholar?cites=15082403977285558392&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BBayesian%2BBandits%2BShowdown:%2BAn%2BEmpirical%2BComparison%2Bof%2BBayesian%2BDeep%2BNetworks%2Bfor%2BThompson%2BSampling%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=eGAL4YF2T9EJ&ei=2scqX9PxEojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:eGAL4YF2T9EJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrKNttmvaf3iIqc7ChljeSSuNEV1iJv&scisig=AAGBfm0AAAAAXyrKNsayQC7ooHSM4XJgnrV42_iw3S_n&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
73
-------------------------------------------------
2020-08-05 14:53:18
Got the results of the query
{'bib': {'abstract': 'In implicit models, one often interpolates between '
                     'sampled points in latent space. As we show in this '
                     'paper, care needs to be taken to match-up the '
                     'distributional assumptions on code vectors with the '
                     'geometry of the interpolating paths. Otherwise, typical '
                     'assumptions about the quality and semantics of '
                     'in-between points may not be justified. Based on our '
                     'analysis we propose to modify the prior code '
                     'distribution to put significantly more probability mass '
                     'closer to the origin. As a result, linear interpolation '
                     'paths are not only shortest paths',
         'author': ['Y Kilcher', 'A Lucchi', 'T Hofmann'],
         'cites': '5',
         'eprint': 'https://arxiv.org/pdf/1710.11381',
         'gsrank': '1',
         'title': 'Semantic interpolation in implicit models',
         'url': 'https://arxiv.org/abs/1710.11381',
         'venue': 'arXiv preprint arXiv:1710.11381',
         'year': '2017'},
 'citations_link': '/scholar?cites=11943550239831757729&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSemantic%2BInterpolation%2Bin%2BImplicit%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=oRM02qkDwKUJ&ei=58cqX7P7H6iBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:oRM02qkDwKUJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrKRGDOR-HZTTu1c2tpBCQ1E1Z6MWl3&scisig=AAGBfm0AAAAAXyrKRHi6lXBWXEkxBoo0DqwNIwsW48Ro&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 14:53:32
Got the results of the query
{'bib': {'abstract': 'Training deep neural networks requires many training '
                     'samples, but in practice training labels are expensive '
                     'to obtain and may be of varying quality, as some may be '
                     'from trusted expert labelers while others might be from '
                     'heuristics or other sources of weak supervision',
         'author': ['M Dehghani', 'A Mehrjou', 'S Gouws', 'J Kamps'],
         'cites': '29',
         'eprint': 'https://arxiv.org/pdf/1711.02799',
         'gsrank': '1',
         'title': 'Fidelity-weighted learning',
         'url': 'https://arxiv.org/abs/1711.02799',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=6679593996852506297&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFidelity-Weighted%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ua45olmvslwJ&ei=78cqX5ryIIjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ua45olmvslwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrKT6_0SmaBuTdayKjso1D4HRC4VThc&scisig=AAGBfm0AAAAAXyrKT8Y5gazrm4wZaKiiUAzIUt_jfjGF&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
29
-------------------------------------------------
2020-08-05 14:53:43
Got the results of the query
{'bib': {'abstract': 'Deep generative models provide a systematic way to learn '
                     'nonlinear data distributions, through a set of latent '
                     'variables and a nonlinear" generator" function that maps '
                     'latent points into the input space. The nonlinearity of '
                     'the generator imply that the latent space gives a '
                     'distorted view of the input space. Under mild '
                     'conditions, we show that this distortion can be '
                     'characterized by a stochastic Riemannian metric, and '
                     'demonstrate that distances and interpolants are '
                     'significantly improved under this metric. This in turn '
                     'improves probability',
         'author': ['G Arvanitidis', 'LK Hansen', 'S Hauberg'],
         'cites': '60',
         'eprint': 'https://arxiv.org/pdf/1710.11379',
         'gsrank': '1',
         'title': 'Latent space oddity: on the curvature of deep generative '
                  'models',
         'url': 'https://arxiv.org/abs/1710.11379',
         'venue': 'arXiv preprint arXiv:1710.11379',
         'year': '2017'},
 'citations_link': '/scholar?cites=10064730276614480748&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLatent%2BSpace%2BOddity:%2Bon%2Bthe%2BCurvature%2Bof%2BDeep%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=bKO5gdwarYsJ&ei=-8cqX7LgMI-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:bKO5gdwarYsJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrKWlPPmTpRqslObyAkrWnvwlf_Benu&scisig=AAGBfm0AAAAAXyrKWg3GvMpyRq1SMQo2MWMxazXH8D2s&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
60
-------------------------------------------------
2020-08-05 14:53:54
Got the results of the query
{'bib': {'abstract': 'Recent advances in learning from demonstrations (LfD) '
                     'with deep neural networks have enabled learning complex '
                     'robot skills that involve high dimensional perception '
                     'such as raw image inputs. LfD algorithms generally '
                     'assume learning from single task demonstrations. In '
                     'practice, however, it is more efficient for a teacher to '
                     'demonstrate a multitude of tasks without careful task '
                     'set up, labeling, and engineering. Unfortunately in such '
                     'cases, traditional imitation learning techniques fail to '
                     'represent the multi-modal nature of the data, and often',
         'author': ['A Tamar', 'K Rohanimanesh', 'Y Chow'],
         'cites': '3',
         'eprint': 'https://pdfs.semanticscholar.org/5808/839715d5d94b0646b74bbebd3db23f521823.pdf',
         'gsrank': '1',
         'title': 'Imitation Learning from Visual Data with Multiple '
                  'Intentions.',
         'url': 'https://pdfs.semanticscholar.org/5808/839715d5d94b0646b74bbebd3db23f521823.pdf',
         'venue': 'ICLR …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1679668584294504646&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImitation%2BLearning%2Bfrom%2BVisual%2BData%2Bwith%2BMultiple%2BIntentions%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=xthBm61hTxcJ&ei=D8gqX6D9IMKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:xthBm61hTxcJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrKa-cdyZAy64YBhTKFdRIP-w8U9WLA&scisig=AAGBfm0AAAAAXyrKa_9mZMRZEYt7Nbk0rRGoSz_mk9Rn&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
3
-------------------------------------------------
2020-08-05 14:54:11
Got the results of the query
{'bib': {'abstract': 'We give a simple, fast algorithm for hyperparameter '
                     'optimization inspired by techniques from the analysis of '
                     'Boolean functions. We focus on the high-dimensional '
                     'regime where the canonical example is training a neural '
                     'network with a large number of hyperparameters. The '
                     'algorithm---an iterative application of compressed '
                     'sensing techniques for orthogonal polynomials---requires '
                     'only uniform sampling of the hyperparameters and is thus '
                     'easily parallelizable. Experiments for training deep '
                     'neural networks on Cifar-10 show that',
         'author': ['E Hazan', 'A Klivans', 'Y Yuan'],
         'cites': '45',
         'eprint': 'https://arxiv.org/pdf/1706.00764',
         'gsrank': '1',
         'title': 'Hyperparameter optimization: A spectral approach',
         'url': 'https://arxiv.org/abs/1706.00764',
         'venue': 'arXiv preprint arXiv:1706.00764',
         'year': '2017'},
 'citations_link': '/scholar?cites=11236398750787903780&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHyperparameter%2Boptimization:%2Ba%2Bspectral%2Bapproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=JCFbfyG175sJ&ei=GMgqX7rSMI-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:JCFbfyG175sJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrKdpN4-yeeR5gI7zuTCcV_LehtI7Sc&scisig=AAGBfm0AAAAAXyrKdnyYoH-CsW2ffWoZ-QyQcWbfvBYT&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
45
-------------------------------------------------
2020-08-05 14:54:22
Got the results of the query
{'bib': {'abstract': 'Program synthesis is the task of automatically '
                     'generating a program consistent with a specification. '
                     'Recent years have seen proposal of a number of neural '
                     'approaches for program synthesis, many of which adopt a '
                     'sequence generation paradigm similar to neural machine '
                     'translation, in which sequence-to-sequence models are '
                     'trained to maximize the likelihood of known reference '
                     'programs. While achieving impressive results, this '
                     'strategy has two key limitations. First, it ignores '
                     'Program Aliasing: the fact that many different',
         'author': ['R Bunel', 'M Hausknecht', 'J Devlin', 'R Singh'],
         'cites': '68',
         'eprint': 'https://arxiv.org/pdf/1805.04276',
         'gsrank': '1',
         'title': 'Leveraging grammar and reinforcement learning for neural '
                  'program synthesis',
         'url': 'https://arxiv.org/abs/1805.04276',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=14342984430007145123&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLeveraging%2BGrammar%2Band%2BReinforcement%2BLearning%2Bfor%2BNeural%2BProgram%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=o9KIwViEDMcJ&ei=IcgqX_3HJYyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:o9KIwViEDMcJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrKfOr0hTgGncSITK35MMdWheycKADy&scisig=AAGBfm0AAAAAXyrKfEmxQJYwaxK2pjO6kyc5h-1eedsw&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
68
-------------------------------------------------
2020-08-05 14:54:28
Got the results of the query
{'bib': {'abstract': 'Convolutional Neural Networks (CNNs) are computationally '
                     'intensive, which limits their application on mobile '
                     'devices. Their energy is dominated by the number of '
                     'multiplies needed to perform the convolutions. '
                     "Winograd's minimal filtering algorithm (Lavin, 2015) and "
                     'network pruning (Han et al., 2015) can reduce the '
                     'operation count, but these two methods cannot be '
                     'directly combined $-$ applying the Winograd transform '
                     'fills in the sparsity in both the weights and the '
                     'activations. We propose two modifications to Winograd',
         'author': ['X Liu', 'J Pool', 'S Han', 'WJ Dally'],
         'cites': '58',
         'eprint': 'https://arxiv.org/pdf/1802.06367',
         'gsrank': '1',
         'title': 'Efficient sparse-winograd convolutional neural networks',
         'url': 'https://arxiv.org/abs/1802.06367',
         'venue': 'arXiv preprint arXiv:1802.06367',
         'year': '2018'},
 'citations_link': '/scholar?cites=5437414522331578688&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEfficient%2BSparse-Winograd%2BConvolutional%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=QMEfDLOTdUsJ&ei=K8gqX7TLCo-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:QMEfDLOTdUsJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrKhvoGqUO7z3EUxbRuhIFxbNKo7jhs&scisig=AAGBfm0AAAAAXyrKhgr_q76LDeinIOAuSWvG65iNr4Wl&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
58
-------------------------------------------------
2020-08-05 14:54:38
Got the results of the query
{'bib': {'abstract': 'There are many applications scenarios for which the '
                     'computational performance and memory footprint of the '
                     'prediction phase of Deep Neural Networks (DNNs) need to '
                     'be optimized. Binary Deep Neural Networks (BDNNs) have '
                     'been shown to be an effective way of achieving this '
                     'objective. In this paper, we show how Convolutional '
                     'Neural Networks (CNNs) can be implemented using binary '
                     'representations. Espresso is a compact, yet powerful '
                     'library written in C/CUDA that features all the '
                     'functionalities required for the forward',
         'author': ['F Pedersoli', 'G Tzanetakis'],
         'cites': '8',
         'eprint': 'https://openreview.net/pdf?id=Sk6fD5yCb',
         'gsrank': '1',
         'title': 'Espresso: Efficient forward propagation for binary deep '
                  'neural networks',
         'url': 'https://openreview.net/forum?id=Sk6fD5yCb&noteId=Sk6fD5yCb',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1646875788450055806&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEspresso:%2BEfficient%2BForward%2BPropagation%2Bfor%2BBinary%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=fsbB183g2hYJ&ei=NcgqX8r3I4vrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:fsbB183g2hYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrKlISeuAORdYXoU8YDPBgJjNqFBb_u&scisig=AAGBfm0AAAAAXyrKlDY8xGQaXLwdgC5EwflL2UOujfxd&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
8
-------------------------------------------------
2020-08-05 14:54:52
Got the results of the query
{'bib': {'abstract': 'We present a real-time method for synthesizing highly '
                     'complex human motions using a novel training regime we '
                     'call the auto-conditioned Recurrent Neural Network '
                     '(acRNN). Recently, researchers have attempted to '
                     'synthesize new motion by using autoregressive '
                     'techniques, but existing methods tend to freeze or '
                     'diverge after a couple of seconds due to an accumulation '
                     'of errors that are fed back into the network. '
                     'Furthermore, such methods have only been shown to be '
                     'reliable for relatively simple human motions, such as '
                     'walking or',
         'author': ['Z Li', 'Y Zhou', 'S Xiao', 'C He', 'Z Huang', 'H Li'],
         'cites': '30',
         'eprint': 'https://arxiv.org/pdf/1707.05363',
         'gsrank': '1',
         'title': 'Auto-conditioned recurrent networks for extended complex '
                  'human motion synthesis',
         'url': 'https://arxiv.org/abs/1707.05363',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=5399907966651514315&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAuto-Conditioned%2BRecurrent%2BNetworks%2Bfor%2BExtended%2BComplex%2BHuman%2BMotion%2BSynthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=y70QjK9T8EoJ&ei=QsgqX8vUPLGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:y70QjK9T8EoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrKoS6pKxsVTfwT0eKT3RHh619o_nSQ&scisig=AAGBfm0AAAAAXyrKoY_KaNIOfGcnswH3InoV4PQpFPQY&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
30
-------------------------------------------------
2020-08-05 14:55:05
Got the results of the query
{'bib': {'abstract': 'We propose a Warped Residual Network (WarpNet) using a '
                     'parallelizable warp operator for forward and backward '
                     'propagation to distant layers that trains faster than '
                     'the original residual neural network. We apply a '
                     'perturbation theory on residual networks and decouple '
                     'the interactions between residual units. The resulting '
                     'warp operator is a first order approximation of the '
                     'output over multiple layers. The first order '
                     'perturbation theory exhibits properties such as binomial '
                     'path lengths and exponential gradient scaling found',
         'author': ['R Fok', 'A An', 'Z Rashidi', 'X Wang'],
         'cites': '0',
         'eprint': 'https://openreview.net/pdf?id=SyMvJrdaW',
         'gsrank': '1',
         'title': 'Decoupling the Layers in Residual Networks',
         'url': 'https://openreview.net/forum?id=SyMvJrdaW&noteId=SyMvJrdaW',
         'venue': 'International Conference on …',
         'year': '2018'},
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDecoupling%2Bthe%2BLayers%2Bin%2BResidual%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=3dzFL1BkfcMJ&ei=TsgqX-qbEsKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:3dzFL1BkfcMJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrKrMt04Eqeipa9EmD-soXoo06g4ML0&scisig=AAGBfm0AAAAAXyrKrETk8r26lptTrdGt0HkCOmH5x-bl&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
0
-------------------------------------------------
2020-08-05 14:55:17
Got the results of the query
{'bib': {'abstract': 'Convolutional neural networks (CNNs) are inherently '
                     'equivariant to translation. Efforts to embed other forms '
                     'of equivariance have concentrated solely on rotation. We '
                     'expand the notion of equivariance in CNNs through the '
                     'Polar Transformer Network (PTN). PTN',
         'author': ['C Esteves', 'C Allen-Blanchette', 'X Zhou'],
         'cites': '62',
         'eprint': 'https://arxiv.org/pdf/1709.01889',
         'gsrank': '1',
         'title': 'Polar transformer networks',
         'url': 'https://arxiv.org/abs/1709.01889',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=15618354521274654533&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPolar%2BTransformer%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=RR-xALCKv9gJ&ei=V8gqX-29NaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:RR-xALCKv9gJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrKtRRDbZezRPBfshTKCLWjVls8G-EF&scisig=AAGBfm0AAAAAXyrKtcLj4bTFTyKtNw9dXuw57DTQf4pv&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
62
-------------------------------------------------
2020-08-05 14:55:25
Got the results of the query
{'bib': {'abstract': 'We consider the problem of detecting out-of-distribution '
                     'images in neural networks. We propose ODIN, a simple and '
                     'effective method that does not require any change to a '
                     'pre-trained neural network. Our method is based on the '
                     'observation that using temperature scaling and adding '
                     'small perturbations to the input can separate the '
                     'softmax score distributions between in-and '
                     'out-of-distribution images, allowing for more effective '
                     'detection. We show in a series of experiments that ODIN '
                     'is compatible with diverse network',
         'author': ['S Liang', 'Y Li', 'R Srikant'],
         'cites': '268',
         'eprint': 'https://arxiv.org/pdf/1706.02690',
         'gsrank': '1',
         'title': 'Enhancing the reliability of out-of-distribution image '
                  'detection in neural networks',
         'url': 'https://arxiv.org/abs/1706.02690',
         'venue': 'arXiv preprint arXiv:1706.02690',
         'year': '2017'},
 'citations_link': '/scholar?cites=7536099354022278878&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnhancing%2BThe%2BReliability%2Bof%2BOut-of-distribution%2BImage%2BDetection%2Bin%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=3naMPG-alWgJ&ei=Y8gqX4i9HqOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:3naMPG-alWgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrKydL1m5kDmSYWjYAB8ogfEZGF_aC4&scisig=AAGBfm0AAAAAXyrKyUav25bbKB34XS84vsAMOGwM-pYU&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
268
-------------------------------------------------
2020-08-05 14:55:45
Got the results of the query
{'bib': {'abstract': 'Adversarial neural networks solve many important '
                     'problems in data science, but are notoriously difficult '
                     'to train. These difficulties come from the fact that '
                     'optimal weights for adversarial nets correspond to '
                     'saddle points, and not minimizers, of the loss function. '
                     'The alternating stochastic gradient methods typically '
                     'used for such problems do not reliably converge to '
                     'saddle points, and when convergence does happen it is '
                     'often highly sensitive to learning rates. We propose a '
                     'simple modification of stochastic gradient descent that',
         'author': ['A Yadav', 'S Shah', 'Z Xu', 'D Jacobs'],
         'cites': '38',
         'eprint': 'https://arxiv.org/pdf/1705.07364',
         'gsrank': '1',
         'title': 'Stabilizing adversarial nets with prediction methods',
         'url': 'https://arxiv.org/abs/1705.07364',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=1304972437215881711&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStabilizing%2BAdversarial%2BNets%2Bwith%2BPrediction%2BMethods%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=76ntgYQxHBIJ&ei=eMgqX-HhBaiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:76ntgYQxHBIJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrK0zFE6X6DC0xZHg7cGO4iuVgqI9gI&scisig=AAGBfm0AAAAAXyrK05wsF7G7zsQ33OzmloKzivVtuAlf&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
38
-------------------------------------------------
2020-08-05 14:55:55
Got the results of the query
{'bib': {'abstract': 'We present graph attention networks (GATs), novel neural '
                     'network architectures that operate on graph-structured '
                     'data, leveraging masked self-attentional layers to '
                     'address the shortcomings of prior methods based on graph '
                     'convolutions or their approximations. By',
         'author': ['P Veličković', 'G Cucurull', 'A Casanova'],
         'cites': '1781',
         'eprint': 'https://arxiv.org/pdf/1710.10903',
         'gsrank': '1',
         'title': 'Graph attention networks',
         'url': 'https://arxiv.org/abs/1710.10903',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=5609128480281463225&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGraph%2BAttention%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=uQm0ZqKg100J&ei=gsgqX-_IG4vrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:uQm0ZqKg100J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrK4Ju1I9iRdqum2VW8sibgGl0k3CZw&scisig=AAGBfm0AAAAAXyrK4On1WRJRN8lGGD1yyhWdwHxNqs0W&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
1781
-------------------------------------------------
2020-08-05 14:56:08
Got the results of the query
{'bib': {'abstract': 'We introduce and study minimax curriculum learning '
                     '(MCL), a new method for adaptively selecting a sequence '
                     'of training subsets for a succession of stages in '
                     'machine learning. The subsets are encouraged to be small '
                     'and diverse early on, and then larger, harder, and '
                     'allowably more homogeneous in later stages. At each '
                     'stage, model weights and training sets are chosen by '
                     'solving a joint continuous-discrete minimax '
                     'optimization, whose objective is composed of a '
                     'continuous loss (reflecting training set hardness) and a '
                     'discrete',
         'author': ['T Zhou', 'J Bilmes'],
         'cites': '11',
         'gsrank': '1',
         'title': 'Minimax curriculum learning: Machine teaching with '
                  'desirable difficulties and scheduled diversity',
         'url': 'https://openreview.net/forum?id=BywyFQlAW&noteId=BywyFQlAW',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15130630439374027502&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMinimax%2BCurriculum%2BLearning:%2BMachine%2BTeaching%2Bwith%2BDesirable%2BDifficulties%2Band%2BScheduled%2BDiversity%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=7vqZFjjM-tEJ&ei=j8gqX87YGZqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:7vqZFjjM-tEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrK71iO_aTEg_M8wKeUgzintiwYyKsv&scisig=AAGBfm0AAAAAXyrK7_UMSdW_SO5FvTS2SDfsuU-Gh-LD&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
11
-------------------------------------------------
2020-08-05 14:56:23
Got the results of the query
{'bib': {'abstract': 'We present a general-purpose method to train Markov '
                     'chain Monte Carlo kernels, parameterized by deep neural '
                     'networks, that converge and mix quickly to their target '
                     'distribution. Our method generalizes Hamiltonian Monte '
                     'Carlo and is trained to maximize expected squared jumped '
                     'distance, a proxy for mixing speed. We demonstrate large '
                     'empirical gains on a collection of simple but '
                     'challenging distributions, for instance achieving a 106x '
                     'improvement in effective sample size in one case, and '
                     'mixing when standard HMC',
         'author': ['D Levy', 'MD Hoffman', 'J Sohl-Dickstein'],
         'cites': '42',
         'eprint': 'https://arxiv.org/pdf/1711.09268',
         'gsrank': '1',
         'title': 'Generalizing hamiltonian monte carlo with neural networks',
         'url': 'https://arxiv.org/abs/1711.09268',
         'venue': 'arXiv preprint arXiv:1711.09268',
         'year': '2017'},
 'citations_link': '/scholar?cites=6189563132756829558&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeneralizing%2BHamiltonian%2BMonte%2BCarlo%2Bwith%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=diXTIuC-5VUJ&ei=ncgqX4jKK7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:diXTIuC-5VUJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrK_V48ffcAn4sxrgaYam6W2TUBBdJC&scisig=AAGBfm0AAAAAXyrK_WpUkOpgC0MFCP-e4yJWBavNv8BY&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
42
-------------------------------------------------
2020-08-05 14:56:37
Got the results of the query
{'bib': {'abstract': 'We consider the problem of training generative models '
                     'with a Generative Adversarial Network (GAN). Although '
                     'GANs can accurately model complex distributions, they '
                     'are known to be difficult to train due to instabilities '
                     'caused by a difficult minimax optimization problem. In '
                     'this paper, we view the problem of training GANs as '
                     'finding a mixed strategy in a zero-sum game. Building on '
                     'ideas from online learning we propose a novel training '
                     'method named Chekhov GAN 1. On the theory side, we show '
                     'that our method provably converges to',
         'author': ['P Grnarova', 'KY Levy', 'A Lucchi', 'T Hofmann'],
         'cites': '44',
         'eprint': 'https://arxiv.org/pdf/1706.03269',
         'gsrank': '1',
         'title': 'An online learning approach to generative adversarial '
                  'networks',
         'url': 'https://arxiv.org/abs/1706.03269',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=13905628345653607313&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2BOnline%2BLearning%2BApproach%2Bto%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=kTtJIU63-sAJ&ei=wMgqX8jDPMKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:kTtJIU63-sAJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrLKimLr84h_19mSB3hSRzES1xKs_yH&scisig=AAGBfm0AAAAAXyrLKiSH4sBud1mZ4rc3lGGxjsgS34nY&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
44
-------------------------------------------------
2020-08-05 14:57:22
Got the results of the query
{'bib': {'abstract': 'We present Optimal Transport GAN (OT-GAN), a variant of '
                     'generative adversarial nets minimizing a new metric '
                     'measuring the distance between the generator '
                     'distribution and the data distribution. This metric, '
                     'which we call mini-batch energy distance, combines '
                     'optimal',
         'author': ['T Salimans', 'H Zhang', 'A Radford', 'D Metaxas'],
         'cites': '109',
         'eprint': 'https://arxiv.org/pdf/1803.05573',
         'gsrank': '1',
         'title': 'Improving GANs using optimal transport',
         'url': 'https://arxiv.org/abs/1803.05573',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=3536589889372403455&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BGANs%2BUsing%2BOptimal%2BTransport%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=_7ZHSMR9FDEJ&ei=1cgqX7fZO4jHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:_7ZHSMR9FDEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrLMkzMm5KSC139cQcH61H-vgtD4vHi&scisig=AAGBfm0AAAAAXyrLMpSsjGh14760CaFQmnshFJwA9EfH&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
109
-------------------------------------------------
2020-08-05 14:57:30
Got the results of the query
{'bib': {'abstract': 'We present an end-to-end trained memory system that '
                     'quickly adapts to new data and generates samples like '
                     "them. Inspired by Kanerva's sparse distributed memory, "
                     'it has a robust distributed reading and writing '
                     'mechanism. The memory is analytically tractable, which '
                     'enables optimal on-line compression via a Bayesian '
                     'update-rule. We formulate it as a hierarchical '
                     'conditional generative model, where memory provides a '
                     'rich data-dependent prior distribution. Consequently, '
                     'the top-down memory and bottom-up perception are',
         'author': ['Y Wu', 'G Wayne', 'A Graves', 'T Lillicrap'],
         'cites': '22',
         'eprint': 'https://arxiv.org/pdf/1804.01756',
         'gsrank': '1',
         'title': 'The kanerva machine: A generative distributed memory',
         'url': 'https://arxiv.org/abs/1804.01756',
         'venue': 'arXiv preprint arXiv:1804.01756',
         'year': '2018'},
 'citations_link': '/scholar?cites=9888262262485457347&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BKanerva%2BMachine:%2BA%2BGenerative%2BDistributed%2BMemory%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=w5lYMSMqOokJ&ei=3cgqX723MI-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:w5lYMSMqOokJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrLO3cFA_MenNFLf3EWTcRD_FIXduMd&scisig=AAGBfm0AAAAAXyrLO_5CSFQDvT0gTJvSHlfBL0WOnInu&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
22
-------------------------------------------------
2020-08-05 14:57:39
Got the results of the query
{'bib': {'abstract': 'Deep neural networks have enabled progress in a wide '
                     'variety of applications. Growing the size of the neural '
                     'network typically results in improved accuracy. As model '
                     'sizes grow, the memory and compute requirements for '
                     'training these models also increases. We introduce a',
         'author': ['P Micikevicius', 'S Narang', 'J Alben', 'G Diamos'],
         'cites': '295',
         'eprint': 'https://arxiv.org/pdf/1710.03740.pdf%EF%BC%89%E3%80%82',
         'gsrank': '1',
         'title': 'Mixed precision training',
         'url': 'https://arxiv.org/abs/1710.03740',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=18172749567892275222&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMixed%2BPrecision%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=FkBa2nKTMvwJ&ei=6MgqX6q7II-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:FkBa2nKTMvwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrLRmMVHwQdgNICInjiye1ufJqSmfz4&scisig=AAGBfm0AAAAAXyrLRioBbQ1lqCHlZCU06RD4NuFDErLj&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
295
-------------------------------------------------
2020-08-05 14:57:50
Got the results of the query
{'bib': {'abstract': 'Deep generative neural networks have proven effective at '
                     'both conditional and unconditional modeling of complex '
                     'data distributions. Conditional generation enables '
                     'interactive control, but creating new controls often '
                     'requires expensive retraining. In this paper, we develop '
                     'a method to condition generation without retraining the '
                     'model. By post-hoc learning latent constraints, value '
                     'functions that identify regions in latent space that '
                     'generate outputs with desired attributes, we can '
                     'conditionally sample from these regions',
         'author': ['J Engel', 'M Hoffman', 'A Roberts'],
         'cites': '56',
         'eprint': 'https://arxiv.org/pdf/1711.05772',
         'gsrank': '1',
         'title': 'Latent constraints: Learning to generate conditionally from '
                  'unconditional generative models',
         'url': 'https://arxiv.org/abs/1711.05772',
         'venue': 'arXiv preprint arXiv:1711.05772',
         'year': '2017'},
 'citations_link': '/scholar?cites=4801471707105268793&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLatent%2BConstraints:%2BLearning%2Bto%2BGenerate%2BConditionally%2Bfrom%2BUnconditional%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ORQZDRRBokIJ&ei=88gqX9XWO6iBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ORQZDRRBokIJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrLU7oZ1ID8jqz8K2n9UtXodJ9IE3Xf&scisig=AAGBfm0AAAAAXyrLU2IVvCyqit-BQGX6y4CnZtYLvRm0&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
56
-------------------------------------------------
2020-08-05 14:58:03
Got the results of the query
{'bib': {'abstract': 'Neural text generation models are often autoregressive '
                     'language models or seq2seq models. These models generate '
                     'text by sampling words sequentially, with each word '
                     'conditioned on the previous word, and are '
                     'state-of-the-art for several machine translation and '
                     'summarization benchmarks. These benchmarks are often '
                     'defined by validation perplexity even though this is not '
                     'a direct measure of the quality of the generated text. '
                     'Additionally, these models are typically trained via '
                     'maxi-mum likelihood and teacher forcing',
         'author': ['W Fedus', 'I Goodfellow', 'AM Dai'],
         'cites': '206',
         'eprint': 'https://arxiv.org/pdf/1801.07736.pdf?source=post_page---------------------------',
         'gsrank': '1',
         'title': 'MaskGAN: Better text generation via filling in the_',
         'url': 'https://arxiv.org/abs/1801.07736',
         'venue': 'arXiv preprint arXiv:1801.07736',
         'year': '2018'},
 'citations_link': '/scholar?cites=8054442901795858629&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMaskGAN:%2BBetter%2BText%2BGeneration%2Bvia%2BFilling%2Bin%2Bthe%2B_______%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=xWg1GSUhx28J&ei=_sgqX_GCDaiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:xWg1GSUhx28J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrLZSizA4OypYqglBNo0VHX-wl7YaZS&scisig=AAGBfm0AAAAAXyrLZZFeN11TBy5h8nb0k8wFdVmWE99L&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
206
-------------------------------------------------
2020-08-05 14:58:21
Got the results of the query
{'bib': {'abstract': 'Determining an effective architecture for multi-layer '
                     'feedforward backpropagation neural networks can be a '
                     'time-consuming effort. In general it requires human '
                     'intervention in determining the number of layers, number '
                     'of hidden cells, the learning rule and the learning',
         'author': ['SG Romaniuk'],
         'cites': '12',
         'eprint': 'https://dl.comp.nus.edu.sg/bitstream/handle/1900.100/1320/report.ps?sequence=2&isAllowed=y',
         'gsrank': '1',
         'title': 'Pruning divide & conquer networks',
         'url': 'https://www.tandfonline.com/doi/abs/10.1088/0954-898X_4_4_005',
         'venue': 'Network: Computation in Neural Systems',
         'year': '1993'},
 'citations_link': '/scholar?cites=18166112198190432330&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDivide%2Band%2BConquer%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=SgAIy8v-GvwJ&ei=E8kqX4XPE7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:SgAIy8v-GvwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrLb2dhuA0G1LNHgd3NzCO6N5aYy408&scisig=AAGBfm0AAAAAXyrLb-DNBPjZVzqfhbwIIa8CWstfknj4&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 14:58:31
Got the results of the query
{'bib': {'abstract': 'Learning to learn is a powerful paradigm for enabling '
                     'models to learn from data more effectively and '
                     'efficiently. A popular approach to meta-learning is to '
                     'train a recurrent model to read in a training dataset as '
                     'input and output the parameters of a learned model, or '
                     'output predictions for new test inputs. Alternatively, a '
                     'more recent approach to meta-learning aims to acquire '
                     'deep representations that can be effectively fine-tuned, '
                     'via standard gradient descent, to new tasks. In this '
                     'paper, we consider the meta-learning problem from the',
         'author': ['C Finn', 'S Levine'],
         'cites': '97',
         'eprint': 'https://arxiv.org/pdf/1710.11622',
         'gsrank': '1',
         'title': 'Meta-learning and universality: Deep representations and '
                  'gradient descent can approximate any learning algorithm',
         'url': 'https://arxiv.org/abs/1710.11622',
         'venue': 'arXiv preprint arXiv:1710.11622',
         'year': '2017'},
 'citations_link': '/scholar?cites=2185954399232722694&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-Learning%2Band%2BUniversality:%2BDeep%2BRepresentations%2Band%2BGradient%2BDescent%2Bcan%2BApproximate%2Bany%2BLearning%2BAlgorithm%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Bgd4xfERVh4J&ei=GskqX8WuLojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Bgd4xfERVh4J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrLeAk5Q693h0PRNaIrsxcothuxVnwh&scisig=AAGBfm0AAAAAXyrLeFmfK0R4iJDdAG-Xk4lda2oVXD6T&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
97
-------------------------------------------------
2020-08-05 14:58:40
Got the results of the query
{'bib': {'abstract': 'We introduce a new algorithm for reinforcement learning '
                     'called Maximum aposteriori Policy Optimisation (MPO) '
                     'based on coordinate ascent on a relative entropy '
                     'objective. We show that several existing methods can '
                     'directly be related to our derivation. We develop two '
                     'off-policy algorithms and demonstrate that they are '
                     'competitive with the state-of-the-art in deep '
                     'reinforcement learning. In particular, for continuous '
                     'control, our method outperforms existing methods with '
                     'respect to sample efficiency, premature convergence and '
                     'robustness to',
         'author': ['A Abdolmaleki', 'JT Springenberg', 'Y Tassa'],
         'cites': '121',
         'eprint': 'https://arxiv.org/pdf/1806.06920',
         'gsrank': '1',
         'title': 'Maximum a posteriori policy optimisation',
         'url': 'https://arxiv.org/abs/1806.06920',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=14521646117118037069&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMaximum%2Ba%2BPosteriori%2BPolicy%2BOptimisation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=TUzAXDRAh8kJ&ei=JMkqX9ztL8KwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:TUzAXDRAh8kJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrLhNFZS3imoml3N9KZLvVEVP_M8eHG&scisig=AAGBfm0AAAAAXyrLhLnLY0gHba46eM_QTdRnaBOSv4vj&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
121
-------------------------------------------------
2020-08-05 14:58:52
Got the results of the query
{'bib': {'abstract': 'We develop a metalearning approach for learning '
                     'hierarchically structured policies, improving sample '
                     'efficiency on unseen tasks through the use of shared '
                     'primitives---policies that are executed for large '
                     'numbers of timesteps. Specifically, a set of primitives '
                     'are shared',
         'author': ['K Frans', 'J Ho', 'X Chen', 'P Abbeel'],
         'cites': '155',
         'eprint': 'https://arxiv.org/pdf/1710.09767.pdf%20http://arxiv.org/abs/1710.09767',
         'gsrank': '1',
         'title': 'Meta learning shared hierarchies',
         'url': 'https://arxiv.org/abs/1710.09767',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=8366113293045727240&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta%2BLearning%2BShared%2BHierarchies%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=CACq5rZnGnQJ&ei=MskqX7_DIrGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:CACq5rZnGnQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrLjp0DMLmZoXDa7LsW9Uqq4omAVaLZ&scisig=AAGBfm0AAAAAXyrLjkP_Eis5krEu5hbQaQE50zsfcXTU&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
155
-------------------------------------------------
2020-08-05 14:59:02
Got the results of the query
{'bib': {'abstract': 'It has long been known that a single-layer '
                     'fully-connected neural network with an iid prior over '
                     'its parameters is equivalent to a Gaussian process (GP), '
                     'in the limit of infinite network width. This '
                     'correspondence enables exact Bayesian inference for '
                     'infinite width neural networks on regression tasks by '
                     'means of evaluating the corresponding GP. Recently, '
                     'kernel functions which mimic multi-layer random neural '
                     'networks have been developed, but only outside of a '
                     'Bayesian framework. As such, previous work has not '
                     'identified that these',
         'author': ['J Lee', 'Y Bahri', 'R Novak', 'SS Schoenholz'],
         'cites': '277',
         'eprint': 'https://arxiv.org/pdf/1711.00165',
         'gsrank': '1',
         'title': 'Deep neural networks as gaussian processes',
         'url': 'https://arxiv.org/abs/1711.00165',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=6709509064500094656&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BNeural%2BNetworks%2Bas%2BGaussian%2BProcesses%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=wKbKFvL2HF0J&ei=QskqX-K-B8KwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:wKbKFvL2HF0J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrLnf7gtyo1RspwGdFzM4Cex1nVa0D3&scisig=AAGBfm0AAAAAXyrLndnIAhwBLVPyb9oiBu8-at4ISTua&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
277
-------------------------------------------------
2020-08-05 14:59:17
Got the results of the query
{'bib': {'abstract': 'Deep generative models have been enjoying success in '
                     'modeling continuous data. However it remains challenging '
                     'to capture the representations for discrete structures '
                     'with formal grammars and semantics, eg, computer '
                     'programs and molecular structures. How to generate both '
                     'syntactically and semantically correct data still '
                     'remains largely an open problem. Inspired by the theory '
                     'of compiler where the syntax and semantics check is done '
                     'via syntax-directed translation (SDT), we propose a '
                     'novel syntax-directed variational',
         'author': ['H Dai', 'Y Tian', 'B Dai', 'S Skiena', 'L Song'],
         'cites': '106',
         'eprint': 'https://arxiv.org/pdf/1802.08786',
         'gsrank': '1',
         'title': 'Syntax-directed variational autoencoder for structured data',
         'url': 'https://arxiv.org/abs/1802.08786',
         'venue': 'arXiv preprint arXiv:1802.08786',
         'year': '2018'},
 'citations_link': '/scholar?cites=7991796845235005593&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSyntax-Directed%2BVariational%2BAutoencoder%2Bfor%2BStructured%2BData%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=mehRbeOQ6G4J&ei=TMkqX7PdKo-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:mehRbeOQ6G4J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrLqufPxJcYKxI380aEafrHb0x1HfuK&scisig=AAGBfm0AAAAAXyrLqsanmazzHqmmcMGYndhhogb2NMBe&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
106
-------------------------------------------------
2020-08-05 14:59:31
Got the results of the query
{'bib': {'abstract': 'Synthesizing user-intended programs from a small number '
                     'of input-output examples is a challenging problem with '
                     'several important applications like spreadsheet '
                     'manipulation, data wrangling and code refactoring. '
                     'Existing synthesis systems either completely rely on '
                     'deductive logic techniques that are extensively '
                     'hand-engineered or on purely statistical models that '
                     'need massive amounts of data, and in general fail to '
                     'provide real-time synthesis on challenging benchmarks. '
                     'In this work, we propose Neural Guided Deductive Search',
         'author': ['A Kalyan', 'A Mohta', 'O Polozov', 'D Batra', 'P Jain'],
         'cites': '50',
         'eprint': 'https://arxiv.org/pdf/1804.01186',
         'gsrank': '1',
         'title': 'Neural-guided deductive search for real-time program '
                  'synthesis from examples',
         'url': 'https://arxiv.org/abs/1804.01186',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=6666411294628297468&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural-Guided%2BDeductive%2BSearch%2Bfor%2BReal-Time%2BProgram%2BSynthesis%2Bfrom%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=_Lr4lsDZg1wJ&ei=WckqX9LpF4yimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:_Lr4lsDZg1wJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrLtMYDL4omxajwqaw1NYRzivgXQgkd&scisig=AAGBfm0AAAAAXyrLtLLFmriqBJG2kpI3vbU99rOZTTcI&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
50
-------------------------------------------------
2020-08-05 14:59:41
Got the results of the query
{'bib': {'abstract': 'A popular recent approach to answering open-domain '
                     'questions is to first search for question-related '
                     'passages and then apply reading comprehension models to '
                     'extract answers. Existing methods usually extract '
                     'answers from single passages independently. But some '
                     'questions require a combination of evidence from across '
                     'different sources to answer correctly. In this paper, we '
                     'propose two models which make use of multiple passages '
                     'to generate their answers. Both use an answer-reranking '
                     'approach which reorders the answer',
         'author': ['S Wang', 'M Yu', 'J Jiang', 'W Zhang', 'X Guo'],
         'cites': '87',
         'eprint': 'https://arxiv.org/pdf/1711.05116',
         'gsrank': '1',
         'title': 'Evidence aggregation for answer re-ranking in open-domain '
                  'question answering',
         'url': 'https://arxiv.org/abs/1711.05116',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=5917321946590508860&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEvidence%2BAggregation%2Bfor%2BAnswer%2BRe-Ranking%2Bin%2BOpen-Domain%2BQuestion%2BAnswering%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=PHcmiPWMHlIJ&ei=YskqX7GgJ6OGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:PHcmiPWMHlIJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrLwKoFw38m-43rB0c8bIvKRxIi3fol&scisig=AAGBfm0AAAAAXyrLwMka0jQh3vlkJXKPFVhubMSNrS8Q&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
87
-------------------------------------------------
2020-08-05 14:59:52
Got the results of the query
{'bib': {'abstract': 'For computer vision applications, prior works have shown '
                     'the efficacy of reducing numeric precision of model '
                     'parameters (network weights) in deep neural networks. '
                     'Activation maps, however, occupy a large memory '
                     'footprint during both the training and inference step '
                     'when',
         'author': ['A Mishra', 'E Nurvitadhi', 'JJ Cook', 'D Marr'],
         'cites': '143',
         'eprint': 'https://arxiv.org/pdf/1709.01134',
         'gsrank': '1',
         'title': 'WRPN: wide reduced-precision networks',
         'url': 'https://arxiv.org/abs/1709.01134',
         'venue': 'arXiv preprint arXiv:1709.01134',
         'year': '2017'},
 'citations_link': '/scholar?cites=7570145345985295337&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DWRPN:%2BWide%2BReduced-Precision%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=6bOHdRWPDmkJ&ei=b8kqX6n1F4yimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:6bOHdRWPDmkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrL1bl2PROw_IkysgbEwhuwJEmdklQL&scisig=AAGBfm0AAAAAXyrL1UgjK8kGymccw0FhITVE6Jvgffdu&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
143
-------------------------------------------------
2020-08-05 15:00:14
Got the results of the query
{'bib': {'abstract': 'We propose in this paper a new approach to train the '
                     'Generative Adversarial Nets (GANs) with a mixture of '
                     'generators to overcome the mode collapsing problem. The '
                     'main intuition is to employ multiple generators, instead '
                     'of using a single one as in the original GAN. The idea '
                     'is simple, yet proven to be extremely effective at '
                     'covering diverse data modes, easily overcoming the mode '
                     'collapsing problem and delivering state-of-the-art '
                     'results. A minimax formulation was able to establish '
                     'among a classifier, a discriminator, and a set of '
                     'generators',
         'author': ['Q Hoang', 'TD Nguyen', 'T Le', 'D Phung'],
         'cites': '67',
         'eprint': 'https://openreview.net/pdf?id=rkmu5b0a-',
         'gsrank': '1',
         'title': 'MGAN: Training generative adversarial nets with multiple '
                  'generators',
         'url': 'https://openreview.net/forum?id=rkmu5b0a-',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15083973924521420990&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMGAN:%2BTraining%2BGenerative%2BAdversarial%2BNets%2Bwith%2BMultiple%2BGenerators%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=vqhatF0KVdEJ&ei=gskqX_L5PKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:vqhatF0KVdEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrL5Acm75QudsRRhjlCp0F1J2Y-P5F1&scisig=AAGBfm0AAAAAXyrL5CPCt6X_56e4mFzWqSHox7weHVc-&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
67
-------------------------------------------------
2020-08-05 15:00:28
Got the results of the query
{'bib': {'abstract': 'In this work we present a new agent architecture, called '
                     'Reactor, which combines multiple algorithmic and '
                     'architectural contributions to produce an agent with '
                     'higher sample-efficiency than Prioritized Dueling DQN '
                     '(Wang et al., 2016) and Categorical DQN (Bellemare et '
                     'al., 2017), while giving better run-time performance '
                     'than A3C (Mnih et al., 2016). Our first contribution is '
                     'a new policy evaluation algorithm called Distributional '
                     'Retrace, which brings multi-step off-policy updates to '
                     'the distributional reinforcement learning setting. The '
                     'same',
         'author': ['A Gruslys', 'W Dabney', 'MG Azar', 'B Piot'],
         'cites': '30',
         'eprint': 'https://arxiv.org/pdf/1704.04651',
         'gsrank': '1',
         'title': 'The reactor: A fast and sample-efficient actor-critic agent '
                  'for reinforcement learning',
         'url': 'https://arxiv.org/abs/1704.04651',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=8283235639510258055&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BReactor:%2BA%2Bfast%2Band%2Bsample-efficient%2BActor-Critic%2Bagent%2Bfor%2BReinforcement%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=h1Hde-3283IJ&ei=kskqX_LiMIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:h1Hde-3283IJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrL9q2s4-ccLY85DCtkcMB3rizuRnFM&scisig=AAGBfm0AAAAAXyrL9up9SDYcMG9gtLkM-lGetNYeVxpM&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
30
-------------------------------------------------
2020-08-05 15:00:46
Got the results of the query
{'bib': {'abstract': 'We propose SEARNN, a novel training algorithm for '
                     'recurrent neural networks (RNNs) inspired by the" '
                     'learning to search"(L2S) approach to structured '
                     'prediction. RNNs have been widely successful in '
                     'structured prediction applications such as machine '
                     'translation or',
         'author': ['R Leblond', 'JB Alayrac', 'A Osokin'],
         'cites': '25',
         'eprint': 'https://arxiv.org/pdf/1706.04499',
         'gsrank': '1',
         'title': 'SEARNN: Training RNNs with global-local losses',
         'url': 'https://arxiv.org/abs/1706.04499',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=10552754146488713829&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSEARNN:%2BTraining%2BRNNs%2Bwith%2Bglobal-local%2Blosses%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ZYJ9P_zpcpIJ&ei=oskqX4WrE4vrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ZYJ9P_zpcpIJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrMAu1IBBnaneQ37OhYemyI23Wa2VEw&scisig=AAGBfm0AAAAAXyrMApoXXrpkqnc61164SzmePxOsksxS&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
25
-------------------------------------------------
2020-08-05 15:00:58
Got the results of the query
{'bib': {'abstract': 'This work adopts the very successful distributional '
                     'perspective on reinforcement learning and adapts it to '
                     'the continuous control setting. We combine this within a '
                     'distributed framework for off-policy learning in order '
                     'to develop what we call the Distributed Distributional '
                     'Deep Deterministic Policy Gradient algorithm, D4PG. We '
                     'also combine this technique with a number of additional, '
                     'simple improvements such as the use of $ N $-step '
                     'returns and prioritized experience replay. '
                     'Experimentally we examine the contribution of',
         'author': ['G Barth-Maron', 'MW Hoffman', 'D Budden'],
         'cites': '164',
         'eprint': 'https://arxiv.org/pdf/1804.08617',
         'gsrank': '1',
         'title': 'Distributed distributional deterministic policy gradients',
         'url': 'https://arxiv.org/abs/1804.08617',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=13940862836810359018&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDistributed%2BDistributional%2BDeterministic%2BPolicy%2BGradients%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=6qCGYuPkd8EJ&ei=2skqX8H9EovrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:6qCGYuPkd8EJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrMVYMf0EjJHx0ZSq1PJ2JofW6biFgd&scisig=AAGBfm0AAAAAXyrMVVXQ_bTAFNnHJlehRZu2hn1iZrQO&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
164
-------------------------------------------------
2020-08-05 15:02:21
Got the results of the query
{'bib': {'abstract': 'Hierarchical reinforcement learning methods offer a '
                     'powerful means of planning flexible behavior in '
                     'complicated domains. However, learning an appropriate '
                     'hierarchical decomposition of a domain into subtasks '
                     'remains a substantial challenge. We present a novel '
                     'algorithm for subtask discovery, based on the recently '
                     'introduced multitask linearly-solvable Markov decision '
                     'process (MLMDP) framework. The MLMDP can perform '
                     'never-before-seen tasks by representing them as a linear '
                     'combination of a previously learned',
         'author': ['AC Earle', 'AM Saxe', 'B Rosman'],
         'cites': '3',
         'eprint': 'https://arxiv.org/pdf/1708.00463',
         'gsrank': '1',
         'title': 'Hierarchical subtask discovery with non-negative matrix '
                  'factorization',
         'url': 'https://arxiv.org/abs/1708.00463',
         'venue': 'arXiv preprint arXiv:1708.00463',
         'year': '2017'},
 'citations_link': '/scholar?cites=7464922712154923612&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BSubtask%2BDiscovery%2Bwith%2BNon-Negative%2BMatrix%2BFactorization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=XGK95ae7mGcJ&ei=GMoqX-DeKojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:XGK95ae7mGcJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrMjwNjKbecYjA0zbBd8N-p_qoOuDqv&scisig=AAGBfm0AAAAAXyrMj_WuPaPyV53M5wHEuy8UOeEvBSbD&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
3
-------------------------------------------------
2020-08-05 15:03:20
Got the results of the query
{'bib': {'abstract': 'Neural programs are highly accurate and structured '
                     'policies that perform algorithmic tasks by controlling '
                     'the behavior of a computation mechanism. Despite the '
                     'potential to increase the interpretability and the '
                     'compositionality of the behavior of artificial agents, '
                     'it remains difficult to learn from demonstrations neural '
                     'networks that represent computer programs. The main '
                     'challenges that set algorithmic domains apart from other '
                     'imitation learning domains are the need for high '
                     'accuracy, the involvement of specific structures of '
                     'data, and the',
         'author': ['R Fox', 'R Shin', 'S Krishnan', 'K Goldberg', 'D Song'],
         'cites': '12',
         'eprint': 'https://par.nsf.gov/servlets/purl/10063833',
         'gsrank': '1',
         'title': 'Parametrized hierarchical procedures for neural programming',
         'url': 'https://par.nsf.gov/biblio/10063833',
         'venue': 'ICLR 2018',
         'year': '2018'},
 'citations_link': '/scholar?cites=11070704689577142553&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DParametrized%2BHierarchical%2BProcedures%2Bfor%2BNeural%2BProgramming%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=GSl_k0ILo5kJ&ei=RMoqX9KHD4vrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:GSl_k0ILo5kJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrMtVwdgsptK6TPlSd9D2hAU6HwKGzH&scisig=AAGBfm0AAAAAXyrMtS0nDolfTivLGgAEOratxaRA_ai1&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 15:03:58
Got the results of the query
{'bib': {'abstract': 'Weight pruning has proven to be an effective method in '
                     'reducing the model size and computation cost while not '
                     'sacrificing the model accuracy. Conventional sparse '
                     'matrix formats, however, involve irregular index '
                     'structures with large storage requirement and sequential '
                     'reconstruction process, resulting in inefficient use of '
                     'highly parallel computing resources. Hence, pruning is '
                     'usually restricted to inference with a batch size of '
                     'one, for which an efficient parallel matrix-vector '
                     'multiplication method exists. In this paper, a new',
         'author': ['D Lee', 'D Ahn', 'T Kim', 'PI Chuang'],
         'cites': '12',
         'eprint': 'https://openreview.net/pdf?id=S1D8MPxA-',
         'gsrank': '1',
         'title': 'Viterbi-based pruning for sparse matrix with fixed and high '
                  'index compression ratio',
         'url': 'https://openreview.net/forum?id=S1D8MPxA-&noteId=S1D8MPxA-',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8920021848200630175&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DViterbi-based%2BPruning%2Bfor%2BSparse%2BMatrix%2Bwith%2BFixed%2Band%2BHigh%2BIndex%2BCompression%2BRatio%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=n8-4UpxIynsJ&ei=bcoqX9KVAaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:n8-4UpxIynsJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrM0umoOzKhlGUSh2WJWu7yjiZXCGy0&scisig=AAGBfm0AAAAAXyrM0sslkkFL1NWbg_napz3wWILwLm5O&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 15:04:27
Got the results of the query
{'bib': {'abstract': 'We propose a novel, projection based way to incorporate '
                     'the conditional information into the discriminator of '
                     'GANs that respects the role of the conditional '
                     'information in the underlining probabilistic model. This '
                     'approach is in contrast with most frameworks of '
                     'conditional GANs',
         'author': ['T Miyato', 'M Koyama'],
         'cites': '154',
         'eprint': 'https://arxiv.org/pdf/1802.05637',
         'gsrank': '1',
         'title': 'cGANs with projection discriminator',
         'url': 'https://arxiv.org/abs/1802.05637',
         'venue': 'arXiv preprint arXiv:1802.05637',
         'year': '2018'},
 'citations_link': '/scholar?cites=2383594201116967790&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DcGANs%2Bwith%2BProjection%2BDiscriminator%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=bgNCDkw6FCEJ&ei=icoqX7-JCLGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:bgNCDkw6FCEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrM5S8UiLlQ_kAY_-oYK04a-7emCrxG&scisig=AAGBfm0AAAAAXyrM5YVvb1M7NFlP4CfRdZr0LfF_kuo7&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
154
-------------------------------------------------
2020-08-05 15:04:45
Got the results of the query
{'bib': {'abstract': 'Over the last years, deep convolutional neural networks '
                     '(ConvNets) have transformed the field of computer vision '
                     'thanks to their unparalleled capacity to learn high '
                     'level semantic image features. However, in order to '
                     'successfully learn those features, they usually require '
                     'massive amounts of manually labeled data, which is both '
                     'expensive and impractical to scale. Therefore, '
                     'unsupervised semantic feature learning, ie, learning '
                     'without requiring manual annotation effort, is of '
                     'crucial importance in order to successfully harvest the '
                     'vast',
         'author': ['S Gidaris', 'P Singh', 'N Komodakis'],
         'cites': '393',
         'eprint': 'https://arxiv.org/pdf/1803.07728',
         'gsrank': '1',
         'title': 'Unsupervised representation learning by predicting image '
                  'rotations',
         'url': 'https://arxiv.org/abs/1803.07728',
         'venue': 'arXiv preprint arXiv:1803.07728',
         'year': '2018'},
 'citations_link': '/scholar?cites=12748509220929577948&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BRepresentation%2BLearning%2Bby%2BPredicting%2BImage%2BRotations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=3KNMiZfN67AJ&ei=kcoqX8vHCZqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:3KNMiZfN67AJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrM73wKzigIbeNPuByP--jZLwkEnOow&scisig=AAGBfm0AAAAAXyrM7z3z4qxynsyK8KarbrPnuPrmWYak&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
393
-------------------------------------------------
2020-08-05 15:04:55
Got the results of the query
{'bib': {'abstract': 'Inspired by previous work on emergent communication in '
                     'referential games, we propose a novel multi-modal, '
                     'multi-step referential game, where the sender and '
                     'receiver have access to distinct modalities of an '
                     'object, and their information exchange is bidirectional '
                     'and of arbitrary duration. The multi-modal multi-step '
                     'setting allows agents to develop an internal '
                     'communication significantly closer to natural language, '
                     'in that they share a single set of messages, and that '
                     'the length of the conversation may vary according to the '
                     'difficulty of the',
         'author': ['K Evtimova', 'A Drozdov', 'D Kiela', 'K Cho'],
         'cites': '31',
         'eprint': 'https://arxiv.org/pdf/1705.10369',
         'gsrank': '1',
         'title': 'Emergent communication in a multi-modal, multi-step '
                  'referential game',
         'url': 'https://arxiv.org/abs/1705.10369',
         'venue': 'arXiv preprint arXiv:1705.10369',
         'year': '2017'},
 'citations_link': '/scholar?cites=6581857213563474520&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergent%2BCommunication%2Bin%2Ba%2BMulti-Modal,%2BMulti-Step%2BReferential%2BGame%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=WBLLeEN0V1sJ&ei=m8oqX_TiFI-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:WBLLeEN0V1sJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrM-bFDOch396pYmYDIFU4AAVWHbWJS&scisig=AAGBfm0AAAAAXyrM-a4lDfF3ZH-nBPvYMvPCkzvU00YD&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
31
-------------------------------------------------
2020-08-05 15:05:05
Got the results of the query
{'bib': {'abstract': 'The graph convolutional networks (GCN) recently proposed '
                     'by Kipf and Welling are an effective graph model for '
                     'semi-supervised learning. This model, however, was '
                     'originally designed to be learned with the presence of '
                     'both training and test data. Moreover, the recursive '
                     'neighborhood expansion across layers poses time and '
                     'memory challenges for training with large, dense graphs. '
                     'To relax the requirement of simultaneous availability of '
                     'test data, we interpret graph convolutions as integral '
                     'transforms of embedding functions',
         'author': ['J Chen', 'T Ma', 'C Xiao'],
         'cites': '271',
         'eprint': 'https://arxiv.org/pdf/1801.10247',
         'gsrank': '1',
         'title': 'Fastgcn: fast learning with graph convolutional networks '
                  'via importance sampling',
         'url': 'https://arxiv.org/abs/1801.10247',
         'venue': 'arXiv preprint arXiv:1801.10247',
         'year': '2018'},
 'citations_link': '/scholar?cites=18054036108684442257&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFastGCN:%2BFast%2BLearning%2Bwith%2BGraph%2BConvolutional%2BNetworks%2Bvia%2BImportance%2BSampling%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=kTo2vC_SjPoJ&ei=p8oqX_DKPLGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:kTo2vC_SjPoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNA1lrOs_UWQ1cwF-mN2zrJ7Sci2rc&scisig=AAGBfm0AAAAAXyrNA36V2t3dnDCbmI8ISgcwx_7v06s0&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
271
-------------------------------------------------
2020-08-05 15:05:15
Got the results of the query
{'bib': {'abstract': 'While most machine translation systems to date are '
                     'trained on large parallel corpora, humans learn language '
                     'in a different way: by being grounded in an environment '
                     'and interacting with other humans. In this work, we '
                     'propose a communication game where two agents, native '
                     'speakers of their own respective languages, jointly '
                     'learn to solve a visual referential task. We find that '
                     'the ability to understand and translate a foreign '
                     'language emerges as a means to achieve shared goals. The '
                     'emergent translation is interactive and',
         'author': ['J Lee', 'K Cho', 'J Weston', 'D Kiela'],
         'cites': '29',
         'eprint': 'https://arxiv.org/pdf/1710.06922',
         'gsrank': '1',
         'title': 'Emergent translation in multi-agent communication',
         'url': 'https://arxiv.org/abs/1710.06922',
         'venue': 'arXiv preprint arXiv:1710.06922',
         'year': '2017'},
 'citations_link': '/scholar?cites=16875774594076963034&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergent%2BTranslation%2Bin%2BMulti-Agent%2BCommunication%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=2vCE5ZTLMuoJ&ei=ssoqX4XjG7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:2vCE5ZTLMuoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNEF3A5z_qElrm67gcet60jT86pkLu&scisig=AAGBfm0AAAAAXyrNECRcg5AGuqSwIlHNdpJJJmMDApcQ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
29
-------------------------------------------------
2020-08-05 15:05:28
Got the results of the query
{'bib': {'abstract': 'In this work we propose a simple and efficient framework '
                     'for learning sentence representations from unlabelled '
                     'data. Drawing inspiration from the distributional '
                     'hypothesis and recent work on learning sentence '
                     'representations, we reformulate the problem of '
                     'predicting the context in which a sentence appears as a '
                     'classification problem. Given a sentence and its '
                     'context, a classifier distinguishes context sentences '
                     'from other contrastive sentences based on their vector '
                     'representations. This allows us to efficiently learn '
                     'different',
         'author': ['L Logeswaran', 'H Lee'],
         'cites': '158',
         'eprint': 'https://arxiv.org/pdf/1803.02893',
         'gsrank': '1',
         'title': 'An efficient framework for learning sentence '
                  'representations',
         'url': 'https://arxiv.org/abs/1803.02893',
         'venue': 'arXiv preprint arXiv:1803.02893',
         'year': '2018'},
 'citations_link': '/scholar?cites=12944665931993057404&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAn%2Befficient%2Bframework%2Bfor%2Blearning%2Bsentence%2Brepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=fAiM1BSxpLMJ&ei=v8oqX8zQO6OGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:fAiM1BSxpLMJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNG6TfWkEeHHGgFmGfOyMQvjJ30SlY&scisig=AAGBfm0AAAAAXyrNG7TRhY-nymZOqrDT8u4FkZKuOihb&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
158
-------------------------------------------------
2020-08-05 15:05:39
Got the results of the query
{'bib': {'abstract': 'We address the problem of learning structured policies '
                     'for continuous control. In traditional reinforcement '
                     'learning, policies of agents are learned by MLPs which '
                     'take the concatenation of all observations from the '
                     'environment as input for predicting actions. In this',
         'author': ['T Wang', 'R Liao', 'J Ba', 'S Fidler'],
         'cites': '60',
         'eprint': 'https://openreview.net/pdf?id=S1sqHMZCb',
         'gsrank': '1',
         'title': 'Nervenet: Learning structured policy with graph neural '
                  'networks',
         'url': 'https://openreview.net/forum?id=S1sqHMZCb',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15124575448090085151&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNerveNet:%2BLearning%2BStructured%2BPolicy%2Bwith%2BGraph%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=HwcBkjxJ5dEJ&ei=ycoqX4DIB6OGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:HwcBkjxJ5dEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNJu6bqUX_eL-s5-P8i9oqzJ3oAvPx&scisig=AAGBfm0AAAAAXyrNJnn7y2l4i2vAOXO9n2iUO8kT_iT9&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
60
-------------------------------------------------
2020-08-05 15:05:50
Got the results of the query
{'bib': {'abstract': 'In this paper, we propose a novel unsupervised '
                     'clustering approach exploiting the hidden information '
                     'that is indirectly introduced through a pseudo '
                     'classification objective. Specifically, we randomly '
                     'assign a pseudo parent-class label to each observation '
                     'which is then modified by applying the domain specific '
                     'transformation associated with the assigned label. '
                     'Generated pseudo observation-label pairs are '
                     'subsequently used to train a neural network with '
                     'Auto-clustering Output Layer (ACOL) that introduces '
                     'multiple softmax nodes for',
         'author': ['O Kilinc', 'I Uysal'],
         'cites': '18',
         'eprint': 'https://arxiv.org/pdf/1802.03063',
         'gsrank': '1',
         'title': 'Learning latent representations in neural networks for '
                  'clustering through pseudo supervision and graph-based '
                  'activity regularization',
         'url': 'https://arxiv.org/abs/1802.03063',
         'venue': 'arXiv preprint arXiv:1802.03063',
         'year': '2018'},
 'citations_link': '/scholar?cites=11376218691210990570&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BLatent%2BRepresentations%2Bin%2BNeural%2BNetworks%2Bfor%2BClustering%2Bthrough%2BPseudo%2BSupervision%2Band%2BGraph-based%2BActivity%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=6lPKOKBy4J0J&ei=0coqX-3yNrGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:6lPKOKBy4J0J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNMDDZQGkYihNLpiUPgxVT7XLQGj13&scisig=AAGBfm0AAAAAXyrNMEUfaG6I_aWLPJm90Bjy5g0aUosj&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
18
-------------------------------------------------
2020-08-05 15:06:00
Got the results of the query
{'bib': {'abstract': 'We present a method for transferring neural '
                     'representations from label-rich source domains to '
                     'unlabeled target domains. Recent adversarial methods '
                     'proposed for this task learn to align features across '
                     'domains by fooling a special domain critic network. '
                     'However, a',
         'author': ['K Saito', 'Y Ushiku', 'T Harada', 'K Saenko'],
         'cites': '66',
         'eprint': 'https://arxiv.org/pdf/1711.01575',
         'gsrank': '1',
         'title': 'Adversarial dropout regularization',
         'url': 'https://arxiv.org/abs/1711.01575',
         'venue': 'arXiv preprint arXiv:1711.01575',
         'year': '2017'},
 'citations_link': '/scholar?cites=6341114078934760238&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAdversarial%2BDropout%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Lnsie6gpAFgJ&ei=3coqX-nHE4yimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Lnsie6gpAFgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNOo3r2Vsp7gBNffnvgAM0TGvtTSDV&scisig=AAGBfm0AAAAAXyrNOuUL3tuHW_5HIvijozu2WRrrB5XH&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
66
-------------------------------------------------
2020-08-05 15:06:10
Got the results of the query
{'bib': {'abstract': 'We investigate the training and performance of '
                     'generative adversarial networks using the Maximum Mean '
                     'Discrepancy (MMD) as critic, termed MMD GANs. As our '
                     'main theoretical contribution, we clarify the situation '
                     'with bias in GAN loss functions raised by recent work',
         'author': ['M Bińkowski', 'DJ Sutherland', 'M Arbel'],
         'cites': '188',
         'eprint': 'https://arxiv.org/pdf/1801.01401.pdf)',
         'gsrank': '1',
         'title': 'Demystifying mmd gans',
         'url': 'https://arxiv.org/abs/1801.01401',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=10236052458128513824&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDemystifying%2BMMD%2BGANs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=IJsDTXrDDY4J&ei=5soqX-POEqOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:IJsDTXrDDY4J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNRNgcCAp5J_o2_2equOFrGORzfjpn&scisig=AAGBfm0AAAAAXyrNRIF1_lNDSPWjHMnE1u4M4YUm8fms&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
188
-------------------------------------------------
2020-08-05 15:06:21
Got the results of the query
{'bib': {'abstract': 'The top-k error is a common measure of performance in '
                     'machine learning and computer vision. In practice, top-k '
                     'classification is typically performed with deep neural '
                     'networks trained with the cross-entropy loss. '
                     'Theoretical results indeed suggest that cross-entropy is '
                     'an optimal learning objective for such a task in the '
                     'limit of infinite data. In the context of limited and '
                     'noisy data however, the use of a loss function that is '
                     'specifically designed for top-k classification can bring '
                     'significant improvements. Our empirical evidence '
                     'suggests that the',
         'author': ['L Berrada', 'A Zisserman', 'MP Kumar'],
         'cites': '33',
         'eprint': 'https://arxiv.org/pdf/1802.07595',
         'gsrank': '1',
         'title': 'Smooth loss functions for deep top-k classification',
         'url': 'https://arxiv.org/abs/1802.07595',
         'venue': 'arXiv preprint arXiv:1802.07595',
         'year': '2018'},
 'citations_link': '/scholar?cites=2261810241418874442&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSmooth%2BLoss%2BFunctions%2Bfor%2BDeep%2BTop-k%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Ss4cVW6QYx8J&ei=9MoqX8nKAo-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Ss4cVW6QYx8J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNUv_TWR4wJikvRNnwxItKNf2RVa-p&scisig=AAGBfm0AAAAAXyrNUu_EMmU8wPnEmwOqNK21Vf8c5CDK&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
33
-------------------------------------------------
2020-08-05 15:06:34
Got the results of the query
{'bib': {'abstract': 'As neural networks grow deeper and wider, learning '
                     'networks with hard-threshold activations is becoming '
                     'increasingly important, both for network quantization, '
                     'which can drastically reduce time and energy '
                     'requirements, and for creating large integrated systems '
                     'of deep networks, which may have non-differentiable '
                     'components and must avoid vanishing and exploding '
                     'gradients for effective learning. However, since '
                     'gradient descent is not applicable to hard-threshold '
                     'functions, it is not clear how to learn networks of them '
                     'in a',
         'author': ['AL Friesen', 'P Domingos'],
         'cites': '13',
         'eprint': 'https://arxiv.org/pdf/1710.11573',
         'gsrank': '1',
         'title': 'Deep learning as a mixed convex-combinatorial optimization '
                  'problem',
         'url': 'https://arxiv.org/abs/1710.11573',
         'venue': 'arXiv preprint arXiv:1710.11573',
         'year': '2017'},
 'citations_link': '/scholar?cites=14079107033151501838&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLearning%2Bas%2Ba%2BMixed%2BConvex-Combinatorial%2BOptimization%2BProblem%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=DvaymUAJY8MJ&ei=AcsqX_KPFaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:DvaymUAJY8MJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNXyfwcm8eyl6H3GD9Ubua2jlbS3NG&scisig=AAGBfm0AAAAAXyrNX4xJwUPjdtbBwp9kSHHW0TbrReya&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
13
-------------------------------------------------
2020-08-05 15:06:47
Got the results of the query
{'bib': {'abstract': 'Structured prediction energy networks (SPENs; Belanger & '
                     'McCallum 2016) use neural network architectures to '
                     'define energy functions that can capture arbitrary '
                     'dependencies among parts of structured outputs. Prior '
                     'work used gradient descent for inference, relaxing the '
                     'structured output to a set of continuous variables and '
                     'then optimizing the energy with respect to them. We '
                     'replace this use of gradient descent with a neural '
                     'network trained to approximate structured argmax '
                     'inference. This" inference network" outputs continuous',
         'author': ['L Tu', 'K Gimpel'],
         'cites': '27',
         'eprint': 'https://arxiv.org/pdf/1803.03376',
         'gsrank': '1',
         'title': 'Learning approximate inference networks for structured '
                  'prediction',
         'url': 'https://arxiv.org/abs/1803.03376',
         'venue': 'arXiv preprint arXiv:1803.03376',
         'year': '2018'},
 'citations_link': '/scholar?cites=15139166138025386131&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BApproximate%2BInference%2BNetworks%2Bfor%2BStructured%2BPrediction%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=kyCgTGQfGdIJ&ei=DssqX-ybC4jHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:kyCgTGQfGdIJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNbFbO6jIcC8MfRFwIlZEgPie_wGfN&scisig=AAGBfm0AAAAAXyrNbALM5Wwwixc-qHNIvK-kh4zxKvDh&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
27
-------------------------------------------------
2020-08-05 15:07:00
Got the results of the query
{'bib': {'abstract': 'Deep neural networks (DNNs) usually contain millions, '
                     'maybe billions, of parameters/weights, making both '
                     'storage and computation very expensive. This has '
                     'motivated a large body of work to reduce the complexity '
                     'of the neural network by using sparsity-inducing '
                     'regularizers. Another well-known approach for '
                     'controlling the complexity of DNNs is parameter '
                     'sharing/tying, where certain sets of weights are forced '
                     'to share a common value. Some forms of weight sharing '
                     'are hard-wired to express certain in',
         'author': ['D Zhang', 'H Wang', 'M Figueiredo'],
         'cites': '12',
         'eprint': 'https://openreview.net/pdf?id=rypT3fb0b',
         'gsrank': '1',
         'title': 'Learning to share: Simultaneous parameter tying and '
                  'sparsification in deep learning',
         'url': 'https://openreview.net/forum?id=rypT3fb0b&noteId=rkwxPE67M',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=2292611582498005860&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BShare:%2Bsimultaneous%2Bparameter%2Btying%2Band%2BSparsification%2Bin%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ZGO9Zhb-0B8J&ei=GcsqX8SgL8KwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ZGO9Zhb-0B8J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNdrgJMy4E0V15955LsjRb3P6ZNoHQ&scisig=AAGBfm0AAAAAXyrNdp-x_mR4NScolJyQ8emkI8ksWpIi&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 15:07:10
Got the results of the query
{'bib': {'abstract': 'Deep neural networks (DNNs) continue to make significant '
                     'advances, solving tasks from image classification to '
                     'translation or reinforcement learning. One aspect of the '
                     'field receiving considerable attention is efficiently '
                     'executing deep models in resource-constrained '
                     'environments, such as mobile or embedded devices. This '
                     'paper focuses on this problem, and proposes two new '
                     'compression methods, which jointly leverage weight '
                     'quantization and distillation of larger teacher networks '
                     'into smaller student networks. The first method we',
         'author': ['A Polino', 'R Pascanu', 'D Alistarh'],
         'cites': '174',
         'eprint': 'https://arxiv.org/pdf/1802.05668',
         'gsrank': '1',
         'title': 'Model compression via distillation and quantization',
         'url': 'https://arxiv.org/abs/1802.05668',
         'venue': 'arXiv preprint arXiv:1802.05668',
         'year': '2018'},
 'citations_link': '/scholar?cites=9862176539747361028&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModel%2Bcompression%2Bvia%2Bdistillation%2Band%2Bquantization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=BCHVk0993YgJ&ei=I8sqX_meBpqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:BCHVk0993YgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNgBuNi41rGJvUfBMaS_amt7s16Xim&scisig=AAGBfm0AAAAAXyrNgOidgMgzbFXMNKvawnEU2j_Rds04&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
174
-------------------------------------------------
2020-08-05 15:07:20
Got the results of the query
{'bib': {'abstract': 'Recent efforts on combining deep models with '
                     'probabilistic graphical models are promising in '
                     'providing flexible models that are also easy to '
                     'interpret. We propose a variational message-passing '
                     'algorithm for variational inference in such models. We '
                     'make three contributions. First, we propose structured '
                     'inference networks that incorporate the structure of the '
                     'graphical model in the inference network of variational '
                     'auto-encoders (VAE). Second, we establish conditions '
                     'under which such inference networks enable fast '
                     'amortized',
         'author': ['W Lin', 'N Hubacher', 'ME Khan'],
         'cites': '26',
         'eprint': 'https://arxiv.org/pdf/1803.05589',
         'gsrank': '1',
         'title': 'Variational message passing with structured inference '
                  'networks',
         'url': 'https://arxiv.org/abs/1803.05589',
         'venue': 'arXiv preprint arXiv:1803.05589',
         'year': '2018'},
 'citations_link': '/scholar?cites=4788714492758509312&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BMessage%2BPassing%2Bwith%2BStructured%2BInference%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=AMein3XudEIJ&ei=N8sqX6XYHYyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:AMein3XudEIJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNlXzTCj25D7NtcJvwaZEoAtxDeJsG&scisig=AAGBfm0AAAAAXyrNlVE3nYmr5WNMzj6mwc5vbc_hxyet&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
26
-------------------------------------------------
2020-08-05 15:07:41
Got the results of the query
{'bib': {'abstract': 'Policy gradient methods have achieved remarkable '
                     'successes in solving challenging reinforcement learning '
                     'problems. However, it still often suffers from the large '
                     'variance issue on policy gradient estimation, which '
                     'leads to poor sample efficiency during training. In this '
                     'work, we propose a control variate method to effectively '
                     'reduce variance for policy gradient methods. Motivated '
                     "by the Stein's identity, our method extends the previous "
                     'control variate methods used in REINFORCE and advantage '
                     'actor-critic by introducing more general action',
         'author': ['H Liu', 'Y Feng', 'Y Mao', 'D Zhou', 'J Peng', 'Q Liu'],
         'cites': '36',
         'eprint': 'https://arxiv.org/pdf/1710.11198',
         'gsrank': '1',
         'title': 'Action-depedent Control Variates for Policy Optimization '
                  "via Stein's Identity",
         'url': 'https://arxiv.org/abs/1710.11198',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=17185260958337372478&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAction-dependent%2BControl%2BVariates%2Bfor%2BPolicy%2BOptimization%2Bvia%2BStein%2BIdentity%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=PpEWSspPfu4J&ei=QcsqX4T3O7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:PpEWSspPfu4J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNnpncaiJhQHQ0PIxY9TreUcBLF4SF&scisig=AAGBfm0AAAAAXyrNngK4AVdK2OubifnlC3D557plZAGh&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
36
-------------------------------------------------
2020-08-05 15:07:50
Got the results of the query
{'bib': {'abstract': 'We describe an end-to-end trainable model for image '
                     'compression based on variational autoencoders. The model '
                     'incorporates a hyperprior to effectively capture spatial '
                     'dependencies in the latent representation. This '
                     'hyperprior relates to side information, a concept '
                     'universal to virtually all modern image codecs, but '
                     'largely unexplored in image compression using artificial '
                     'neural networks (ANNs). Unlike existing autoencoder '
                     'compression methods, our model trains a complex prior '
                     'jointly with the underlying',
         'author': ['J Ballé', 'D Minnen', 'S Singh', 'SJ Hwang'],
         'cites': '203',
         'eprint': 'https://arxiv.org/pdf/1802.01436',
         'gsrank': '1',
         'title': 'Variational image compression with a scale hyperprior',
         'url': 'https://arxiv.org/abs/1802.01436',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=17854603515786987053&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2Bimage%2Bcompression%2Bwith%2Ba%2Bscale%2Bhyperprior%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=LYbsw0xLyPcJ&ei=S8sqX6XUBI-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:LYbsw0xLyPcJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNqJHJOt2aKk6JsG603_kFM4O4OwFJ&scisig=AAGBfm0AAAAAXyrNqDywvKIhIs6-3wtVJtZ8Zn_J8pzM&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
203
-------------------------------------------------
2020-08-05 15:08:00
Got the results of the query
{'bib': {'abstract': 'Disentangled representations, where the higher level '
                     'data generative factors are reflected in disjoint latent '
                     'dimensions, offer several benefits such as ease of '
                     'deriving invariant representations, transferability to '
                     'other tasks, interpretability, etc. We consider the '
                     'problem of unsupervised learning of disentangled '
                     'representations from large pool of unlabeled '
                     'observations, and propose a variational inference based '
                     'approach to infer disentangled latent factors. We '
                     'introduce a regularizer on the expectation of the '
                     'approximate posterior',
         'author': ['A Kumar', 'P Sattigeri', 'A Balakrishnan'],
         'cites': '122',
         'eprint': 'https://arxiv.org/pdf/1711.00848',
         'gsrank': '1',
         'title': 'Variational inference of disentangled latent concepts from '
                  'unlabeled observations',
         'url': 'https://arxiv.org/abs/1711.00848',
         'venue': 'arXiv preprint arXiv:1711.00848',
         'year': '2017'},
 'citations_link': '/scholar?cites=84314681776183574&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVariational%2BInference%2Bof%2BDisentangled%2BLatent%2BConcepts%2Bfrom%2BUnlabeled%2BObservations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=FpmeoMGLKwEJ&ei=VssqX5zNOYvrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:FpmeoMGLKwEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNtPuHLbBfjYqb4ib6ciJ1UcqtoSES&scisig=AAGBfm0AAAAAXyrNtKnUJFU02r4cJd077bAQgYxZJ1R7&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
122
-------------------------------------------------
2020-08-05 15:08:12
Got the results of the query
{'bib': {'abstract': 'Stochastic neural net weights are used in a variety of '
                     'contexts, including regularization, Bayesian neural '
                     'nets, exploration in reinforcement learning, and '
                     'evolution strategies. Unfortunately, due to the large '
                     'number of weights, all the examples in a mini-batch '
                     'typically share the same weight perturbation, thereby '
                     'limiting the variance reduction effect of large '
                     'mini-batches. We introduce flipout, an efficient method '
                     'for decorrelating the gradients within a mini-batch by '
                     'implicitly sampling pseudo-independent weight '
                     'perturbations for each',
         'author': ['Y Wen', 'P Vicol', 'J Ba', 'D Tran', 'R Grosse'],
         'cites': '55',
         'eprint': 'https://arxiv.org/pdf/1803.04386',
         'gsrank': '1',
         'title': 'Flipout: Efficient pseudo-independent weight perturbations '
                  'on mini-batches',
         'url': 'https://arxiv.org/abs/1803.04386',
         'venue': 'arXiv preprint arXiv:1803.04386',
         'year': '2018'},
 'citations_link': '/scholar?cites=3775795413523094771&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFlipout:%2BEfficient%2BPseudo-Independent%2BWeight%2BPerturbations%2Bon%2BMini-Batches%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=89yXb-xRZjQJ&ei=X8sqX9qyJo-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:89yXb-xRZjQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrNvCGoqMXaqNC8e2STZZmbG9KimY5u&scisig=AAGBfm0AAAAAXyrNvLTI3zk_IYXXVieENypC_69gibmd&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
55
-------------------------------------------------
2020-08-05 15:08:20
Got the results of the query
{'bib': {'abstract': 'Recent progress in variational inference has paid much '
                     'attention to the flexibility of variational posteriors. '
                     'One promising direction is to use implicit '
                     'distributions, ie, distributions without tractable '
                     'densities as the variational posterior. However, '
                     'existing methods on implicit',
         'author': ['J Shi', 'S Sun', 'J Zhu'],
         'cites': '26',
         'eprint': 'https://arxiv.org/pdf/1705.10119',
         'gsrank': '1',
         'title': 'Kernel implicit variational inference',
         'url': 'https://arxiv.org/abs/1705.10119',
         'venue': 'arXiv preprint arXiv:1705.10119',
         'year': '2017'},
 'citations_link': '/scholar?cites=12164297985186299084&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DKernel%2BImplicit%2BVariational%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=zHwHfZFE0KgJ&ei=ccsqX57UA4jHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:zHwHfZFE0KgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrN0KmlYzwIAKXsvl_eg4It_WiNXki_&scisig=AAGBfm0AAAAAXyrN0B28jprEOQ7gr0ycICmCG5S_X2lR&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
26
-------------------------------------------------
2020-08-05 15:08:40
Got the results of the query
{'bib': {'abstract': 'We leverage recent insights from second-order '
                     'optimisation for neural networks to construct a '
                     'Kronecker factored Laplace approximation to the '
                     'posterior over the weights of a trained network. Our '
                     'approximation requires no modification of the training '
                     'procedure, enabling practitioners to estimate the '
                     'uncertainty of their models currently used in production '
                     'without having to retrain them. We extensively compare '
                     'our method to using Dropout and a diagonal Laplace '
                     'approximation for estimating the uncertainty of a '
                     'network. We demonstrate that our',
         'author': ['H Ritter', 'A Botev', 'D Barber'],
         'cites': '56',
         'eprint': 'https://discovery.ucl.ac.uk/10080902/1/kflaplace.pdf',
         'gsrank': '1',
         'title': 'A scalable laplace approximation for neural networks',
         'url': 'https://discovery.ucl.ac.uk/id/eprint/10080902/',
         'venue': '6th International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=3068639073703398000&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BScalable%2BLaplace%2BApproximation%2Bfor%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=cIr5q_r-lSoJ&ei=f8sqX7-7CqiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:cIr5q_r-lSoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrN5EvbYLcEDQn_fyTaogcCXLkd8JDS&scisig=AAGBfm0AAAAAXyrN5Et1do7EEFtxoUU7hqNvyln4pMo0&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
56
-------------------------------------------------
2020-08-05 15:09:00
Got the results of the query
{'bib': {'abstract': 'Recent research has shown that one can train a neural '
                     'network with binary weights and activations at train '
                     'time by augmenting the weights with a high-precision '
                     'continuous latent variable that accumulates small '
                     'changes from stochastic gradient descent. However, there '
                     'is a dearth of theoretical analysis to explain why we '
                     'can effectively capture the features in our data with '
                     'binary weights and activations. Our main result is that '
                     'the neural networks with binary weights and activations '
                     'trained using the method of Courbariaux, Hubara et '
                     'al.(2016)',
         'author': ['AG Anderson', 'CP Berg'],
         'cites': '40',
         'eprint': 'https://arxiv.org/pdf/1705.07199',
         'gsrank': '1',
         'title': 'The high-dimensional geometry of binary neural networks',
         'url': 'https://arxiv.org/abs/1705.07199',
         'venue': 'arXiv preprint arXiv:1705.07199',
         'year': '2017'},
 'citations_link': '/scholar?cites=16436940415078137739&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2BHigh-Dimensional%2BGeometry%2Bof%2BBinary%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=i3_-iji-G-QJ&ei=mMsqX8KAIbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:i3_-iji-G-QJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrN9d8FwrELSCQQtYDLaT4WIYUp-q5d&scisig=AAGBfm0AAAAAXyrN9Vk-JkVLEWfuk0K1AKQjkoIMr9Qu&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
40
-------------------------------------------------
2020-08-05 15:09:17
Got the results of the query
{'bib': {'abstract': 'Deep learning networks have achieved state-of-the-art '
                     'accuracies on computer vision workloads like image '
                     'classification and object detection. The performant '
                     'systems, however, typically involve big models with '
                     'numerous parameters. Once trained, a challenging aspect '
                     'for such top performing models is deployment on resource '
                     'constrained inference systems-the models (often deep '
                     'networks or wide networks or both) are compute and '
                     'memory intensive. Low-precision numerics and model '
                     'compression using knowledge distillation are',
         'author': ['A Mishra', 'D Marr'],
         'cites': '117',
         'eprint': 'https://arxiv.org/pdf/1711.05852',
         'gsrank': '1',
         'title': 'Apprentice: Using knowledge distillation techniques to '
                  'improve low-precision network accuracy',
         'url': 'https://arxiv.org/abs/1711.05852',
         'venue': 'arXiv preprint arXiv:1711.05852',
         'year': '2017'},
 'citations_link': '/scholar?cites=4466439802890649800&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DApprentice:%2BUsing%2BKnowledge%2BDistillation%2BTechniques%2BTo%2BImprove%2BLow-Precision%2BNetwork%2BAccuracy%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=yID2JVb7-z0J&ei=ossqX5uQNKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:yID2JVb7-z0J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOAXC0PQwceZiqL_DUhiU7WMOJcxmm&scisig=AAGBfm0AAAAAXyrOAdmD6KCAjZSqPBv4-KxVHEfpDQfw&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
117
-------------------------------------------------
2020-08-05 15:09:29
Got the results of the query
{'bib': {'abstract': 'We propose a distributed architecture for deep '
                     'reinforcement learning at scale, that enables agents to '
                     'learn effectively from orders of magnitude more data '
                     'than previously possible. The algorithm decouples acting '
                     'from learning: the actors interact with their own '
                     'instances of the',
         'author': ['D Horgan', 'J Quan', 'D Budden', 'G Barth-Maron'],
         'cites': '214',
         'eprint': 'https://arxiv.org/pdf/1803.00933',
         'gsrank': '1',
         'title': 'Distributed prioritized experience replay',
         'url': 'https://arxiv.org/abs/1803.00933',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11991031813817503126&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDistributed%2BPrioritized%2BExperience%2BReplay%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=lg0ASeezaKYJ&ei=rcsqX4KeNaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:lg0ASeezaKYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOCiqw3GGpckPzMun17OKDl2lE5sCd&scisig=AAGBfm0AAAAAXyrOCtPqPYC-1gGJlMl_ZcrQ8v-z_XTu&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
214
-------------------------------------------------
2020-08-05 15:09:39
Got the results of the query
{'bib': {'abstract': 'Deep learning methods have achieved high performance in '
                     'sound recognition tasks. Deciding how to feed the '
                     'training data is important for further performance '
                     'improvement. We propose a novel learning method for deep '
                     'sound recognition: Between-Class learning (BC learning). '
                     'Our strategy is to learn a discriminative feature space '
                     'by recognizing the between-class sounds as between-class '
                     'sounds. We generate between-class sounds by mixing two '
                     'sounds belonging to different classes with a random '
                     'ratio. We then input the mixed sound to',
         'author': ['Y Tokozume', 'Y Ushiku', 'T Harada'],
         'cites': '74',
         'eprint': 'https://arxiv.org/pdf/1711.10282',
         'gsrank': '1',
         'title': 'Learning from between-class examples for deep sound '
                  'recognition',
         'url': 'https://arxiv.org/abs/1711.10282',
         'venue': 'arXiv preprint arXiv:1711.10282',
         'year': '2017'},
 'citations_link': '/scholar?cites=13221046760066147561&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bfrom%2BBetween-class%2BExamples%2Bfor%2BDeep%2BSound%2BRecognition%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=6WSS7PqXercJ&ei=tcsqX4O5DIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:6WSS7PqXercJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOFD_UU1w2TVB5erxftki3Wm_DBcwT&scisig=AAGBfm0AAAAAXyrOFPrEyo3fC9f0gVBr7U5I62b4jG80&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
74
-------------------------------------------------
2020-08-05 15:09:49
Got the results of the query
{'bib': {'abstract': 'The problem of detecting whether a test sample is from '
                     'in-distribution (ie, training distribution by a '
                     'classifier) or out-of-distribution sufficiently '
                     'different from it arises in many real-world machine '
                     'learning applications. However, the state-of-art deep '
                     'neural networks are known to be highly overconfident in '
                     'their predictions, ie, do not distinguish in-and '
                     'out-of-distributions. Recently, to handle this issue, '
                     'several threshold-based detectors have been proposed '
                     'given pre-trained neural classifiers. However, the '
                     'performance of prior works highly depends on',
         'author': ['K Lee', 'H Lee', 'K Lee', 'J Shin'],
         'cites': '210',
         'eprint': 'https://arxiv.org/pdf/1711.09325',
         'gsrank': '1',
         'title': 'Training confidence-calibrated classifiers for detecting '
                  'out-of-distribution samples',
         'url': 'https://arxiv.org/abs/1711.09325',
         'venue': 'arXiv preprint arXiv:1711.09325',
         'year': '2017'},
 'citations_link': '/scholar?cites=14294577348397503039&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTraining%2BConfidence-calibrated%2BClassifiers%2Bfor%2BDetecting%2BOut-of-Distribution%2BSamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=PxKUyVyKYMYJ&ei=wMsqX7PXGYvrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:PxKUyVyKYMYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOG9k01KjmayZ1uVTbcn6A37uJd0bu&scisig=AAGBfm0AAAAAXyrOG4IarOuEqhe1kH4LaWDVSlfdqVRC&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
210
-------------------------------------------------
2020-08-05 15:09:56
Got the results of the query
{'bib': {'abstract': 'We present a new neural text to speech (TTS) method that '
                     'is able to transform text to speech in voices that are '
                     'sampled in the wild. Unlike other systems, our solution '
                     'is able to deal with unconstrained voice samples and '
                     'without requiring aligned phonemes or linguistic '
                     'features. The network architecture is simpler than those '
                     'in the existing literature and is based on a novel '
                     'shifting buffer working memory. The same buffer is used '
                     'for estimating the attention, computing the output '
                     'audio, and for updating the buffer itself. The input '
                     'sentence is encoded',
         'author': ['Y Taigman', 'L Wolf', 'A Polyak', 'E Nachmani'],
         'cites': '67',
         'eprint': 'https://arxiv.org/pdf/1707.06588.pdf?source=post_page---------------------------',
         'gsrank': '1',
         'title': 'Voiceloop: Voice fitting and synthesis via a phonological '
                  'loop',
         'url': 'https://arxiv.org/abs/1707.06588',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=14159878382438547497&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DVoiceLoop:%2BVoice%2BFitting%2Band%2BSynthesis%2Bvia%2Ba%2BPhonological%2BLoop%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=KYD7113-gcQJ&ei=yMsqX9z6DZqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:KYD7113-gcQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOJRieUsrhjXCFGHKNC5pAC6sls8pF&scisig=AAGBfm0AAAAAXyrOJXj0hnsRfvljIimiJxIu9VffUmXm&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
67
-------------------------------------------------
2020-08-05 15:10:05
Got the results of the query
{'bib': {'abstract': 'Techniques such as ensembling and distillation promise '
                     'model quality improvements when paired with almost any '
                     'base model. However, due to increased test-time cost '
                     '(for ensembles) and increased complexity of the training '
                     'pipeline (for distillation), these techniques are '
                     'challenging to use in industrial settings. In this paper '
                     'we explore a variant of distillation which is relatively '
                     'straightforward to use as it does not require a '
                     'complicated multi-stage setup or many new '
                     'hyperparameters. Our first claim is that online '
                     'distillation enables',
         'author': ['R Anil', 'G Pereyra', 'A Passos', 'R Ormandi'],
         'cites': '107',
         'eprint': 'https://arxiv.org/pdf/1804.03235',
         'gsrank': '1',
         'title': 'Large scale distributed neural network training through '
                  'online distillation',
         'url': 'https://arxiv.org/abs/1804.03235',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=1698767877858492764&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLarge%2Bscale%2Bdistributed%2Bneural%2Bnetwork%2Btraining%2Bthrough%2Bonline%2Bdistillation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=XIGZa2I8kxcJ&ei=0csqX8LOKcKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:XIGZa2I8kxcJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOLhpqyvbd_oGi5ZIMbOZcywAAkrUO&scisig=AAGBfm0AAAAAXyrOLhHFc7UXuncoLs2F-zHX0u6pzBdd&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
107
-------------------------------------------------
2020-08-05 15:10:14
Got the results of the query
{'bib': {'abstract': 'We demonstrate that it is possible to train large '
                     'recurrent language models with user-level differential '
                     'privacy guarantees with only a negligible cost in '
                     'predictive accuracy. Our work builds on recent advances '
                     'in the training of deep networks on user-partitioned '
                     'data and',
         'author': ['HB McMahan', 'D Ramage', 'K Talwar'],
         'cites': '141',
         'eprint': 'https://arxiv.org/pdf/1710.06963',
         'gsrank': '1',
         'title': 'Learning differentially private recurrent language models',
         'url': 'https://arxiv.org/abs/1710.06963',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=1102128856283131840&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BDifferentially%2BPrivate%2BRecurrent%2BLanguage%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=wK92_1qMSw8J&ei=2csqX86YOLGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:wK92_1qMSw8J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrON5av_ImJv-T736O3POp38vzhLHa7&scisig=AAGBfm0AAAAAXyrONyhYTl8jw1fa8njmKADr7304fncq&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
141
-------------------------------------------------
2020-08-05 15:10:24
Got the results of the query
{'bib': {'abstract': 'Contrary to most natural language processing research, '
                     'which makes use of static datasets, humans learn '
                     'language interactively, grounded in an environment. In '
                     'this work we propose an interactive learning procedure '
                     'called Mechanical Turker Descent (MTD) and use it to '
                     'train agents to execute natural language commands '
                     'grounded in a fantasy text adventure game. In MTD, '
                     'Turkers compete to train better agents in the short '
                     "term, and collaborate by sharing their agents' skills in "
                     'the long term. This results in a gamified, engaging '
                     'experience',
         'author': ['Z Yang', 'S Zhang', 'J Urbanek', 'W Feng', 'AH Miller'],
         'cites': '12',
         'eprint': 'https://arxiv.org/pdf/1711.07950',
         'gsrank': '1',
         'title': 'Mastering the dungeon: Grounded language learning by '
                  'mechanical turker descent',
         'url': 'https://arxiv.org/abs/1711.07950',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=13176067834774442541&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMastering%2Bthe%2BDungeon:%2BGrounded%2BLanguage%2BLearning%2Bby%2BMechanical%2BTurker%2BDescent%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=LVIuzOLL2rYJ&ei=4ssqX5CeF4vrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:LVIuzOLL2rYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOQT6gM2ODdY05U2XtSxbS6S1kDJ5H&scisig=AAGBfm0AAAAAXyrOQa2JPdBpVaXjYlqgA5CTGoi0yxB-&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 15:10:33
Got the results of the query
{'bib': {'abstract': 'We show that generating English Wikipedia articles can '
                     'be approached as a multi-document summarization of '
                     'source documents. We use extractive summarization to '
                     'coarsely identify salient information and a neural '
                     'abstractive model to generate the article. For the '
                     'abstractive model, we introduce a decoder-only '
                     'architecture that can scalably attend to very long '
                     'sequences, much longer than typical encoder-decoder '
                     'architectures used in sequence transduction. We show '
                     'that this model can generate fluent, coherent '
                     'multi-sentence',
         'author': ['PJ Liu', 'M Saleh', 'E Pot', 'B Goodrich', 'R Sepassi'],
         'cites': '223',
         'eprint': 'https://arxiv.org/pdf/1801.10198',
         'gsrank': '1',
         'title': 'Generating wikipedia by summarizing long sequences',
         'url': 'https://arxiv.org/abs/1801.10198',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=9480555348664414627&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerating%2BWikipedia%2Bby%2BSummarizing%2BLong%2BSequences%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=o3GSBtyykYMJ&ei=7MsqX6nXO7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:o3GSBtyykYMJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOSJEsOKpvY26p45b1NGY2uNi2QQif&scisig=AAGBfm0AAAAAXyrOSOeAFopNAA33rwRfQPU-WIdTum_H&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
223
-------------------------------------------------
2020-08-05 15:10:41
Got the results of the query
{'bib': {'abstract': 'Machine translation has recently achieved impressive '
                     'performance thanks to recent advances in deep learning '
                     'and the availability of large-scale parallel corpora. '
                     'There have been numerous attempts to extend these '
                     'successes to low-resource language pairs, yet requiring '
                     'tens of thousands of parallel sentences. In this work, '
                     'we take this research direction to the extreme and '
                     'investigate whether it is possible to learn to translate '
                     'even without any parallel data. We propose a model that '
                     'takes sentences from monolingual',
         'author': ['G Lample', 'A Conneau', 'L Denoyer'],
         'cites': '451',
         'eprint': 'https://arxiv.org/pdf/1711.00043.pdf;Guillaume',
         'gsrank': '1',
         'title': 'Unsupervised machine translation using monolingual corpora '
                  'only',
         'url': 'https://arxiv.org/abs/1711.00043',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=682955820897938264&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BMachine%2BTranslation%2BUsing%2BMonolingual%2BCorpora%2BOnly%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=WCvoXbNYegkJ&ei=9MsqX5DnKrGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:WCvoXbNYegkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOU682oFegt3bHHEvhofgdUHOwKviU&scisig=AAGBfm0AAAAAXyrOU75t3m1TsK0ZgmkgCuFSwvaOZQ48&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
451
-------------------------------------------------
2020-08-05 15:10:51
Got the results of the query
{'bib': {'abstract': 'Attentional, RNN-based encoder-decoder models for '
                     'abstractive summarization have achieved good performance '
                     'on short input and output sequences. For longer '
                     'documents and summaries however these models often '
                     'include repetitive and incoherent phrases. We',
         'author': ['R Paulus', 'C Xiong', 'R Socher'],
         'cites': '678',
         'eprint': 'https://arxiv.org/pdf/1705.04304',
         'gsrank': '1',
         'title': 'A deep reinforced model for abstractive summarization',
         'url': 'https://arxiv.org/abs/1705.04304',
         'venue': 'arXiv preprint arXiv:1705.04304',
         'year': '2017'},
 'citations_link': '/scholar?cites=439043726958667778&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BDeep%2BReinforced%2BModel%2Bfor%2BAbstractive%2BSummarization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Aig8ivHLFwYJ&ei=_ssqX9fQI4-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Aig8ivHLFwYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOXPFS3NcOU6wNMrss5KKDiM57BWuO&scisig=AAGBfm0AAAAAXyrOXC1HvHbcY0sf513lZvjw08d83SIN&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
678
-------------------------------------------------
2020-08-05 15:11:00
Got the results of the query
{'bib': {'abstract': 'Natural language processing (NLP) models often require a '
                     'massive number of parameters for word embeddings, '
                     'resulting in a large storage or memory footprint. '
                     'Deploying neural NLP models to mobile devices requires '
                     'compressing the word embeddings without any significant '
                     'sacrifices in performance. For this purpose, we propose '
                     'to construct the embeddings with few basis vectors. For '
                     'each word, the composition of basis vectors is '
                     'determined by a hash code. To maximize the compression '
                     'rate, we adopt the multi',
         'author': ['R Shu', 'H Nakayama'],
         'cites': '57',
         'eprint': 'https://arxiv.org/pdf/1711.01068',
         'gsrank': '1',
         'title': 'Compressing word embeddings via deep compositional code '
                  'learning',
         'url': 'https://arxiv.org/abs/1711.01068',
         'venue': 'arXiv preprint arXiv:1711.01068',
         'year': '2017'},
 'citations_link': '/scholar?cites=7815180689701290231&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCompressing%2BWord%2BEmbeddings%2Bvia%2BDeep%2BCompositional%2BCode%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=90AjQW4ZdWwJ&ei=BswqX4GpPKiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:90AjQW4ZdWwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOYjSurx8PBS5kLuKZDGWQXL3DyMHq&scisig=AAGBfm0AAAAAXyrOYsY_v8pChLJH3De-ZLakV8ih5eoq&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
57
-------------------------------------------------
2020-08-05 15:11:06
Got the results of the query
{'bib': {'abstract': 'Large-scale distributed training requires significant '
                     'communication bandwidth for gradient exchange that '
                     'limits the scalability of multi-node training, and '
                     'requires expensive high-bandwidth network '
                     'infrastructure. The situation gets even worse with '
                     'distributed training on mobile devices (federated '
                     'learning), which suffers from higher latency, lower '
                     'throughput, and intermittent poor connections. In this '
                     'paper, we find 99.9% of the gradient exchange in '
                     'distributed SGD is redundant, and propose Deep Gradient '
                     'Compression (DGC) to greatly',
         'author': ['Y Lin', 'S Han', 'H Mao', 'Y Wang', 'WJ Dally'],
         'cites': '320',
         'eprint': 'https://arxiv.org/pdf/1712.01887.pdf?source=post_page---------------------------',
         'gsrank': '1',
         'title': 'Deep gradient compression: Reducing the communication '
                  'bandwidth for distributed training',
         'url': 'https://arxiv.org/abs/1712.01887',
         'venue': 'arXiv preprint arXiv:1712.01887',
         'year': '2017'},
 'citations_link': '/scholar?cites=2485379403852124678&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BGradient%2BCompression:%2BReducing%2Bthe%2BCommunication%2BBandwidth%2Bfor%2BDistributed%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=BpKtUmbXfSIJ&ei=D8wqX_3lJo-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:BpKtUmbXfSIJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrObjI-IIJ45QPPpOZo7P6kRoUDIK9q&scisig=AAGBfm0AAAAAXyrObvLeBOwX4d6Q1AQ9sZn2awRPgIS5&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
320
-------------------------------------------------
2020-08-05 15:11:18
Got the results of the query
{'bib': {'abstract': 'Current end-to-end machine reading and question '
                     'answering (Q\\&A) models are primarily based on '
                     'recurrent neural networks (RNNs) with attention. Despite '
                     'their success, these models are often slow for both '
                     'training and inference due to the sequential nature of '
                     'RNNs',
         'author': ['AW Yu', 'D Dohan', 'MT Luong', 'R Zhao', 'K Chen'],
         'cites': '385',
         'eprint': 'https://arxiv.org/pdf/1804.09541',
         'gsrank': '1',
         'title': 'Qanet: Combining local convolution with global '
                  'self-attention for reading comprehension',
         'url': 'https://arxiv.org/abs/1804.09541',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15745561136241294753&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQANet:%2BCombining%2BLocal%2BConvolution%2Bwith%2BGlobal%2BSelf-Attention%2Bfor%2BReading%2BComprehension%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ob3bCm54g9oJ&ei=GswqX5bMNqiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ob3bCm54g9oJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOecw3o23p06MWz54NG4auOgRczV7x&scisig=AAGBfm0AAAAAXyrOeaw0XehoDwPUdnvFuDcADPF9rQmk&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
385
-------------------------------------------------
2020-08-05 15:11:29
Got the results of the query
{'bib': {'abstract': 'In spite of the recent success of neural machine '
                     'translation (NMT) in standard benchmarks, the lack of '
                     'large parallel corpora poses a major practical problem '
                     'for many language pairs. There have been several '
                     'proposals to alleviate this issue with, for instance, '
                     'triangulation',
         'author': ['M Artetxe', 'G Labaka', 'E Agirre', 'K Cho'],
         'cites': '373',
         'eprint': 'https://arxiv.org/pdf/1710.11041.pdf?source=post_page---------------------------',
         'gsrank': '1',
         'title': 'Unsupervised neural machine translation',
         'url': 'https://arxiv.org/abs/1710.11041',
         'venue': 'arXiv preprint arXiv:1710.11041',
         'year': '2017'},
 'citations_link': '/scholar?cites=6109181985493123662&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BNeural%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ToKI4aUsyFQJ&ei=J8wqX5SVMIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ToKI4aUsyFQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOg2nWXgL9Nm1mIlMTFm_3_E_y9SW4&scisig=AAGBfm0AAAAAXyrOg-pLjUlha7ujCG4QccFkRm0bcRPU&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
373
-------------------------------------------------
2020-08-05 15:11:40
Got the results of the query
{'bib': {'abstract': 'We consider the problem of learning a one-hidden-layer '
                     'neural network: we assume the input $ x\\in\\mathbb {R}^ '
                     'd $ is from Gaussian distribution and the label $ y= '
                     'a^\\top\\sigma (Bx)+\\xi $, where $ a $ is a nonnegative '
                     'vector in $\\mathbb {R}^ m $ with $ m\\le d $, $ '
                     'B\\in\\mathbb {R}^{m\\times d} $ is a full-rank weight '
                     'matrix, and $\\xi $ is a noise vector. We first give an '
                     'analytic formula for the population risk of the standard '
                     'squared loss and demonstrate that it implicitly attempts '
                     'to decompose a sequence of low-rank tensors',
         'author': ['R Ge', 'JD Lee', 'T Ma'],
         'cites': '135',
         'eprint': 'https://arxiv.org/pdf/1711.00501',
         'gsrank': '1',
         'title': 'Learning one-hidden-layer neural networks with landscape '
                  'design',
         'url': 'https://arxiv.org/abs/1711.00501',
         'venue': 'arXiv preprint arXiv:1711.00501',
         'year': '2017'},
 'citations_link': '/scholar?cites=14789158572261278986&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BOne-hidden-layer%2BNeural%2BNetworks%2Bwith%2BLandscape%2BDesign%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=CoWOel2lPc0J&ei=MMwqX7PsFaiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:CoWOel2lPc0J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOjC506J0pzwYsNnHxOssy7LM9OCQm&scisig=AAGBfm0AAAAAXyrOjL2k4RU4a1qaj5uu14R1eRfIbkvK&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
135
-------------------------------------------------
2020-08-05 15:11:48
Got the results of the query
{'bib': {'abstract': 'Due to the success of deep learning to solving a variety '
                     'of challenging machine learning tasks, there is a rising '
                     'interest in understanding loss functions for training '
                     'neural networks from a theoretical aspect. Particularly, '
                     'the properties of critical points and the landscape '
                     'around them are of importance to determine the '
                     'convergence performance of optimization algorithms. In '
                     'this paper, we provide a necessary and sufficient '
                     'characterization of the analytical forms for the '
                     'critical points (as well as global minimizers) of the '
                     'square loss',
         'author': ['Y Zhou', 'Y Liang'],
         'cites': '22',
         'eprint': 'https://openreview.net/pdf?id=SysEexbRb',
         'gsrank': '1',
         'title': 'Critical points of linear neural networks: Analytical forms '
                  'and landscape properties',
         'url': 'https://openreview.net/forum?id=SysEexbRb',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=18406123238305951513&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCritical%2BPoints%2Bof%2BLinear%2BNeural%2BNetworks:%2BAnalytical%2BForms%2Band%2BLandscape%2BProperties%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Ge9Jv5Cvb_8J&ei=OMwqX8e7EaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Ge9Jv5Cvb_8J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOmBRLcIdJvaB9dmDv_PM56F-23luT&scisig=AAGBfm0AAAAAXyrOmFxG5awwygEP1b85dsfQ7cCWz7K-&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
22
-------------------------------------------------
2020-08-05 15:12:00
Got the results of the query
{'bib': {'abstract': 'Multiagent systems where agents interact among '
                     'themselves and with a stochastic environment can be '
                     'formalized as stochastic games. We study a subclass '
                     'named Markov potential games (MPGs) that appear often in '
                     'economic and engineering applications when the agents '
                     'share a common resource. We consider MPGs with '
                     'continuous state-action variables, coupled constraints '
                     'and nonconvex rewards. Previous analysis followed a '
                     'variational approach that is only valid for very simple '
                     'cases (convex rewards, invertible',
         'author': ['SV Macua', 'J Zazo', 'S Zazo'],
         'cites': '5',
         'eprint': 'https://arxiv.org/pdf/1802.00899',
         'gsrank': '1',
         'title': 'Learning Parametric Closed-Loop Policies for Markov '
                  'Potential Games',
         'url': 'https://arxiv.org/abs/1802.00899',
         'venue': 'arXiv preprint arXiv:1802.00899',
         'year': '2018'},
 'citations_link': '/scholar?cites=6885879364058847450&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BParametric%2BClosed-Loop%2BPolicies%2Bfor%2BMarkov%2BPotential%2BGames%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=2pjrdMyOj18J&ei=RMwqX5uZF4yimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:2pjrdMyOj18J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOomstVziDdY373VhQLS383fu7W3BP&scisig=AAGBfm0AAAAAXyrOol33CHA2Kbg0FJ5gqR3feq-VckcC&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
5
-------------------------------------------------
2020-08-05 15:12:11
Got the results of the query
{'bib': {'abstract': 'It is well-known that neural networks are universal '
                     'approximators, but that deeper networks tend in practice '
                     'to be more powerful than shallower ones. We shed light '
                     'on this by proving that the total number of neurons $ m '
                     '$ required to approximate natural classes of '
                     'multivariate polynomials of $ n $ variables grows only '
                     'linearly with $ n $ for deep neural networks, but grows '
                     'exponentially when merely a single hidden layer is '
                     'allowed. We also provide evidence that when the number '
                     'of hidden layers is increased from $1 $ to $ k $, the',
         'author': ['D Rolnick', 'M Tegmark'],
         'cites': '73',
         'eprint': 'https://arxiv.org/pdf/1705.05502',
         'gsrank': '1',
         'title': 'The power of deeper networks for expressing natural '
                  'functions',
         'url': 'https://arxiv.org/abs/1705.05502',
         'venue': 'arXiv preprint arXiv:1705.05502',
         'year': '2017'},
 'citations_link': '/scholar?cites=11248399658974838640&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThe%2Bpower%2Bof%2Bdeeper%2Bnetworks%2Bfor%2Bexpressing%2Bnatural%2Bfunctions%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=cP_RrORXGpwJ&ei=TcwqX7zmO4-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:cP_RrORXGpwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOqb2_VHX7RphNq_rYoO2QSqEDimBC&scisig=AAGBfm0AAAAAXyrOqUz-iUzrVmA7AhLS4doN8h0Devyf&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
73
-------------------------------------------------
2020-08-05 15:12:17
Got the results of the query
{'bib': {'abstract': 'This work aims to provide comprehensive landscape '
                     'analysis of empirical risk in deep neural networks '
                     '(DNNs), including the convergence behavior of its '
                     'gradient, its stationary points and the empirical risk '
                     'itself to their corresponding population counterparts, '
                     'which reveals how various network parameters determine '
                     'the convergence performance. In particular, for an $ l '
                     '$-layer linear neural network consisting of $\\dm_i $ '
                     'neurons in the $ i $-th layer, we prove the gradient of '
                     'its empirical risk uniformly converges to the one of its',
         'author': ['P Zhou', 'J Feng'],
         'cites': '6',
         'eprint': 'https://openreview.net/pdf?id=B1QgVti6Z',
         'gsrank': '1',
         'title': 'Empirical risk landscape analysis for understanding deep '
                  'neural networks',
         'url': 'https://openreview.net/forum?id=rkzlEFsTb',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=4445416685477578234&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmpirical%2BRisk%2BLandscape%2BAnalysis%2Bfor%2BUnderstanding%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=-kV8MOxKsT0J&ei=V8wqX9bXGYvrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:-kV8MOxKsT0J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOtjOej2W_zagYUXznFYXOkKBvxmcK&scisig=AAGBfm0AAAAAXyrOttOOXWXDHCIf2zGwISiEKFgnAf8R&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
6
-------------------------------------------------
2020-08-05 15:12:30
Got the results of the query
{'bib': {'abstract': 'Generative adversarial training can be generally '
                     'understood as minimizing certain moment matching loss '
                     'defined by a set of discriminator functions, typically '
                     'neural networks. The discriminator set should be large '
                     'enough to be able to uniquely identify the true '
                     'distribution (discriminative), and also be small enough '
                     'to go beyond memorizing samples (generalizable). In this '
                     'paper, we show that a discriminator set is guaranteed to '
                     'be discriminative whenever its linear span is dense in '
                     'the set of bounded continuous functions',
         'author': ['P Zhang', 'Q Liu', 'D Zhou', 'T Xu', 'X He'],
         'cites': '33',
         'eprint': 'https://arxiv.org/pdf/1711.02771',
         'gsrank': '1',
         'title': 'On the discrimination-generalization tradeoff in GANs',
         'url': 'https://arxiv.org/abs/1711.02771',
         'venue': 'arXiv preprint arXiv:1711.02771',
         'year': '2017'},
 'citations_link': '/scholar?cites=5417043349429629784&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BDiscrimination-Generalization%2BTradeoff%2Bin%2BGANs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=WDOlvzk0LUsJ&ei=YcwqX9a7LpqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:WDOlvzk0LUsJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOvSnl6KyvV2Vi9LD_pbC5LADpbBI9&scisig=AAGBfm0AAAAAXyrOvZMdu2cUv8K8T3iG2TtYOOX6zyF8&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
33
-------------------------------------------------
2020-08-05 15:12:37
Got the results of the query
{'bib': {'abstract': 'Many machine learning algorithms are vulnerable to '
                     'almost imperceptible perturbations of their inputs. So '
                     'far it was unclear how much risk adversarial '
                     'perturbations carry for the safety of real-world machine '
                     'learning applications because most methods used to '
                     'generate such perturbations rely either on detailed '
                     'model information (gradient-based attacks) or on '
                     'confidence scores such as class probabilities '
                     '(score-based attacks), neither of which are available in '
                     'most real-world scenarios. In many such cases one '
                     'currently needs to retreat to',
         'author': ['W Brendel', 'J Rauber', 'M Bethge'],
         'cites': '319',
         'eprint': 'https://arxiv.org/pdf/1712.04248',
         'gsrank': '1',
         'title': 'Decision-based adversarial attacks: Reliable attacks '
                  'against black-box machine learning models',
         'url': 'https://arxiv.org/abs/1712.04248',
         'venue': 'arXiv preprint arXiv:1712.04248',
         'year': '2017'},
 'citations_link': '/scholar?cites=1222517566911879461&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDecision-Based%2BAdversarial%2BAttacks:%2BReliable%2BAttacks%2BAgainst%2BBlack-Box%2BMachine%2BLearning%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=JRl1-z9B9xAJ&ei=a8wqX4SvHYyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:JRl1-z9B9xAJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOxx5GuHUH60ypGHPdvxGonjhE4-my&scisig=AAGBfm0AAAAAXyrOx3mq4CO2dx3tDaowBVifI48j3VQx&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
319
-------------------------------------------------
2020-08-05 15:12:47
Got the results of the query
{'bib': {'abstract': 'The novel Unbiased Online Recurrent Optimization (UORO) '
                     'algorithm allows for online learning of general '
                     'recurrent computational graphs such as recurrent network '
                     'models. It works in a streaming fashion and avoids '
                     'backtracking through past activations and inputs',
         'author': ['C Tallec', 'Y Ollivier'],
         'cites': '32',
         'eprint': 'https://arxiv.org/pdf/1702.05043',
         'gsrank': '1',
         'title': 'Unbiased online recurrent optimization',
         'url': 'https://arxiv.org/abs/1702.05043',
         'venue': 'arXiv preprint arXiv:1702.05043',
         'year': '2017'},
 'citations_link': '/scholar?cites=3493841590728342658&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnbiased%2BOnline%2BRecurrent%2BOptimization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=gjzZZGqefDAJ&ei=c8wqX-vrO5qGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:gjzZZGqefDAJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrOz757tPLEUXu5891pl49xqstBVvgn&scisig=AAGBfm0AAAAAXyrOzyoax8boHRk58maHQs8SQsPjJ-ZO&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
32
-------------------------------------------------
2020-08-05 15:12:55
Got the results of the query
{'bib': {'abstract': 'Many recently trained neural networks employ large '
                     'numbers of parameters to achieve good performance. One '
                     'may intuitively use the number of parameters required as '
                     'a rough gauge of the difficulty of a problem. But how '
                     'accurate are such notions? How many parameters are '
                     'really needed? In this paper we attempt to answer this '
                     'question by training networks not in their native '
                     'parameter space, but instead in a smaller, randomly '
                     'oriented subspace. We slowly increase the dimension of '
                     'this subspace, note at which dimension solutions first',
         'author': ['C Li', 'H Farkhoor', 'R Liu', 'J Yosinski'],
         'cites': '73',
         'eprint': 'https://arxiv.org/pdf/1804.08838',
         'gsrank': '1',
         'title': 'Measuring the intrinsic dimension of objective landscapes',
         'url': 'https://arxiv.org/abs/1804.08838',
         'venue': 'arXiv preprint arXiv:1804.08838',
         'year': '2018'},
 'citations_link': '/scholar?cites=17182266159657033387&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeasuring%2Bthe%2BIntrinsic%2BDimension%2Bof%2BObjective%2BLandscapes%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=q56SYgmsc-4J&ei=fswqX7vSNKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:q56SYgmsc-4J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrO3RDOKN8xLJvjdcI0CWB8uYrdH5G0&scisig=AAGBfm0AAAAAXyrO3W-u_mwqTjzAmg0uYEHoNsbI88Cx&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
73
-------------------------------------------------
2020-08-05 15:13:09
Got the results of the query
{'bib': {'abstract': 'We propose an approach to address two issues that '
                     'commonly occur during training of unsupervised GANs. '
                     'First, since GANs use only a continuous latent '
                     'distribution to embed multiple classes or clusters of '
                     'data, they often do not correctly handle the structural '
                     'discontinuity between disparate classes in a latent '
                     'space. Second, discriminators of GANs easily forget '
                     'about past generated samples by generators, incurring '
                     'instability during adversarial training. We argue that '
                     'these two infamous problems of unsupervised GAN',
         'author': ['Y Kim', 'M Kim', 'G Kim'],
         'cites': '15',
         'eprint': 'https://arxiv.org/pdf/1803.01500',
         'gsrank': '1',
         'title': 'Memorization precedes generation: Learning unsupervised '
                  'gans with memory networks',
         'url': 'https://arxiv.org/abs/1803.01500',
         'venue': 'arXiv preprint arXiv:1803.01500',
         'year': '2018'},
 'citations_link': '/scholar?cites=7548592689214672445&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMemorization%2BPrecedes%2BGeneration:%2BLearning%2BUnsupervised%2BGANs%2Bwith%2BMemory%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=PU7Agg79wWgJ&ei=icwqX4miIojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:PU7Agg79wWgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrO5wxWZazGpqv25SMWQLSXWg1enrSU&scisig=AAGBfm0AAAAAXyrO54FsYwJEhwzUwZnaNqHYoFkg6BuZ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 15:13:20
Got the results of the query
{'bib': {'abstract': 'Neural networks are known to be vulnerable to '
                     'adversarial examples. Carefully chosen perturbations to '
                     'real images, while imperceptible to humans, induce '
                     'misclassification and threaten the reliability of deep '
                     'learning systems in the wild. To guard against '
                     'adversarial examples, we take inspiration from game '
                     'theory and cast the problem as a minimax zero-sum game '
                     'between the adversary and the model. In general, for '
                     'such games, the optimal strategy for both players '
                     'requires a stochastic policy, also known as a mixed '
                     'strategy. In this',
         'author': ['GS Dhillon', 'K Azizzadenesheli', 'ZC Lipton'],
         'cites': '203',
         'eprint': 'https://arxiv.org/pdf/1803.01442',
         'gsrank': '1',
         'title': 'Stochastic activation pruning for robust adversarial '
                  'defense',
         'url': 'https://arxiv.org/abs/1803.01442',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15156063651812856477&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStochastic%2BActivation%2BPruning%2Bfor%2BRobust%2BAdversarial%2BDefense%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ndKDT5cnVdIJ&ei=kswqX-T6EbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ndKDT5cnVdIJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrO7ybtHlMb5RiZ71FlM3N_8PeVEijy&scisig=AAGBfm0AAAAAXyrO7z-_6C0LQ59zvI4r_2FxYnjy__jS&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
203
-------------------------------------------------
2020-08-05 15:13:28
Got the results of the query
{'bib': {'abstract': 'Recurrent Neural Networks (RNNs) are powerful tools for '
                     'solving sequence-based problems, but their efficacy and '
                     'execution time are dependent on the size of the network. '
                     'Following recent work in simplifying these networks with '
                     'model pruning and a novel mapping of work onto GPUs, we '
                     'design an efficient implementation for sparse RNNs. We '
                     'investigate several optimizations and tradeoffs: Lamport '
                     'timestamps, wide memory loads, and a bank-aware weight '
                     'layout. With these optimizations, we achieve speedups of '
                     'over 6x',
         'author': ['F Zhu', 'J Pool', 'M Andersch', 'J Appleyard'],
         'cites': '16',
         'eprint': 'https://arxiv.org/pdf/1804.10223',
         'gsrank': '1',
         'title': 'Sparse persistent RNNs: Squeezing large recurrent networks '
                  'on-chip',
         'url': 'https://arxiv.org/abs/1804.10223',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=16979469535338999392&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSparse%2BPersistent%2BRNNs:%2BSqueezing%2BLarge%2BRecurrent%2BNetworks%2BOn-Chip%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=YK5y0JQxo-sJ&ei=ncwqX_u2LZqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:YK5y0JQxo-sJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrO-uKpGPSW_cmmDyKBtYKEFo3-mJtN&scisig=AAGBfm0AAAAAXyrO-ntD974BX078OYqlcsfvLyaq2SNB&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
16
-------------------------------------------------
2020-08-05 15:13:38
Got the results of the query
{'bib': {'abstract': 'Estimating individualized treatment effects (ITE) is a '
                     "challenging task due to the need for an individual's "
                     'potential outcomes to be learned from biased data and '
                     'without having access to the counterfactuals. We propose '
                     'a novel method for inferring ITE based on the Generative '
                     'Adversarial Nets (GANs) framework. Our method, termed '
                     'Generative Adversarial Nets for inference of '
                     'Individualized Treatment Effects (GANITE), is motivated '
                     'by the possibility that we can capture the uncertainty '
                     'in the counterfactual distributions by attempting to '
                     'learn them',
         'author': ['J Yoon', 'J Jordon', 'M van der Schaar'],
         'cites': '44',
         'eprint': 'https://openreview.net/pdf?id=ByKWUeWA-',
         'gsrank': '1',
         'title': 'GANITE: Estimation of individualized treatment effects '
                  'using generative adversarial nets',
         'url': 'https://openreview.net/forum?id=ByKWUeWA-',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=880825106986572029&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGANITE:%2BEstimation%2Bof%2BIndividualized%2BTreatment%2BEffects%2Busing%2BGenerative%2BAdversarial%2BNets%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=_Rgum8RROQwJ&ei=qMwqX_PgFaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:_Rgum8RROQwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrPBln3_NQyL9pG67ph0M77L29VmaMt&scisig=AAGBfm0AAAAAXyrPBh0xMXXkhkWgd_fNAoteCzMc6UBU&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
44
-------------------------------------------------
2020-08-05 15:13:50
Got the results of the query
{'bib': {'abstract': 'It is well known that it is possible to construct" '
                     'adversarial examples" for neural networks: inputs which '
                     'are misclassified by the network yet indistinguishable '
                     'from true data. We propose a simple modification to '
                     'standard neural network architectures, thermometer '
                     'encoding, which significantly increases the robustness '
                     'of the network to adversarial examples. We demonstrate '
                     'this robustness with experiments on the MNIST, CIFAR-10, '
                     'CIFAR-100, and SVHN datasets, and show that models with '
                     'thermometer-encoded inputs',
         'author': ['J Buckman', 'A Roy', 'C Raffel'],
         'cites': '185',
         'eprint': 'https://openreview.net/pdf?id=S18Su--CW',
         'gsrank': '1',
         'title': 'Thermometer encoding: One hot way to resist adversarial '
                  'examples',
         'url': 'https://openreview.net/forum?id=H1SBu-WRW',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=14437133120740920933&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DThermometer%2BEncoding:%2BOne%2BHot%2BWay%2BTo%2BResist%2BAdversarial%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ZQ7KBBUAW8gJ&ei=scwqX_yJCojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ZQ7KBBUAW8gJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrPGE8GoLbEmldgcvlrxHQjxVGI9xXt&scisig=AAGBfm0AAAAAXyrPGDyqZXNFO_qwBeqy5n9AMymHh2XN&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
185
-------------------------------------------------
2020-08-05 15:14:08
Got the results of the query
{'bib': {'abstract': 'Trust region methods, such as TRPO, are often used to '
                     'stabilize policy optimization algorithms in '
                     'reinforcement learning (RL). While current trust region '
                     'strategies are effective for continuous control, they '
                     'typically require a prohibitively large amount of '
                     'on-policy interaction with the environment. To address '
                     'this problem, we propose an off-policy trust region '
                     'method, Trust-PCL. The algorithm is the result of '
                     'observing that the optimal policy and state values of a '
                     'maximum reward objective with a relative-entropy '
                     'regularizer satisfy a',
         'author': ['O Nachum', 'M Norouzi', 'K Xu', 'D Schuurmans'],
         'cites': '60',
         'eprint': 'https://arxiv.org/pdf/1707.01891',
         'gsrank': '1',
         'title': 'Trust-pcl: An off-policy trust region method for continuous '
                  'control',
         'url': 'https://arxiv.org/abs/1707.01891',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=11034633680493566157&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTrust-PCL:%2BAn%2BOff-Policy%2BTrust%2BRegion%2BMethod%2Bfor%2BContinuous%2BControl%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=zVhhUN7kIpkJ&ei=xswqX8TKIYjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:zVhhUN7kIpkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrPIzRaPHFy_GpZ0ywliashaoOFEiLf&scisig=AAGBfm0AAAAAXyrPIyR02VOp5VHDd1vM89CGUx8oooNQ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
60
-------------------------------------------------
2020-08-05 15:14:19
Got the results of the query
{'bib': {'abstract': 'Predicting the future in real-world settings, '
                     'particularly from raw sensory observations such as '
                     'images, is exceptionally challenging. Real-world events '
                     'can be stochastic and unpredictable, and the high '
                     'dimensionality and complexity of natural images requires '
                     'the',
         'author': ['M Babaeizadeh', 'C Finn', 'D Erhan', 'RH Campbell'],
         'cites': '186',
         'eprint': 'https://arxiv.org/pdf/1710.11252',
         'gsrank': '1',
         'title': 'Stochastic variational video prediction',
         'url': 'https://arxiv.org/abs/1710.11252',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=16282826800103721009&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DStochastic%2BVariational%2BVideo%2BPrediction%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=MYhkX7Q4-OEJ&ei=z8wqX9K9C4vrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:MYhkX7Q4-OEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrPKh8wABjE3SlN8SlITDBBHe_L64Qn&scisig=AAGBfm0AAAAAXyrPKgxMBIWb1w1RENl44Gccrf_gN9Qw&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
186
-------------------------------------------------
2020-08-05 15:14:27
Got the results of the query
{'bib': {'abstract': 'Motivated by recent work on deep neural network '
                     '(DNN)-based image compression methods showing potential '
                     'improvements in image quality, savings in storage, and '
                     'bandwidth reduction, we propose to perform image '
                     'understanding tasks such as classification and '
                     'segmentation directly on the compressed representations '
                     'produced by these compression methods. Since the '
                     'encoders and decoders in DNN-based compression methods '
                     'are neural networks with feature-maps as internal '
                     'representations of the images',
         'author': ['R Torfason', 'F Mentzer', 'E Agustsson'],
         'cites': '32',
         'eprint': 'https://arxiv.org/pdf/1803.06131',
         'gsrank': '1',
         'title': 'Towards image understanding from deep compression without '
                  'decoding',
         'url': 'https://arxiv.org/abs/1803.06131',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=10655381109239631918&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BImage%2BUnderstanding%2Bfrom%2BDeep%2BCompression%2BWithout%2BDecoding%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=LtCiCqqE35MJ&ei=2MwqX4bTJ8KwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:LtCiCqqE35MJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrPNT2Jp8d4139ptbsTNYkr7acCNqTM&scisig=AAGBfm0AAAAAXyrPNZmTypEsHRZPsasf-XQrQNJhVgO9&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
32
-------------------------------------------------
2020-08-05 15:14:37
Got the results of the query
{'bib': {'abstract': 'Spatiotemporal forecasting has become an increasingly '
                     'important prediction task in machine learning and '
                     'statistics due to its vast applications, such as climate '
                     'modeling, traffic prediction, video caching predictions, '
                     'and so on. While numerous studies have been conducted, '
                     'most existing works assume that the data from different '
                     'sources or across different locations are equally '
                     'reliable. Due to cost, accessibility, or other factors, '
                     'it is inevitable that the data quality could vary, which '
                     'introduces significant biases into the model',
         'author': ['S Seo', 'A Mohegh', 'G Ban-Weiss', 'Y Liu'],
         'cites': '0',
         'eprint': 'https://openreview.net/pdf?id=ByJIWUnpW',
         'gsrank': '1',
         'title': 'Automatically Inferring Data Quality for Spatiotemporal '
                  'Forecasting',
         'url': 'https://openreview.net/forum?id=ByJIWUnpW&noteId=ByJIWUnpW',
         'venue': 'International Conference on …',
         'year': '2018'},
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAutomatically%2BInferring%2BData%2BQuality%2Bfor%2BSpatiotemporal%2BForecasting%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=68-xPC5Y4KYJ&ei=4cwqX4iiGoyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:68-xPC5Y4KYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrPPhk03FR0SPcUkJELVa6-hUBCHffw&scisig=AAGBfm0AAAAAXyrPPqbECJoPlGKc6DGgyItrFYHA004c&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
0
-------------------------------------------------
2020-08-05 15:14:46
Got the results of the query
{'bib': {'abstract': 'Understanding the flow of information in Deep Neural '
                     'Networks (DNNs) is a challenging problem that has gain '
                     'increasing attention over the last few years. While '
                     'several methods have been proposed to explain network '
                     'predictions, there have been only a few attempts to',
         'author': ['M Ancona', 'E Ceolini', 'C Öztireli', 'M Gross'],
         'cites': '244',
         'eprint': 'https://arxiv.org/pdf/1711.06104',
         'gsrank': '1',
         'title': 'Towards better understanding of gradient-based attribution '
                  'methods for deep neural networks',
         'url': 'https://arxiv.org/abs/1711.06104',
         'venue': 'arXiv preprint arXiv:1711.06104',
         'year': '2017'},
 'citations_link': '/scholar?cites=7129422820232184089&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2Bbetter%2Bunderstanding%2Bof%2Bgradient-based%2Battribution%2Bmethods%2Bfor%2BDeep%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=GcGAHUjM8GIJ&ei=68wqX-vFCMKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:GcGAHUjM8GIJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrPV36gKWUh8xadXiTclVl9z7j5Oz4Y&scisig=AAGBfm0AAAAAXyrPV4TC6oH4e9o0ys8qrao18nG-ZqaZ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
244
-------------------------------------------------
2020-08-05 15:15:12
Got the results of the query
{'bib': {'abstract': 'This paper investigates strategies that defend against '
                     'adversarial-example attacks on image-classification '
                     'systems by transforming the inputs before feeding them '
                     'to the system. Specifically, we study applying image '
                     'transformations such as bit-depth reduction, JPEG',
         'author': ['C Guo', 'M Rana', 'M Cisse', 'L Van Der Maaten'],
         'cites': '417',
         'eprint': 'https://arxiv.org/pdf/1711.00117',
         'gsrank': '1',
         'title': 'Countering adversarial images using input transformations',
         'url': 'https://arxiv.org/abs/1711.00117',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=3375700876994648267&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCountering%2BAdversarial%2BImages%2Busing%2BInput%2BTransformations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=y5g9-Q_m2C4J&ei=Bs0qX7L1HbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:y5g9-Q_m2C4J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrPY-RgGREEfNMAoRlX3ywoF7E32mvR&scisig=AAGBfm0AAAAAXyrPY59g8oVKXecDkkq85F0URm_IIlMk&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
417
-------------------------------------------------
2020-08-05 15:15:23
Got the results of the query
{'bib': {'abstract': 'Recurrent Neural Networks (RNNs) continue to show '
                     'outstanding performance in sequence modeling tasks. '
                     'However, training RNNs on long sequences often face '
                     'challenges like slow inference, vanishing gradients and '
                     'difficulty in capturing long term dependencies. In '
                     'backpropagation through time settings, these issues are '
                     'tightly coupled with the large, sequential computational '
                     'graph resulting from unfolding the RNN in time. We '
                     'introduce the Skip RNN model which extends existing RNN '
                     'models by learning to skip state updates and',
         'author': ['V Campos', 'B Jou', 'X Giró-i-Nieto', 'J Torres'],
         'cites': '105',
         'eprint': 'https://arxiv.org/pdf/1708.06834',
         'gsrank': '1',
         'title': 'Skip rnn: Learning to skip state updates in recurrent '
                  'neural networks',
         'url': 'https://arxiv.org/abs/1708.06834',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=4452574796134429216&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSkip%2BRNN:%2BLearning%2Bto%2BSkip%2BState%2BUpdates%2Bin%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=IDJzsy-5yj0J&ei=EM0qX_T9CIjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:IDJzsy-5yj0J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrPa5sJpj-AwDDilSxo7d9XyGlegyhc&scisig=AAGBfm0AAAAAXyrPaxd6Vhj6ijIjOaD0J3E78FWR3_QK&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
105
-------------------------------------------------
2020-08-05 15:15:32
Got the results of the query
{'bib': {'abstract': 'A core aspect of human intelligence is the ability to '
                     'learn new tasks quickly and switch between them '
                     'flexibly. Here, we describe a modular continual '
                     'reinforcement learning paradigm inspired by these '
                     'abilities. We first introduce a visual interaction '
                     'environment that allows many types of tasks to be '
                     'unified in a single framework. We then describe a reward '
                     'map prediction scheme that learns new tasks robustly in '
                     'the very large state and action spaces required by such '
                     'an environment. We investigate how properties of module',
         'author': ['KT Feigelis', 'B Sheffer', 'DLK Yamins'],
         'cites': '1',
         'eprint': 'https://arxiv.org/pdf/1711.07425',
         'gsrank': '1',
         'title': 'Modular continual learning in a unified visual environment',
         'url': 'https://arxiv.org/abs/1711.07425',
         'venue': 'arXiv preprint arXiv:1711.07425',
         'year': '2017'},
 'citations_link': '/scholar?cites=18112736468437527039&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DModular%2BContinual%2BLearning%2Bin%2Ba%2BUnified%2BVisual%2BEnvironment%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=_2l7R9pdXfsJ&ei=Gs0qX8uxE4yimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:_2l7R9pdXfsJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrPeCQdzZW8Dbc_W-I3_5pfqw3wlIx7&scisig=AAGBfm0AAAAAXyrPeDEnKaQiD6RvJ32RZFVyAi5WqZpL&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
1
-------------------------------------------------
2020-08-05 15:15:44
Got the results of the query
{'bib': {'abstract': 'We propose a simple technique for encouraging generative '
                     'RNNs to plan ahead. We train a" backward" recurrent '
                     'network to generate a given sequence in reverse order, '
                     'and we encourage states of the forward model to predict '
                     'cotemporal states of the backward model. The backward '
                     'network is used only during training, and plays no role '
                     'during sampling or inference. We hypothesize that our '
                     'approach eases modeling of long-term dependencies by '
                     'implicitly forcing the forward states to hold '
                     'information about the longer-term future (as',
         'author': ['D Serdyuk', 'NR Ke', 'A Sordoni', 'A Trischler', 'C Pal'],
         'cites': '31',
         'eprint': 'https://arxiv.org/pdf/1708.06742',
         'gsrank': '1',
         'title': 'Twin networks: Matching the future for sequence generation',
         'url': 'https://arxiv.org/abs/1708.06742',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=18040787837429694230&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTwin%2BNetworks:%2BMatching%2Bthe%2BFuture%2Bfor%2BSequence%2BGeneration%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Fj8vNvTAXfoJ&ei=Js0qX6_gE6iBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Fj8vNvTAXfoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrPg2rY0W3sXdVwd-dcKmih9stw88uf&scisig=AAGBfm0AAAAAXyrPg6WbXoqjukRPumrekAVMCy7rKgop&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
31
-------------------------------------------------
2020-08-05 15:15:56
Got the results of the query
{'bib': {'abstract': 'Questions that require counting a variety of objects in '
                     'images remain a major challenge in visual question '
                     'answering (VQA). The most common approaches to VQA '
                     'involve either classifying answers based on fixed length '
                     'representations of both the image and question or '
                     'summing fractional counts estimated from each section of '
                     'the image. In contrast, we treat counting as a '
                     'sequential decision process and force our model to make '
                     'discrete choices of what to count. Specifically, the '
                     'model sequentially selects from detected objects and '
                     'learns',
         'author': ['A Trott', 'C Xiong', 'R Socher'],
         'cites': '29',
         'eprint': 'https://arxiv.org/pdf/1712.08697',
         'gsrank': '1',
         'title': 'Interpretable counting for visual question answering',
         'url': 'https://arxiv.org/abs/1712.08697',
         'venue': 'arXiv preprint arXiv:1712.08697',
         'year': '2017'},
 'citations_link': '/scholar?cites=14039637797688698399&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInterpretable%2BCounting%2Bfor%2BVisual%2BQuestion%2BAnswering%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=H-ZhGjHQ1sIJ&ei=Ms0qX6mnFZqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:H-ZhGjHQ1sIJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrPjDI5oo056S-qdksInRcNLnb3xwbo&scisig=AAGBfm0AAAAAXyrPjGAgYTPbaHpunCDXNikdwsAa9Llf&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
29
-------------------------------------------------
2020-08-05 15:16:04
Got the results of the query
{'bib': {'abstract': 'We build a virtual agent for learning language in a 2D '
                     'maze-like world. The agent sees images of the '
                     'surrounding environment, listens to a virtual teacher, '
                     'and takes actions to receive rewards. It interactively '
                     "learns the teacher's language from scratch based on two "
                     'language use cases: sentence-directed navigation and '
                     'question answering. It learns simultaneously the visual '
                     'representations of the world, the language, and the '
                     'action control. By disentangling language grounding from '
                     'other computational routines and sharing a',
         'author': ['H Yu', 'H Zhang', 'W Xu'],
         'cites': '34',
         'eprint': 'https://arxiv.org/pdf/1802.01433',
         'gsrank': '1',
         'title': 'Interactive grounded language acquisition and '
                  'generalization in a 2d world',
         'url': 'https://arxiv.org/abs/1802.01433',
         'venue': 'arXiv preprint arXiv:1802.01433',
         'year': '2018'},
 'citations_link': '/scholar?cites=4696587271474463712&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInteractive%2BGrounded%2BLanguage%2BAcquisition%2Band%2BGeneralization%2Bin%2Ba%2B2D%2BWorld%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=4EPbZD2hLUEJ&ei=OM0qX_P5OojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:4EPbZD2hLUEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrPlEBwX4EFPS332zy9eORJmf9KUD2D&scisig=AAGBfm0AAAAAXyrPlDWeS4ruXk1AWFqDCslPmS1qTMTf&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
34
-------------------------------------------------
2020-08-05 15:16:12
Got the results of the query
{'bib': {'abstract': 'Reinforcement learning algorithms can train agents that '
                     'solve problems in complex, interesting environments. '
                     'Normally, the complexity of the trained agent is closely '
                     'related to the complexity of the environment. This '
                     'suggests that a highly capable agent requires a complex '
                     'environment for training. In this paper, we point out '
                     'that a competitive multi-agent environment trained with '
                     'self-play can produce behaviors that are far more '
                     'complex than the environment itself. We also point out '
                     'that such environments come with a natural curriculum',
         'author': ['T Bansal', 'J Pachocki', 'S Sidor', 'I Sutskever'],
         'cites': '164',
         'eprint': 'https://arxiv.org/pdf/1710.03748.pdf%3Cp%3EKEYWORDS:&nbsp;Artificial',
         'gsrank': '1',
         'title': 'Emergent complexity via multi-agent competition',
         'url': 'https://arxiv.org/abs/1710.03748',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=12865596457557919071&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergent%2BComplexity%2Bvia%2BMulti-Agent%2BCompetition%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=X4VDNNDHi7IJ&ei=Qs0qX--fEpqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:X4VDNNDHi7IJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrPoKtgbP3YDbH6Awu15y4I3VUT7OHk&scisig=AAGBfm0AAAAAXyrPoB5mM1b1Z6r6HD9C5-I5Pdm2Tngi&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
164
-------------------------------------------------
2020-08-05 15:16:24
Got the results of the query
{'bib': {'abstract': 'Recent state-of-the-art reinforcement learning '
                     'algorithms are trained under the goal of excelling in '
                     'one specific task. Hence, both environment and task '
                     'specific knowledge are entangled into one framework. '
                     'However, there are often scenarios where the environment '
                     '(eg the physical world) is fixed while only the target '
                     'task changes. Hence, borrowing the idea from '
                     'hierarchical reinforcement learning, we propose a '
                     'framework that disentangles task and environment '
                     'specific knowledge by separating them into two units. '
                     'The',
         'author': ['J Mao', 'H Dong', 'JJ Lim'],
         'cites': '4',
         'eprint': 'https://openreview.net/pdf?id=B1mvVm-C-',
         'gsrank': '1',
         'title': 'Universal agent for disentangling environments and tasks',
         'url': 'https://openreview.net/forum?id=B1mvVm-C-&noteId=B1mvVm-C-',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=15599654995562798125&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUniversal%2BAgent%2Bfor%2BDisentangling%2BEnvironments%2Band%2BTasks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=LRwrZZEbfdgJ&ei=Tc0qX_S1EpqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:LRwrZZEbfdgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrPthCk6l7Ai8LxqexuaNPlE4r5riky&scisig=AAGBfm0AAAAAXyrPtr-9hj_v7AqXPfVTpqeNFMqRd30J&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
4
-------------------------------------------------
2020-08-05 15:16:47
Got the results of the query
{'bib': {'abstract': 'Residual networks (Resnets) have become a prominent '
                     'architecture in deep learning. However, a comprehensive '
                     'understanding of Resnets is still a topic of ongoing '
                     'research. A recent view argues that Resnets perform '
                     'iterative refinement of features. We attempt to',
         'author': ['S Jastrzębski', 'D Arpit', 'N Ballas', 'V Verma', 'T Che'],
         'cites': '47',
         'eprint': 'https://arxiv.org/pdf/1710.04773',
         'gsrank': '1',
         'title': 'Residual connections encourage iterative inference',
         'url': 'https://arxiv.org/abs/1710.04773',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=14453729681594903284&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DResidual%2BConnections%2BEncourage%2BIterative%2BInference%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=9Or79pD2lcgJ&ei=Zs0qX4zwBqiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:9Or79pD2lcgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrPykYPmH8Erm4tzCoYQwXHiTKV_6C4&scisig=AAGBfm0AAAAAXyrPyrsDUnyiSrX0YcKATqeepURTlHNP&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
47
-------------------------------------------------
2020-08-05 15:17:06
Got the results of the query
{'bib': {'abstract': 'Multi-agent reinforcement learning offers a way to study '
                     'how communication could emerge in communities of agents '
                     'needing to solve specific problems. In this paper, we '
                     'study the emergence of communication in the negotiation '
                     'environment, a semi-cooperative model of agent '
                     'interaction. We introduce two communication '
                     'protocols--one grounded in the semantics of the game, '
                     'and one which is\\textit {a priori} ungrounded and is a '
                     'form of cheap talk. We show that self-interested agents '
                     'can use the pre-grounded communication channel',
         'author': ['K Cao', 'A Lazaridou', 'M Lanctot', 'JZ Leibo'],
         'cites': '58',
         'eprint': 'https://arxiv.org/pdf/1804.03980',
         'gsrank': '1',
         'title': 'Emergent communication through negotiation',
         'url': 'https://arxiv.org/abs/1804.03980',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=8825869866742501521&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergent%2BCommunication%2Bthrough%2BNegotiation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=kdAC4eHJe3oJ&ei=es0qX5i8CIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:kdAC4eHJe3oJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrP11DH-eiLdjPCRIoEkDQZel8K04Np&scisig=AAGBfm0AAAAAXyrP1_2bc1ldVDugexJ_Xm7yBXBPk8Ak&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
58
-------------------------------------------------
2020-08-05 15:17:19
Got the results of the query
{'bib': {'abstract': 'We introduce a new memory architecture for navigation in '
                     'previously unseen environments, inspired by '
                     'landmark-based navigation in animals. The proposed '
                     'semi-parametric topological memory (SPTM) consists of a '
                     '(non-parametric) graph with nodes corresponding to '
                     'locations in the environment and a (parametric) deep '
                     'network capable of retrieving nodes from the graph based '
                     'on observations. The graph stores no metric information, '
                     'only connectivity of locations corresponding to the '
                     'nodes. We use SPTM as a planning module in',
         'author': ['N Savinov', 'A Dosovitskiy', 'V Koltun'],
         'cites': '97',
         'eprint': 'https://arxiv.org/pdf/1803.00653',
         'gsrank': '1',
         'title': 'Semi-parametric topological memory for navigation',
         'url': 'https://arxiv.org/abs/1803.00653',
         'venue': 'arXiv preprint arXiv:1803.00653',
         'year': '2018'},
 'citations_link': '/scholar?cites=10804671962093103166&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSemi-parametric%2Btopological%2Bmemory%2Bfor%2Bnavigation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=PqguZ-fn8ZUJ&ei=g80qX5brG5qGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:PqguZ-fn8ZUJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrP4vBbQ6BUBYiuinXvQHOd0RZWRXMs&scisig=AAGBfm0AAAAAXyrP4myJCtt3EUMXjCsUYNK1NQKm-xpC&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
97
-------------------------------------------------
2020-08-05 15:17:30
Got the results of the query
{'bib': {'abstract': 'Visual Question Answering (VQA) models have struggled '
                     'with counting objects in natural images so far. We '
                     'identify a fundamental problem due to soft attention in '
                     'these models as a cause. To circumvent this problem, we '
                     'propose a neural network component that allows robust '
                     'counting from object proposals. Experiments on a toy '
                     'task show the effectiveness of this component and we '
                     'obtain state-of-the-art accuracy on the number category '
                     'of the VQA v2 dataset without negatively affecting other '
                     'categories, even outperforming ensemble',
         'author': ['Y Zhang', 'J Hare', 'A Prügel-Bennett'],
         'cites': '79',
         'eprint': 'https://arxiv.org/pdf/1802.05766',
         'gsrank': '1',
         'title': 'Learning to count objects in natural images for visual '
                  'question answering',
         'url': 'https://arxiv.org/abs/1802.05766',
         'venue': 'arXiv preprint arXiv:1802.05766',
         'year': '2018'},
 'citations_link': '/scholar?cites=5291501502665174038&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2BCount%2BObjects%2Bin%2BNatural%2BImages%2Bfor%2BVisual%2BQuestion%2BAnswering%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=FqzNwpQwb0kJ&ei=kM0qX87COcKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:FqzNwpQwb0kJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrP7HFk5MMbhi1fApN6wFQXzl8pCxtR&scisig=AAGBfm0AAAAAXyrP7JqQgYlOVmYqt7U16bfJ1dRMz3Y4&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
79
-------------------------------------------------
2020-08-05 15:17:40
Got the results of the query
{'bib': {'abstract': 'It is widely believed that the success of deep '
                     'convolutional networks is based on progressively '
                     'discarding uninformative variability about the input '
                     'with respect to the problem at hand. This is supported '
                     'empirically by the difficulty of recovering images from '
                     'their hidden',
         'author': ['JH Jacobsen', 'A Smeulders', 'E Oyallon'],
         'cites': '127',
         'eprint': 'https://arxiv.org/pdf/1802.07088',
         'gsrank': '1',
         'title': 'i-revnet: Deep invertible networks',
         'url': 'https://arxiv.org/abs/1802.07088',
         'venue': 'arXiv preprint arXiv:1802.07088',
         'year': '2018'},
 'citations_link': '/scholar?cites=14608880224467079528&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3Di-RevNet:%2BDeep%2BInvertible%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=aNGWuSkrvcoJ&ei=mM0qX978KbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:aNGWuSkrvcoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrP9hGA8PLyxnyo0Y4ray607xWS4UFy&scisig=AAGBfm0AAAAAXyrP9unL3mybTz4ize0nYaq7vQkOUgw8&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
127
-------------------------------------------------
2020-08-05 15:17:50
Got the results of the query
{'bib': {'abstract': 'The robustness of neural networks to adversarial '
                     'examples has received great attention due to security '
                     'implications. Despite various attack approaches to '
                     'crafting visually imperceptible adversarial examples, '
                     'little has been developed towards a comprehensive '
                     'measure of robustness. In this paper, we provide a '
                     'theoretical justification for converting robustness '
                     'analysis into a local Lipschitz constant estimation '
                     'problem, and propose to use the Extreme Value Theory for '
                     'efficient evaluation. Our analysis yields a novel '
                     'robustness metric called',
         'author': ['TW Weng', 'H Zhang', 'PY Chen', 'J Yi', 'D Su', 'Y Gao'],
         'cites': '104',
         'eprint': 'https://arxiv.org/pdf/1801.10578',
         'gsrank': '1',
         'title': 'Evaluating the robustness of neural networks: An extreme '
                  'value theory approach',
         'url': 'https://arxiv.org/abs/1801.10578',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=2078120094241692942&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEvaluating%2Bthe%2BRobustness%2Bof%2BNeural%2BNetworks:%2BAn%2BExtreme%2BValue%2BTheory%2BApproach%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Dp3gITf31hwJ&ei=pM0qX53rAYjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Dp3gITf31hwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQASJuXDR0n_Osv4BSZggrWdlhKClm&scisig=AAGBfm0AAAAAXyrQATpGMYaCCl21Xa81u8l9Cd5ohPOl&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
104
-------------------------------------------------
2020-08-05 15:18:01
Got the results of the query
{'bib': {'abstract': 'The effectiveness of Convolutional Neural Networks stems '
                     'in large part from their ability to exploit the '
                     'translation invariance that is inherent in many learning '
                     'problems. Recently, it was shown that CNNs can exploit '
                     'other invariances, such as rotation invariance, by using '
                     'group',
         'author': ['E Hoogeboom', 'JWT Peters', 'TS Cohen'],
         'cites': '26',
         'eprint': 'https://arxiv.org/pdf/1803.02108',
         'gsrank': '1',
         'title': 'Hexaconv',
         'url': 'https://arxiv.org/abs/1803.02108',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=3503620825946735449&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHexaConv%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=WUM3BpRcnzAJ&ei=rM0qX829HIjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:WUM3BpRcnzAJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQCOFEGT7A6rwcKU1MS-WzQGNZnK1S&scisig=AAGBfm0AAAAAXyrQCFGAt9oSWnA9FqDmukNF9JbK85Ms&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
26
-------------------------------------------------
2020-08-05 15:18:08
Got the results of the query
{'bib': {'abstract': 'Recent work has demonstrated that deep neural networks '
                     'are vulnerable to adversarial examples---inputs that are '
                     'almost indistinguishable from natural data and yet '
                     'classified incorrectly by the network. In fact, some of '
                     'the latest findings suggest that the existence of '
                     'adversarial attacks may be an inherent weakness of deep '
                     'learning models. To address this problem, we study the '
                     'adversarial robustness of neural networks through the '
                     'lens of robust optimization. This approach provides us '
                     'with a broad and unifying view on much of the prior',
         'author': ['A Madry', 'A Makelov', 'L Schmidt', 'D Tsipras'],
         'cites': '1947',
         'eprint': 'https://arxiv.org/pdf/1706.06083)',
         'gsrank': '1',
         'title': 'Towards deep learning models resistant to adversarial '
                  'attacks',
         'url': 'https://arxiv.org/abs/1706.06083',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=14165082781627851489&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BDeep%2BLearning%2BModels%2BResistant%2Bto%2BAdversarial%2BAttacks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=4SLudL17lMQJ&ei=tc0qX8OFGqOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:4SLudL17lMQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQEWkcmjzcQZcN9XXzVfJeGKtdQscS&scisig=AAGBfm0AAAAAXyrQEWTz2cIUu3eSmg0GMlPCQy1vfdKp&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
1947
-------------------------------------------------
2020-08-05 15:18:17
Got the results of the query
{'bib': {'abstract': 'We consider the use of deep learning methods for '
                     'modeling complex phenomena like those occurring in '
                     'natural physical processes. With the large amount of '
                     'data gathered on these phenomena the data intensive '
                     'paradigm could begin to challenge more traditional '
                     'approaches elaborated over the years in fields like '
                     'maths or physics. However, despite considerable '
                     'successes in a variety of application domains, the '
                     'machine learning field is not yet ready to handle the '
                     'level of complexity required by such problems. Using an '
                     'example',
         'author': ['E de Bezenac', 'A Pajot', 'P Gallinari'],
         'cites': '67',
         'eprint': 'https://arxiv.org/pdf/1711.07970',
         'gsrank': '1',
         'title': 'Deep learning for physical processes: Incorporating prior '
                  'scientific knowledge',
         'url': 'https://iopscience.iop.org/article/10.1088/1742-5468/ab3195/meta',
         'venue': 'Journal of Statistical …',
         'year': '2019'},
 'citations_link': '/scholar?cites=339008717685681020&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BLearning%2Bfor%2BPhysical%2BProcesses:%2BIncorporating%2BPrior%2BScientific%2BKnowledge%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=fGtq8aFmtAQJ&ei=vM0qX5jPBIjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:fGtq8aFmtAQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQGcyvTP6_2fQ3IXpE8QrzyA-ssQdr&scisig=AAGBfm0AAAAAXyrQGTjlseNLOcn84OFgsziWqB60prt1&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
67
-------------------------------------------------
2020-08-05 15:18:25
Got the results of the query
{'bib': {'abstract': 'Coding theory is a central discipline underpinning '
                     'wireline and wireless modems that are the workhorses of '
                     'the information age. Progress in coding theory is '
                     'largely driven by individual human ingenuity with '
                     'sporadic breakthroughs over the past century. In this '
                     'paper we study whether it is possible to automate the '
                     'discovery of decoding algorithms via deep learning. We '
                     'study a family of sequential codes parameterized by '
                     'recurrent neural network (RNN) architectures. We show '
                     'that creatively designed and trained RNN architectures '
                     'can',
         'author': ['H Kim', 'Y Jiang', 'R Rana', 'S Kannan', 'S Oh'],
         'cites': '103',
         'eprint': 'https://arxiv.org/pdf/1805.09317',
         'gsrank': '1',
         'title': 'Communication algorithms via deep learning',
         'url': 'https://arxiv.org/abs/1805.09317',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=3745511757842142493&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCommunication%2BAlgorithms%2Bvia%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=HdU6ZBm7-jMJ&ei=xM0qX_HMMY-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:HdU6ZBm7-jMJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQJ83xaorP4b2Twsr5AbyTfYBQHAze&scisig=AAGBfm0AAAAAXyrQJ3UGHRlGCwZ3bRXLDMAT4OB27tQA&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
103
-------------------------------------------------
2020-08-05 15:18:39
Got the results of the query
{'bib': {'abstract': 'Understanding procedural language requires anticipating '
                     'the causal effects of actions, even when they are not '
                     'explicitly stated. In this work, we introduce Neural '
                     'Process Networks to understand procedural text through '
                     '(neural) simulation of action dynamics. Our model '
                     'complements existing memory architectures with dynamic '
                     'entity tracking by explicitly modeling actions as state '
                     'transformers. The model updates the states of the '
                     'entities by executing learned action operators. '
                     'Empirical results demonstrate that our proposed model',
         'author': ['A Bosselut', 'O Levy', 'A Holtzman', 'C Ennis', 'D Fox'],
         'cites': '46',
         'eprint': 'https://arxiv.org/pdf/1711.05313',
         'gsrank': '1',
         'title': 'Simulating action dynamics with neural process networks',
         'url': 'https://arxiv.org/abs/1711.05313',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=15481167973154467970&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSimulating%2BAction%2BDynamics%2Bwith%2BNeural%2BProcess%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=grAGtD8o2NYJ&ei=080qX8fXJ5qGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:grAGtD8o2NYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQLtM437MOW61Gq9qgmeMfYZ_aSVu9&scisig=AAGBfm0AAAAAXyrQLidkKYuR1al8GuAERHEsrXDZBeUG&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
46
-------------------------------------------------
2020-08-05 15:18:46
Got the results of the query
{'bib': {'abstract': 'This work details CipherGAN, an architecture inspired by '
                     'CycleGAN used for inferring the underlying cipher '
                     'mapping given banks of unpaired ciphertext and '
                     'plaintext. We demonstrate that CipherGAN is capable of '
                     'cracking language data enciphered using shift and '
                     'Vigenere ciphers to a high degree of fidelity and for '
                     'vocabularies much larger than previously achieved. We '
                     'present how CycleGAN can be made compatible with '
                     'discrete data and train in a stable way. We then prove '
                     'that the technique used in CipherGAN avoids the',
         'author': ['AN Gomez', 'S Huang', 'I Zhang', 'BM Li', 'M Osama'],
         'cites': '28',
         'eprint': 'https://arxiv.org/pdf/1801.04883',
         'gsrank': '1',
         'title': 'Unsupervised cipher cracking using discrete gans',
         'url': 'https://arxiv.org/abs/1801.04883',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=3064134608179971225&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DUnsupervised%2BCipher%2BCracking%2BUsing%2BDiscrete%2BGANs%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=mUSMEDH-hSoJ&ei=3M0qX8ycGo-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:mUSMEDH-hSoJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQOmdR6txeM9K11LVK9ErDDp0Cn6lr&scisig=AAGBfm0AAAAAXyrQOvxUzBD2RYvSz2zEqWV242JvvHhz&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
28
-------------------------------------------------
2020-08-05 15:18:59
Got the results of the query
{'bib': {'abstract': 'Inspired by the principles of speed reading, we '
                     'introduce Skim-RNN, a recurrent neural network (RNN) '
                     'that dynamically decides to update only a small fraction '
                     'of the hidden state for relatively unimportant input '
                     'tokens. Skim-RNN gives computational advantage over an',
         'author': ['M Seo', 'S Min', 'A Farhadi', 'H Hajishirzi'],
         'cites': '15',
         'eprint': 'https://arxiv.org/pdf/1711.02085',
         'gsrank': '1',
         'title': 'Neural speed reading via skim-rnn',
         'url': 'https://arxiv.org/abs/1711.02085',
         'venue': 'arXiv preprint arXiv:1711.02085',
         'year': '2017'},
 'citations_link': '/scholar?cites=8873138418966688907&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNeural%2BSpeed%2BReading%2Bvia%2BSkim-RNN%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=i3RhUWG4I3sJ&ei=5s0qX5u-KJqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:i3RhUWG4I3sJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQRSyAwAAdfAFsG7Hw4h5H2LgXT7xB&scisig=AAGBfm0AAAAAXyrQRVL6-i_ikAdbCW9AwMNlCXltfNfl&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 15:19:09
Got the results of the query
{'bib': {'abstract': 'Deep residual networks (ResNets) and their variants are '
                     'widely used in many computer vision applications and '
                     'natural language processing tasks. However, the '
                     'theoretical principles for designing and training '
                     'ResNets are still not fully understood. Recently, '
                     'several points of view have emerged to try to interpret '
                     'ResNet theoretically, such as unraveled view, unrolled '
                     'iterative estimation and dynamical systems view. In this '
                     'paper, we adopt the dynamical systems point of view, and '
                     'analyze the lesioning properties of ResNet both',
         'author': ['B Chang', 'L Meng', 'E Haber', 'F Tung'],
         'cites': '71',
         'eprint': 'https://arxiv.org/pdf/1710.10348',
         'gsrank': '1',
         'title': 'Multi-level residual networks from dynamical systems view',
         'url': 'https://arxiv.org/abs/1710.10348',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=13446938182344461290&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMulti-level%2BResidual%2BNetworks%2Bfrom%2BDynamical%2BSystems%2BView%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=6ktHKwgfnboJ&ei=8s0qX9mgHKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:6ktHKwgfnboJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQUJvx6GWEoVl8IC7HUjiLw6IRmRdG&scisig=AAGBfm0AAAAAXyrQUDOyhg4UOCerSynCOiFb5vqrchi0&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
71
-------------------------------------------------
2020-08-05 15:19:20
Got the results of the query
{'bib': {'abstract': 'In this paper, we present Neural Phrase-based Machine '
                     'Translation (NPMT). Our method explicitly models the '
                     'phrase structures in output sequences using Sleep-WAke '
                     'Networks (SWAN), a recently proposed segmentation-based '
                     'sequence modeling method. To mitigate the monotonic '
                     'alignment requirement of SWAN, we introduce a new layer '
                     'to perform (soft) local reordering of input sequences. '
                     'Different from existing neural machine translation (NMT) '
                     'approaches, NPMT does not use attention-based decoding '
                     'mechanisms. Instead, it',
         'author': ['PS Huang', 'C Wang', 'S Huang', 'D Zhou'],
         'cites': '58',
         'eprint': 'https://arxiv.org/pdf/1706.05565',
         'gsrank': '1',
         'title': 'Towards neural phrase-based machine translation',
         'url': 'https://arxiv.org/abs/1706.05565',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=14839462711165509564&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DTowards%2BNeural%2BPhrase-based%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=vN-shLZc8M0J&ei=_80qX9yNI8KwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:vN-shLZc8M0J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQX_PeSypri2QSjbbYDgabKJLGW1Jm&scisig=AAGBfm0AAAAAXyrQX_81K3zeFtICf0zlSoDRGx9uiHoi&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
58
-------------------------------------------------
2020-08-05 15:19:35
Got the results of the query
{'bib': {'abstract': 'Ongoing innovations in recurrent neural network '
                     'architectures have provided a steady influx of '
                     'apparently state-of-the-art results on language '
                     'modelling benchmarks. However, these have been evaluated '
                     'using differing code bases and limited computational '
                     'resources, which represent uncontrolled sources of '
                     'experimental variation. We reevaluate several popular '
                     'architectures and regularisation methods with '
                     'large-scale automatic black-box hyperparameter tuning '
                     'and arrive at the somewhat surprising conclusion that '
                     'standard',
         'author': ['G Melis', 'C Dyer', 'P Blunsom'],
         'cites': '282',
         'eprint': 'https://arxiv.org/pdf/1707.05589.pdf),',
         'gsrank': '1',
         'title': 'On the state of the art of evaluation in neural language '
                  'models',
         'url': 'https://arxiv.org/abs/1707.05589',
         'venue': 'arXiv preprint arXiv:1707.05589',
         'year': '2017'},
 'citations_link': '/scholar?cites=10520579957359692654&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BState%2Bof%2Bthe%2BArt%2Bof%2BEvaluation%2Bin%2BNeural%2BLanguage%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=bsuEFbubAJIJ&ei=Cs4qX__wFaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:bsuEFbubAJIJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQa7mKMHxgHw-pmmZM9_c2SLLZkq-6&scisig=AAGBfm0AAAAAXyrQa0pWjYp4atqPQm7l-vNvFNlESdZv&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
282
-------------------------------------------------
2020-08-05 15:19:47
Got the results of the query
{'bib': {'abstract': 'Deep neural networks have excelled on a wide range of '
                     'problems, from vision to language and game playing. '
                     'Neural networks very gradually incorporate information '
                     'into weights as they process data, requiring very low '
                     'learning rates. If the training distribution shifts, the',
         'author': ['P Sprechmann', 'SM Jayakumar', 'JW Rae'],
         'cites': '42',
         'eprint': 'https://arxiv.org/pdf/1802.10542',
         'gsrank': '1',
         'title': 'Memory-based parameter adaptation',
         'url': 'https://arxiv.org/abs/1802.10542',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=13158888045275274984&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMemory-based%2BParameter%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=6F7EWfXCnbYJ&ei=Gs4qX4azOaiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:6F7EWfXCnbYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQeJTA0GSMhdl3wmVdXks5M0TZ7hCy&scisig=AAGBfm0AAAAAXyrQeKsd0tIyMC231oMZVP8npvzyh4Oj&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
42
-------------------------------------------------
2020-08-05 15:20:00
Got the results of the query
{'bib': {'abstract': 'Learning to predict complex time-series data is a '
                     'fundamental challenge in a range of disciplines '
                     'including Machine Learning, Robotics, and Natural '
                     'Language Processing. Predictive State Recurrent Neural '
                     'Networks (PSRNNs)(Downey et al.) are a state-of-the-art '
                     'approach for modeling time-series data which combine the '
                     'benefits of probabilistic filters and Recurrent Neural '
                     'Networks into a single model. PSRNNs leverage the '
                     'concept of Hilbert Space Embeddings of distributions '
                     '(Smola et al.) to embed predictive states into a',
         'author': ['K Choromanski', 'C Downey', 'B Boots'],
         'cites': '9',
         'eprint': 'https://openreview.net/pdf?id=HJJ23bW0b',
         'gsrank': '1',
         'title': 'Initialization matters: Orthogonal predictive state '
                  'recurrent neural networks',
         'url': 'https://openreview.net/forum?id=HJJ23bW0b&noteId=HJJ23bW0b',
         'venue': 'International Conference on …',
         'year': '2018'},
 'citations_link': '/scholar?cites=6668969337949062074&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DInitialization%2Bmatters:%2BOrthogonal%2BPredictive%2BState%2BRecurrent%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ugNtdkfwjFwJ&ei=JM4qX4jnEI-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ugNtdkfwjFwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQglTDZnBfvjiRiY_g-rfzy6wjkszA&scisig=AAGBfm0AAAAAXyrQgvfKBKWhov5Kyj9rwfwhStOGbGFT&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
9
-------------------------------------------------
2020-08-05 15:20:11
Got the results of the query
{'bib': {'abstract': 'Adversarial perturbations of normal images are usually '
                     'imperceptible to humans, but they can seriously confuse '
                     'state-of-the-art machine learning models. What makes '
                     'them so special in the eyes of image classifiers? In '
                     'this paper, we show empirically that adversarial '
                     'examples mainly lie in the low probability regions of '
                     'the training distribution, regardless of attack types '
                     'and targeted models. Using statistical hypothesis '
                     'testing, we find that modern neural density models are '
                     'surprisingly good at detecting imperceptible image '
                     'perturbations',
         'author': ['Y Song', 'T Kim', 'S Nowozin', 'S Ermon'],
         'cites': '296',
         'eprint': 'https://arxiv.org/pdf/1710.10766',
         'gsrank': '1',
         'title': 'Pixeldefend: Leveraging generative models to understand and '
                  'defend against adversarial examples',
         'url': 'https://arxiv.org/abs/1710.10766',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=9269726813530152599&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DPixelDefend:%2BLeveraging%2BGenerative%2BModels%2Bto%2BUnderstand%2Band%2BDefend%2Bagainst%2BAdversarial%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=l95gfWyvpIAJ&ei=MM4qX8SSMovrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:l95gfWyvpIAJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQjeFB6I0i256siL5ZnYZn-1-pOJJK&scisig=AAGBfm0AAAAAXyrQjU-5-_w4phpMJO-JMuMhXXuyqXh9&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
296
-------------------------------------------------
2020-08-05 15:20:21
Got the results of the query
{'bib': {'abstract': 'While neural networks have achieved high accuracy on '
                     'standard image classification benchmarks, their accuracy '
                     'drops to nearly zero in the presence of small '
                     'adversarial perturbations to test inputs. Defenses based '
                     'on regularization and adversarial training have been '
                     'proposed, but often followed by new, stronger attacks '
                     'that defeat these defenses. Can we somehow end this arms '
                     'race? In this work, we study this problem for neural '
                     'networks with one hidden layer. We first propose a '
                     'method based on a semidefinite relaxation that',
         'author': ['A Raghunathan', 'J Steinhardt', 'P Liang'],
         'cites': '387',
         'eprint': 'https://arxiv.org/pdf/1801.09344',
         'gsrank': '1',
         'title': 'Certified defenses against adversarial examples',
         'url': 'https://arxiv.org/abs/1801.09344',
         'venue': 'arXiv preprint arXiv:1801.09344',
         'year': '2018'},
 'citations_link': '/scholar?cites=17145877608540180848&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCertified%2BDefenses%2Bagainst%2BAdversarial%2BExamples%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=cJFcm9dk8u0J&ei=Oc4qX6joL7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:cJFcm9dk8u0J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQlRdf-uoqQvYk4lJ0ATrtOZTPhFtr&scisig=AAGBfm0AAAAAXyrQlSQVw3x-f7BbldSsU3DmJnuY5Gor&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
387
-------------------------------------------------
2020-08-05 15:20:29
Got the results of the query
{'bib': {'abstract': 'In recent years, deep neural network approaches have '
                     'been widely adopted for machine learning tasks, '
                     'including classification. However, they were shown to be '
                     'vulnerable to adversarial perturbations: carefully '
                     'crafted small perturbations can cause misclassification '
                     'of legitimate images. We propose Defense-GAN, a new '
                     'framework leveraging the expressive capability of '
                     'generative models to defend deep neural networks against '
                     'such attacks. Defense-GAN is trained to model the '
                     'distribution of unperturbed images. At inference time, '
                     'it',
         'author': ['P Samangouei', 'M Kabkab', 'R Chellappa'],
         'cites': '426',
         'eprint': 'https://arxiv.org/pdf/1805.06605',
         'gsrank': '1',
         'title': 'Defense-gan: Protecting classifiers against adversarial '
                  'attacks using generative models',
         'url': 'https://arxiv.org/abs/1805.06605',
         'venue': 'arXiv preprint arXiv:1805.06605',
         'year': '2018'},
 'citations_link': '/scholar?cites=4356922002684962280&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDefense-GAN:%2BProtecting%2BClassifiers%2BAgainst%2BAdversarial%2BAttacks%2BUsing%2BGenerative%2BModels%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=6OUePHrldjwJ&ei=RM4qX4nLFoyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:6OUePHrldjwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQobhTGrUyXFO6DE1BZbsMMVf0tKIy&scisig=AAGBfm0AAAAAXyrQodYT3BNPJRAen0toQAdFhvqUK5nX&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
426
-------------------------------------------------
2020-08-05 15:20:41
Got the results of the query
{'bib': {'abstract': 'Adversarial examples are perturbed inputs designed to '
                     'fool machine learning models. Adversarial training '
                     'injects such examples into training data to increase '
                     'robustness. To scale this technique to large datasets, '
                     'perturbations are crafted using fast single-step methods '
                     "that maximize a linear approximation of the model's "
                     'loss. We show that this form of adversarial training '
                     'converges to a degenerate global minimum, wherein small '
                     'curvature artifacts near the data points obfuscate a '
                     'linear approximation of the loss. The model thus',
         'author': ['F Tramèr', 'A Kurakin', 'N Papernot', 'I Goodfellow'],
         'cites': '876',
         'eprint': 'https://arxiv.org/pdf/1705.07204.pdf?',
         'gsrank': '1',
         'title': 'Ensemble adversarial training: Attacks and defenses',
         'url': 'https://arxiv.org/abs/1705.07204',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=10511209374384426640&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEnsemble%2BAdversarial%2BTraining:%2BAttacks%2Band%2BDefenses%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=kI7wNzxR35EJ&ei=TM4qX42zDcKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:kI7wNzxR35EJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQpgm3h6FRGeij8LSAs36iFYKKZ1xY&scisig=AAGBfm0AAAAAXyrQpgP-0eQwSTRMwPUtcEoP6J7uwC7j&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
876
-------------------------------------------------
2020-08-05 15:20:46
Got the results of the query
{'bib': {'abstract': 'Recurrent neural networks (RNNs) are important class of '
                     'architectures among neural networks useful for language '
                     'modeling and sequential prediction. However, optimizing '
                     'RNNs is known to be harder compared to feed-forward '
                     'neural networks. A number of',
         'author': ['K Zolna', 'D Arpit', 'D Suhubdy', 'Y Bengio'],
         'cites': '22',
         'eprint': 'https://arxiv.org/pdf/1711.00066',
         'gsrank': '1',
         'title': 'Fraternal dropout',
         'url': 'https://arxiv.org/abs/1711.00066',
         'venue': 'arXiv preprint arXiv:1711.00066',
         'year': '2017'},
 'citations_link': '/scholar?cites=4593127166702636404&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFraternal%2BDropout%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=dM2Bm9IQvj8J&ei=Us4qX_CmDojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:dM2Bm9IQvj8J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQsnKiPj2vcfr20Jmj7ctHdFIWDUgr&scisig=AAGBfm0AAAAAXyrQsstDkFFW52rYXPwFBDJ111HWBIAP&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
22
-------------------------------------------------
2020-08-05 15:20:58
Got the results of the query
{'bib': {'abstract': 'Successful recurrent models such as long short-term '
                     'memories (LSTMs) and gated recurrent units (GRUs) use ad '
                     'hoc gating mechanisms. Empirically these models have '
                     'been found to improve the learning of medium to long '
                     'term temporal dependencies and to help with vanishing '
                     'gradient issues. We prove that learnable gates in a '
                     'recurrent model formally provide quasi-invariance to '
                     'general time transformations in the input data. We '
                     'recover part of the LSTM architecture from a simple '
                     'axiomatic approach. This result leads to a new way of',
         'author': ['C Tallec', 'Y Ollivier'],
         'cites': '46',
         'eprint': 'https://arxiv.org/pdf/1804.11188',
         'gsrank': '1',
         'title': 'Can recurrent neural networks warp time?',
         'url': 'https://arxiv.org/abs/1804.11188',
         'venue': 'arXiv preprint arXiv:1804.11188',
         'year': '2018'},
 'citations_link': '/scholar?cites=17268685300536863187&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCan%2Brecurrent%2Bneural%2Bnetworks%2Bwarp%2Btime%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=02mClMmxpu8J&ei=X84qX6nIMbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:02mClMmxpu8J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQvr4dbB4Y0nvw5PqAvE7J1vYYWnXE&scisig=AAGBfm0AAAAAXyrQviNJZkakuP9tR3j1ZTWUs3KvMPCp&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
46
-------------------------------------------------
2020-08-05 15:21:10
Got the results of the query
{'bib': {'abstract': 'Recurrent neural networks (RNNs) are widely used to '
                     'model sequential data but their non-linear dependencies '
                     'between sequence elements prevent parallelizing training '
                     'over sequence length. We show the training of RNNs with '
                     'only linear sequential dependencies can be parallelized '
                     'over the sequence length using the parallel scan '
                     'algorithm, leading to rapid training on long sequences '
                     'even with small minibatch size. We develop a parallel '
                     'linear recurrence CUDA kernel and show that it can be '
                     'applied to immediately speed up',
         'author': ['E Martin', 'C Cundy'],
         'cites': '11',
         'eprint': 'https://arxiv.org/pdf/1709.04057',
         'gsrank': '1',
         'title': 'Parallelizing linear recurrent neural nets over sequence '
                  'length',
         'url': 'https://arxiv.org/abs/1709.04057',
         'venue': 'arXiv preprint arXiv:1709.04057',
         'year': '2017'},
 'citations_link': '/scholar?cites=9088138984771485785&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DParallelizing%2BLinear%2BRecurrent%2BNeural%2BNets%2BOver%2BSequence%2BLength%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=WQCvcEGOH34J&ei=bM4qX_DHOo-YmAHmlayoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:WQCvcEGOH34J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQyTo4QuwtCOMDXMwzro5TgDuIlEEW&scisig=AAGBfm0AAAAAXyrQyaPyBR4CnCjpSExw1PL2V_pj-4BJ&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
11
-------------------------------------------------
2020-08-05 15:21:21
Got the results of the query
{'bib': {'abstract': 'Neural networks with low-precision weights and '
                     'activations offer compelling efficiency advantages over '
                     'their full-precision equivalents. The two most '
                     'frequently discussed benefits of quantization are '
                     'reduced memory consumption, and a faster forward pass '
                     'when implemented with efficient bitwise operations. We '
                     'propose a third benefit of very low-precision neural '
                     'networks: improved robustness against some adversarial '
                     'attacks, and in the worst case, performance that is on '
                     'par with full-precision models. We focus on the very',
         'author': ['A Galloway', 'GW Taylor', 'M Moussa'],
         'cites': '40',
         'eprint': 'https://arxiv.org/pdf/1711.00449',
         'gsrank': '1',
         'title': 'Attacking binarized neural networks',
         'url': 'https://arxiv.org/abs/1711.00449',
         'venue': 'arXiv preprint arXiv:1711.00449',
         'year': '2017'},
 'citations_link': '/scholar?cites=4964512256521124807&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DAttacking%2BBinarized%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=x7sDHZh95UQJ&ei=ds4qX4qUNMKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:x7sDHZh95UQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQ1QnTTXJYmlvGF3ZL6MLjiOpi3OBk&scisig=AAGBfm0AAAAAXyrQ1aWHDu2MQ1oWJSp8r2dsNYhxRjx7&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
40
-------------------------------------------------
2020-08-05 15:21:33
Got the results of the query
{'bib': {'abstract': 'Depthwise separable convolutions reduce the number of '
                     'parameters and computation used in convolutional '
                     'operations while increasing representational efficiency. '
                     'They have been shown to be successful in image '
                     'classification models, both in obtaining better models '
                     'than',
         'author': ['L Kaiser', 'AN Gomez', 'F Chollet'],
         'cites': '116',
         'eprint': 'https://arxiv.org/pdf/1706.03059',
         'gsrank': '1',
         'title': 'Depthwise separable convolutions for neural machine '
                  'translation',
         'url': 'https://arxiv.org/abs/1706.03059',
         'venue': 'arXiv preprint arXiv:1706.03059',
         'year': '2017'},
 'citations_link': '/scholar?cites=7520360878420709403&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDepthwise%2BSeparable%2BConvolutions%2Bfor%2BNeural%2BMachine%2BTranslation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=G9AC1V-wXWgJ&ei=gs4qX5vpAZqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:G9AC1V-wXWgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQ4D1PwCsndQgQ-prQZTQ46SOYUW4k&scisig=AAGBfm0AAAAAXyrQ4BNrDUXViJkWnE4zOx33g0kN9z79&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
116
-------------------------------------------------
2020-08-05 15:21:45
Got the results of the query
{'bib': {'abstract': 'We introduce NoisyNet, a deep reinforcement learning '
                     'agent with parametric noise added to its weights, and '
                     "show that the induced stochasticity of the agent's "
                     'policy can be used to aid efficient exploration. The '
                     'parameters of the noise are learned with gradient '
                     'descent along',
         'author': ['M Fortunato', 'MG Azar', 'B Piot', 'J Menick'],
         'cites': '309',
         'eprint': 'https://arxiv.org/pdf/1706.10295.pdf?source=post_page---------------------------',
         'gsrank': '1',
         'title': 'Noisy networks for exploration',
         'url': 'https://arxiv.org/abs/1706.10295',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=13916735202249031707&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DNoisy%2BNetworks%2BFor%2BExploration%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=G0A6s-4sIsEJ&ei=js4qX57bJLGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:G0A6s-4sIsEJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQ64hQEEjjQEo6pUQ0q7rssSP5_UGk&scisig=AAGBfm0AAAAAXyrQ6y4DO4X9Cu2dvpAEL-yQk-TKAtUI&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
309
-------------------------------------------------
2020-08-05 15:21:56
Got the results of the query
{'bib': {'abstract': 'We introduce a hierarchical model for efficient '
                     'placement of computational graphs onto hardware devices, '
                     'especially in heterogeneous environments with a mixture '
                     'of CPUs, GPUs, and other computational devices. Our '
                     'method learns to assign graph operations to groups and '
                     'to allocate those groups to available devices. The '
                     'grouping and device allocations are learned jointly. The '
                     'proposed method is trained with policy gradient and '
                     'requires no human intervention. Experiments with '
                     'widely-used computer vision and natural',
         'author': ['A Mirhoseini', 'A Goldie', 'H Pham', 'B Steiner'],
         'cites': '40',
         'eprint': 'https://openreview.net/pdf?id=Hkc-TeZ0W',
         'gsrank': '1',
         'title': 'A hierarchical model for device placement',
         'url': 'https://openreview.net/forum?id=Hkc-TeZ0W&noteId=Hkc-TeZ0W',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=17910495902735510497&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BHierarchical%2BModel%2Bfor%2BDevice%2BPlacement%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=4aujMSHdjvgJ&ei=nc4qX5WPDaiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:4aujMSHdjvgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrQ-4xAHOpGQo6HbrjELKbdm-VKyQPq&scisig=AAGBfm0AAAAAXyrQ-4bJRKe9eIgC5VmcKv1ZRzMreR6i&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
40
-------------------------------------------------
2020-08-05 15:22:13
Got the results of the query
{'bib': {'abstract': 'Unsupervised anomaly detection on multi-or '
                     'high-dimensional data is of great importance in both '
                     'fundamental machine learning research and industrial '
                     'applications, for which density estimation lies at the '
                     'core. Although previous approaches based on '
                     'dimensionality reduction followed by density estimation '
                     'have made fruitful progress, they mainly suffer from '
                     'decoupled model learning with inconsistent optimization '
                     'goals and incapability of preserving essential '
                     'information in the low-dimensional space. In this paper, '
                     'we present a',
         'author': ['B Zong', 'Q Song', 'MR Min', 'W Cheng'],
         'cites': '184',
         'eprint': 'https://openreview.net/pdf?id=BJJLHbb0-',
         'gsrank': '1',
         'title': 'Deep autoencoding gaussian mixture model for unsupervised '
                  'anomaly detection',
         'url': 'https://openreview.net/forum?id=BJJLHbb0-',
         'venue': 'International …',
         'year': '2018'},
 'citations_link': '/scholar?cites=5266060849312268888&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BAutoencoding%2BGaussian%2BMixture%2BModel%2Bfor%2BUnsupervised%2BAnomaly%2BDetection%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=WILZEHHOFEkJ&ei=rM4qX5-qDovrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:WILZEHHOFEkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrRCjJm_bekayEoxeAWXcPZ3S8G8XMv&scisig=AAGBfm0AAAAAXyrRCr7vJKXpaIlOomO8f3PoerdeBBP-&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
184
-------------------------------------------------
2020-08-05 15:22:26
Got the results of the query
{'bib': {'abstract': 'Recent breakthroughs in computer vision make use of '
                     'large deep neural networks, utilizing the substantial '
                     'speedup offered by GPUs. For applications running on '
                     'limited hardware, however, high precision real-time '
                     'processing can still be a challenge. One approach to '
                     'solving this problem is training networks with binary or '
                     'ternary weights, thus removing the need to calculate '
                     'multiplications and significantly reducing memory size. '
                     'In this work, we introduce LR-nets (Local '
                     'reparameterization networks), a new method for training '
                     'neural',
         'author': ['O Shayer', 'D Levi', 'E Fetaya'],
         'cites': '34',
         'eprint': 'https://arxiv.org/pdf/1710.07739',
         'gsrank': '1',
         'title': 'Learning discrete weights using the local '
                  'reparameterization trick',
         'url': 'https://arxiv.org/abs/1710.07739',
         'venue': 'arXiv preprint arXiv:1710.07739',
         'year': '2017'},
 'citations_link': '/scholar?cites=13612900958005340664&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2BDiscrete%2BWeights%2BUsing%2Bthe%2BLocal%2BReparameterization%2BTrick%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=-Kl7DEy96rwJ&ei=uc4qX5_BHIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:-Kl7DEy96rwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrRFST75VwejihSc4hLLVWSTXix9ejC&scisig=AAGBfm0AAAAAXyrRFWqJlJg_Z-M5BK3mbaVZqn9Xa1vw&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
34
-------------------------------------------------
2020-08-05 15:22:37
Got the results of the query
{'bib': {'abstract': 'Neuromorphic hardware tends to pose limits on the '
                     'connectivity of deep networks that one can run on them. '
                     'But also generic hardware and software implementations '
                     'of deep learning run more efficiently for sparse '
                     'networks. Several methods exist for pruning connections '
                     'of a neural network after it was trained without '
                     'connectivity constraints. We present an algorithm, DEEP '
                     'R, that enables us to train directly a sparsely '
                     'connected neural network. DEEP R automatically rewires '
                     'the network during supervised training so that '
                     'connections are there',
         'author': ['G Bellec', 'D Kappel', 'W Maass', 'R Legenstein'],
         'cites': '45',
         'eprint': 'https://arxiv.org/pdf/1711.05136',
         'gsrank': '1',
         'title': 'Deep rewiring: Training very sparse deep networks',
         'url': 'https://arxiv.org/abs/1711.05136',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=6848423277041156645&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BRewiring:%2BTraining%2Bvery%2Bsparse%2Bdeep%2Bnetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=JQKOm698Cl8J&ei=xM4qX8KaD7GUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:JQKOm698Cl8J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrRIV0OJV1SrvqffcorqKC_qd_tXFs2&scisig=AAGBfm0AAAAAXyrRIbyGyWAdm8APor7b1c5HhZvaTTY1&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
45
-------------------------------------------------
2020-08-05 15:22:49
Got the results of the query
{'bib': {'abstract': 'Generative adversarial networks (GANs) have been '
                     'extremely effective in approximating complex '
                     'distributions of high-dimensional, input data samples, '
                     'and substantial progress has been made in understanding '
                     'and improving GAN performance in terms of both theory '
                     'and application. However, we currently lack quantitative '
                     'methods for model assessment. Because of this, while '
                     'many GAN variants are being proposed, we have relatively '
                     'little understanding of their relative abilities. In '
                     'this paper, we evaluate the performance of various',
         'author': ['DJ Im', 'H Ma', 'G Taylor', 'K Branson'],
         'cites': '44',
         'eprint': 'https://arxiv.org/pdf/1803.01045',
         'gsrank': '1',
         'title': 'Quantitatively evaluating GANs with divergences proposed '
                  'for training',
         'url': 'https://arxiv.org/abs/1803.01045',
         'venue': 'arXiv preprint arXiv:1803.01045',
         'year': '2018'},
 'citations_link': '/scholar?cites=4962866391557443877&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DQuantitatively%2BEvaluating%2BGANs%2BWith%2BDivergences%2BProposed%2Bfor%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=JZGBULCk30QJ&ei=zM4qX4-EI5qGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:JZGBULCk30QJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrRKxhEtHgiVz6fEDJPuAEIiCF47Oq8&scisig=AAGBfm0AAAAAXyrRKxM8sqDC-KC5NERs8qg3X-sUVZtl&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
44
-------------------------------------------------
2020-08-05 15:22:59
Got the results of the query
{'bib': {'abstract': 'We propose a novel regularizer to improve the training '
                     'of Generative Adversarial Networks (GANs). The '
                     'motivation is that when the discriminator D spreads out '
                     'its model capacity in the right way, the learning '
                     'signals given to the generator G are more informative '
                     'and diverse. These in turn help G to explore better and '
                     'discover the real data manifold while avoiding large '
                     'unstable jumps due to the erroneous extrapolation made '
                     'by D. Our regularizer guides the rectifier discriminator '
                     'D to better allocate its model capacity, by encouraging '
                     'the binary',
         'author': ['Y Cao', 'GW Ding', 'KYC Lui', 'R Huang'],
         'cites': '12',
         'eprint': 'https://arxiv.org/pdf/1805.03644',
         'gsrank': '1',
         'title': 'Improving GAN training via binarized representation entropy '
                  '(BRE) regularization',
         'url': 'https://arxiv.org/abs/1805.03644',
         'venue': 'arXiv preprint arXiv:1805.03644',
         'year': '2018'},
 'citations_link': '/scholar?cites=14467671840316463321&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DImproving%2BGAN%2BTraining%2Bvia%2BBinarized%2BRepresentation%2BEntropy%2B(BRE)%2BRegularization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=2Rj8wOJ-x8gJ&ei=4M4qX9vaEovrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:2Rj8wOJ-x8gJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrRPqH7aVUeA7boZrb9bvItmVzJoEiZ&scisig=AAGBfm0AAAAAXyrRPshzirEKHp0VGzU7JTVFrEys0OOR&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
12
-------------------------------------------------
2020-08-05 15:23:18
Got the results of the query
{'bib': {'abstract': 'Generative Adversarial Nets (GANs) and Variational '
                     'Auto-Encoders (VAEs) provide impressive image '
                     'generations from Gaussian white noise, but the '
                     'underlying mathematics are not well understood. We '
                     'compute deep convolutional network generators by '
                     'inverting a fixed embedding operator. Therefore, they do '
                     'not require to be optimized with a discriminator or an '
                     'encoder. The embedding is Lipschitz continuous to '
                     'deformations so that generators transform linear '
                     'interpolations between input white noise vectors into',
         'author': ['T Angles', 'S Mallat'],
         'cites': '15',
         'eprint': 'https://arxiv.org/pdf/1805.06621',
         'gsrank': '1',
         'title': 'Generative networks as inverse problems with scattering '
                  'transforms',
         'url': 'https://arxiv.org/abs/1805.06621',
         'venue': 'arXiv preprint arXiv:1805.06621',
         'year': '2018'},
 'citations_link': '/scholar?cites=2488553421180641259&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2Bnetworks%2Bas%2Binverse%2Bproblems%2Bwith%2BScattering%2Btransforms%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=6x8O0iYeiSIJ&ei=9M4qX8qFJKOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:6x8O0iYeiSIJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrRYXQZuaeB8FpJkU4BeaQ2EepE8405&scisig=AAGBfm0AAAAAXyrRYVDheGSx-qCWYrgVowMyQSMEp-Kp&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 15:23:53
Got the results of the query
{'bib': {'abstract': 'In this paper we approach two relevant deep learning '
                     'topics: i) tackling of graph structured input data and '
                     'ii) a better understanding and analysis of deep networks '
                     'and related learning algorithms. With this in mind we '
                     'focus on the topological classification of reachability '
                     'in a particular subset of planar graphs (Mazes). Doing '
                     'so, we are able to model the topology of data while '
                     'staying in Euclidean space, thus allowing its processing '
                     'with standard CNN architectures. We suggest a suitable '
                     'architecture for this problem and show that it can',
         'author': ['Z Ringel', 'R de Bem'],
         'cites': '0',
         'eprint': 'https://arxiv.org/pdf/1802.02154',
         'gsrank': '1',
         'title': 'Critical Percolation as a Framework to Analyze the Training '
                  'of Deep Networks',
         'url': 'https://arxiv.org/abs/1802.02154',
         'venue': 'arXiv preprint arXiv:1802.02154',
         'year': '2018'},
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCritical%2BPercolation%2Bas%2Ba%2BFramework%2Bto%2BAnalyze%2Bthe%2BTraining%2Bof%2BDeep%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=ucoUHIXdtIkJ&ei=FM8qX7aJLojHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:ucoUHIXdtIkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrRemvbpYZx5mHACQfEDQWK_0XEhtFH&scisig=AAGBfm0AAAAAXyrRep--eSv9wehqScAtwz3knGyfgm_n&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
0
-------------------------------------------------
2020-08-05 15:24:20
Got the results of the query
{'bib': {'abstract': 'Expressive efficiency refers to the relation between two '
                     'architectures A and B, whereby any function realized by '
                     'B could be replicated by A, but there exists functions '
                     'realized by A, which cannot be replicated by B unless '
                     'its size grows significantly larger. For example, it is '
                     'known that deep networks are exponentially efficient '
                     'with respect to shallow networks, in the sense that a '
                     'shallow network must grow exponentially large in order '
                     'to approximate the functions represented by a deep '
                     'network of polynomial size. In this work, we extend the',
         'author': ['O Sharir', 'A Shashua'],
         'cites': '15',
         'eprint': 'https://arxiv.org/pdf/1703.02065',
         'gsrank': '1',
         'title': 'On the expressive power of overlapping architectures of '
                  'deep learning',
         'url': 'https://arxiv.org/abs/1703.02065',
         'venue': 'arXiv preprint arXiv:1703.02065',
         'year': '2017'},
 'citations_link': '/scholar?cites=17865700268037263115&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DOn%2Bthe%2BExpressive%2BPower%2Bof%2BOverlapping%2BArchitectures%2Bof%2BDeep%2BLearning%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=C0_2u7y37_cJ&ei=Ks8qX6H_HbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:C0_2u7y37_cJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrRm5xD3JqDkgUKC_0BoYGcRsbzNB0r&scisig=AAGBfm0AAAAAXyrRm290apJDd1SFVdTcHErmzbHV87RN&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
15
-------------------------------------------------
2020-08-05 15:24:51
Got the results of the query
{'bib': {'abstract': 'Model pruning has become a useful technique that '
                     'improves the computational efficiency of deep learning, '
                     'making it possible to deploy solutions in '
                     'resource-limited scenarios. A widely-used practice in '
                     'relevant work assumes that a smaller-norm parameter or '
                     'feature plays a less informative role at the inference '
                     'time. In this paper, we propose a channel pruning '
                     'technique for accelerating the computations of deep '
                     'convolutional neural networks (CNNs) that does not '
                     'critically rely on this assumption. Instead, it focuses '
                     'on direct simplification of',
         'author': ['J Ye', 'X Lu', 'Z Lin', 'JZ Wang'],
         'cites': '111',
         'eprint': 'https://arxiv.org/pdf/1802.00124',
         'gsrank': '1',
         'title': 'Rethinking the smaller-norm-less-informative assumption in '
                  'channel pruning of convolution layers',
         'url': 'https://arxiv.org/abs/1802.00124',
         'venue': 'arXiv preprint arXiv:1802.00124',
         'year': '2018'},
 'citations_link': '/scholar?cites=17821725364773859726&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRethinking%2Bthe%2BSmaller-Norm-Less-Informative%2BAssumption%2Bin%2BChannel%2BPruning%2Bof%2BConvolution%2BLayers%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=jjHqs8t8U_cJ&ei=SM8qX8G0HrGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:jjHqs8t8U_cJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrRp39Ukd0N_TfjVABmpvkDjikeVfnD&scisig=AAGBfm0AAAAAXyrRpzFmqyhqJBNVbNGKUDPkIOVgUoWS&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
111
-------------------------------------------------
2020-08-05 15:25:03
Got the results of the query
{'bib': {'abstract': 'Spatiotemporal forecasting has various applications in '
                     'neuroscience, climate and transportation domain. Traffic '
                     'forecasting is one canonical example of such learning '
                     'task. The task is challenging due to (1) complex spatial '
                     'dependency on road networks,(2) non-linear temporal '
                     'dynamics with changing road conditions and (3) inherent '
                     'difficulty of long-term forecasting. To address these '
                     'challenges, we propose to model the traffic flow as a '
                     'diffusion process on a directed graph and introduce '
                     'Diffusion Convolutional Recurrent',
         'author': ['Y Li', 'R Yu', 'C Shahabi', 'Y Liu'],
         'cites': '361',
         'eprint': 'https://arxiv.org/pdf/1707.01926',
         'gsrank': '1',
         'title': 'Diffusion convolutional recurrent neural network: '
                  'Data-driven traffic forecasting',
         'url': 'https://arxiv.org/abs/1707.01926',
         'venue': 'arXiv preprint arXiv:1707.01926',
         'year': '2017'},
 'citations_link': '/scholar?cites=6301301566407555232&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDiffusion%2BConvolutional%2BRecurrent%2BNeural%2BNetwork:%2BData-Driven%2BTraffic%2BForecasting%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=oLhVkGO4clcJ&ei=VM8qX9KWJqiBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:oLhVkGO4clcJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrRsp7P_rYLqKh1lVQqsoRwNbglSarc&scisig=AAGBfm0AAAAAXyrRshicgO0eKInjYjdDsCw1zttl_BHK&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
361
-------------------------------------------------
2020-08-05 15:25:14
Got the results of the query
{'bib': {'abstract': 'Collecting a large dataset with high quality annotations '
                     'is expensive and time-consuming. Recently, Shrivastava '
                     'et al.(2017) propose Simulated+ Unsupervised (S+ U) '
                     'learning: It first learns a mapping from synthetic data '
                     'to real data, translates a large amount of labeled '
                     'synthetic data to the ones that resemble real data, and '
                     'then trains a learning model on the translated data. '
                     'Bousmalis et al.(2017) propose a similar framework that '
                     'jointly trains a translation mapping and a learning '
                     'model. While these algorithms are shown to achieve the',
         'author': ['K Lee', 'H Kim', 'C Suh'],
         'cites': '7',
         'eprint': 'https://openreview.net/pdf?id=SkHDoG-Cb',
         'gsrank': '1',
         'title': 'Simulated+ unsupervised learning with adaptive data '
                  'generation and bidirectional mappings',
         'url': 'https://openreview.net/forum?id=SkHDoG-Cb',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=17901477215249153215&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSimulated%252BUnsupervised%2BLearning%2BWith%2BAdaptive%2BData%2BGeneration%2Band%2BBidirectional%2BMappings%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=v7ysYa7SbvgJ&ei=Yc8qX9OACsKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:v7ysYa7SbvgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrRvUzk97dLs4UWzJUXDfaTQ5-8Zzqg&scisig=AAGBfm0AAAAAXyrRvSv_PgtRdVw-y-xdr3rdZYSt0tf6&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
7
-------------------------------------------------
2020-08-05 15:25:25
Got the results of the query
{'bib': {'abstract': 'Common-sense physical reasoning is an essential '
                     'ingredient for any intelligent agent operating in the '
                     'real-world. For example, it can be used to simulate the '
                     'environment, or to infer the state of parts of the world '
                     'that are currently unobserved. In order to match '
                     'real-world conditions this causal knowledge must be '
                     'learned without access to supervised data. To address '
                     'this problem we present a novel method that learns to '
                     'discover objects and model their physical interactions '
                     'from raw visual images in a purely\\emph {unsupervised} '
                     'fashion. It',
         'author': ['S Van Steenkiste', 'M Chang', 'K Greff'],
         'cites': '97',
         'eprint': 'https://arxiv.org/pdf/1802.10353',
         'gsrank': '1',
         'title': 'Relational neural expectation maximization: Unsupervised '
                  'discovery of objects and their interactions',
         'url': 'https://arxiv.org/abs/1802.10353',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11323622217846680222&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DRelational%2BNeural%2BExpectation%2BMaximization:%2BUnsupervised%2BDiscovery%2Bof%2BObjects%2Band%2Btheir%2BInteractions%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=nup4eWmWJZ0J&ei=as8qX6DmDLGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:nup4eWmWJZ0J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrRx-Y55__UUMEsJp6-DX-helb9NA_h&scisig=AAGBfm0AAAAAXyrRx11PNCrCyUswcHarbKv-DPWaQcnf&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
97
-------------------------------------------------
2020-08-05 15:25:35
Got the results of the query
{'bib': {'abstract': 'It is easy for people to imagine what a man with pink '
                     'hair looks like, even if they have never seen such a '
                     'person before. We call the ability to create images of '
                     'novel semantic concepts visually grounded imagination. '
                     'In this paper, we show how we can modify variational '
                     'auto-encoders to perform this task. Our method uses a '
                     'novel training objective, and a novel product-of-experts '
                     'inference network, which can handle partially specified '
                     '(abstract) concepts in a principled and efficient way. '
                     'We also propose a set of easy-to-compute',
         'author': ['R Vedantam', 'I Fischer', 'J Huang', 'K Murphy'],
         'cites': '63',
         'eprint': 'https://arxiv.org/pdf/1705.10762',
         'gsrank': '1',
         'title': 'Generative models of visually grounded imagination',
         'url': 'https://arxiv.org/abs/1705.10762',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=7259828402586264383&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGenerative%2BModels%2Bof%2BVisually%2BGrounded%2BImagination%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=PyNyyncXwGQJ&ei=ds8qX4HGFbGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:PyNyyncXwGQJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrR0gVdcoQiSTWrVrACosw_R4QBfmsy&scisig=AAGBfm0AAAAAXyrR0v4-RZSex9s77zYH5620FxylhOIy&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
63
-------------------------------------------------
2020-08-05 15:25:46
Got the results of the query
{'bib': {'abstract': 'Deep autoregressive models have shown state-of-the-art '
                     'performance in density estimation for natural images on '
                     'large-scale datasets such as ImageNet. However, such '
                     'models require many thousands of gradient-based weight '
                     'updates and unique image examples for training. Ideally, '
                     'the models would rapidly learn visual concepts from only '
                     'a handful of examples, similar to the manner in which '
                     'humans learns across many vision tasks. In this paper, '
                     'we show how 1) neural attention and 2) meta learning '
                     'techniques can be used in',
         'author': ['S Reed', 'Y Chen', 'T Paine', 'A Oord', 'SM Eslami'],
         'cites': '41',
         'eprint': 'https://arxiv.org/pdf/1710.10304.pdf?source=post_page---------------------------',
         'gsrank': '1',
         'title': 'Few-shot autoregressive density estimation: Towards '
                  'learning to learn distributions',
         'url': 'https://arxiv.org/abs/1710.10304',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=12042481610635531058&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFew-shot%2BAutoregressive%2BDensity%2BEstimation:%2BTowards%2BLearning%2Bto%2BLearn%2BDistributions%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=MtMamDh9H6cJ&ei=gM8qX_HkOcKwmAH6vamoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:MtMamDh9H6cJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrR3vBO5M2zc-ND4ObOSfoyao1ifeAl&scisig=AAGBfm0AAAAAXyrR3tPRV3E5FUftdEnsuWc1FNmYMwsv&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
41
-------------------------------------------------
2020-08-05 15:25:58
Got the results of the query
{'bib': {'abstract': 'One of the distinguishing aspects of human language is '
                     'its compositionality, which allows us to describe '
                     'complex environments with limited vocabulary. '
                     'Previously, it has been shown that neural network agents '
                     'can learn to communicate in a highly structured, '
                     'possibly compositional language based on disentangled '
                     'input (eg hand-engineered features). Humans, however, do '
                     'not learn to communicate based on well-summarized '
                     'features. In this work, we train neural agents to '
                     'simultaneously develop visual perception from raw image',
         'author': ['E Choi', 'A Lazaridou', 'N de Freitas'],
         'cites': '31',
         'eprint': 'https://arxiv.org/pdf/1804.02341',
         'gsrank': '1',
         'title': 'Compositional obverter communication learning from raw '
                  'visual input',
         'url': 'https://arxiv.org/abs/1804.02341',
         'venue': 'arXiv preprint arXiv:1804.02341',
         'year': '2018'},
 'citations_link': '/scholar?cites=11340269692127577649&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DCompositional%2BObverter%2BCommunication%2BLearning%2Bfrom%2BRaw%2BVisual%2BInput%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=MYZkoDO7YJ0J&ei=jM8qX6DnE4jHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:MYZkoDO7YJ0J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrR676CRYKEAx4ClQJ8DxzPyrvBaUbi&scisig=AAGBfm0AAAAAXyrR6x0JN5daQ8pxBv6vnnG6ijY8C7co&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
31
-------------------------------------------------
2020-08-05 15:26:11
Got the results of the query
{'bib': {'abstract': 'The seemingly infinite diversity of the natural world '
                     'arises from a relatively small set of coherent rules, '
                     'such as the laws of physics or chemistry. We conjecture '
                     'that these rules give rise to regularities that can be '
                     'discovered through primarily unsupervised experiences '
                     'and represented as abstract concepts. If such '
                     'representations are compositional and hierarchical, they '
                     'can be recombined into an exponentially large set of new '
                     'concepts. This paper describes SCAN (Symbol-Concept '
                     'Association Network), a new framework for',
         'author': ['I Higgins', 'N Sonnerat', 'L Matthey', 'A Pal'],
         'cites': '41',
         'eprint': 'https://arxiv.org/pdf/1707.03389',
         'gsrank': '1',
         'title': 'Scan: Learning hierarchical compositional visual concepts',
         'url': 'https://arxiv.org/abs/1707.03389',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=9825853684146540580&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSCAN:%2BLearning%2BHierarchical%2BCompositional%2BVisual%2BConcepts%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=JJCEut1xXIgJ&ei=ls8qX8T0LpqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:JJCEut1xXIgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrR9GBxhXAnA2aVHXYTafXVC06o1eKI&scisig=AAGBfm0AAAAAXyrR9GeN50NlsDSz5D7DU6e_3gX1qY8s&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
41
-------------------------------------------------
2020-08-05 15:26:20
Got the results of the query
{'bib': {'abstract': 'By representing words with probability densities rather '
                     'than point vectors, probabilistic word embeddings can '
                     'capture rich and interpretable semantic information and '
                     'uncertainty. The uncertainty information can be '
                     'particularly meaningful in capturing entailment '
                     'relationships',
         'author': ['B Athiwaratkun', 'AG Wilson'],
         'cites': '24',
         'eprint': 'https://arxiv.org/pdf/1804.09843.pdf,',
         'gsrank': '1',
         'title': 'Hierarchical density order embeddings',
         'url': 'https://arxiv.org/abs/1804.09843',
         'venue': 'arXiv preprint arXiv:1804.09843',
         'year': '2018'},
 'citations_link': '/scholar?cites=12427920250451702495&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DHierarchical%2BDensity%2BOrder%2BEmbeddings%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=37IOMZ_XeKwJ&ei=os8qX_SqAovrmQHcvJqoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:37IOMZ_XeKwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrR_6K1gu0uvckI4eGmjmQmkwfWzCHy&scisig=AAGBfm0AAAAAXyrR_-0XEUI-yRpJdup8-AZFzMGJeMTG&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
24
-------------------------------------------------
2020-08-05 15:26:31
Got the results of the query
-------------------------------------------------
2020-08-05 15:27:20
Got the results of the query
{'bib': {'abstract': 'Decades of research on the neural code underlying '
                     'spatial navigation have revealed a diverse set of neural '
                     'response properties. The Entorhinal Cortex (EC) of the '
                     'mammalian brain contains a rich set of spatial '
                     'correlates, including grid cells which encode space '
                     'using tessellating patterns. However, the mechanisms and '
                     'functional significance of these spatial representations '
                     'remain largely mysterious. As a new way to understand '
                     'these neural representations, we trained recurrent '
                     'neural networks (RNNs) to perform navigation tasks in',
         'author': ['CJ Cueva', 'XX Wei'],
         'cites': '68',
         'eprint': 'https://arxiv.org/pdf/1803.07770',
         'gsrank': '1',
         'title': 'Emergence of grid-like representations by training '
                  'recurrent neural networks to perform spatial localization',
         'url': 'https://arxiv.org/abs/1803.07770',
         'venue': 'arXiv preprint arXiv:1803.07770',
         'year': '2018'},
 'citations_link': '/scholar?cites=17982363881710964955&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DEmergence%2Bof%2Bgrid-like%2Brepresentations%2Bby%2Btraining%2Brecurrent%2Bneural%2Bnetworks%2Bto%2Bperform%2Bspatial%2Blocalization%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=21Ay_6wwjvkJ&ei=9s8qX9H6C5qGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:21Ay_6wwjvkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrSWphJqhygb6ZpRNrlcHWcXt_Tc9pe&scisig=AAGBfm0AAAAAXyrSWhJgK1jFizMMbQ20wWG5k2ApOWWp&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
68
-------------------------------------------------
2020-08-05 15:28:03
Got the results of the query
{'bib': {'abstract': 'Retinal prostheses for treating incurable blindness are '
                     'designed to electrically stimulate surviving retinal '
                     'neurons, causing them to send artificial visual signals '
                     'to the brain. However, electrical stimulation generally '
                     'cannot precisely reproduce typical patterns of neural '
                     'activity in the retina. Therefore, an electrical '
                     'stimulus must be selected so as to produce a neural '
                     'response as close as possible to the desired response. '
                     'This setting requires a technique for computing the '
                     'distance between a desired response and an achievable '
                     'response that is',
         'author': ['NP Shah', 'S Madugula', 'EJ Chichilnisky', 'J Shlens'],
         'cites': '1',
         'eprint': 'https://www.biorxiv.org/content/biorxiv/early/2018/01/30/226530.full.pdf',
         'gsrank': '1',
         'title': 'Learning a neural response metric for retinal prosthesis',
         'url': 'https://www.biorxiv.org/content/10.1101/226530v2.abstract',
         'venue': 'bioRxiv',
         'year': '2017'},
 'citations_link': '/scholar?cites=12862320072794108403&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Ba%2Bneural%2Bresponse%2Bmetric%2Bfor%2Bretinal%2Bprosthesis%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=8_FEbvUjgLIJ&ei=FdAqX57pOaOGy9YPpO2gqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:8_FEbvUjgLIJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrSdZpua0U3eadjLQJ9TlYDdKDmJpYW&scisig=AAGBfm0AAAAAXyrSdYk-4K5lIHQ8Ho_bUTF-X5z_e-3i&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
1
-------------------------------------------------
2020-08-05 15:28:29
Got the results of the query
{'bib': {'abstract': 'We propose to study the problem of few-shot learning '
                     'with the prism of inference on a partially observed '
                     'graphical model, constructed from a collection of input '
                     'images whose label can be either observed or not. By '
                     'assimilating generic message-passing inference '
                     'algorithms with their neural-network counterparts, we '
                     'define a graph neural network architecture that '
                     'generalizes several of the recently proposed few-shot '
                     'learning models. Besides providing improved numerical '
                     'performance, our framework is easily extended to',
         'author': ['V Garcia', 'J Bruna'],
         'cites': '288',
         'eprint': 'https://arxiv.org/pdf/1711.04043',
         'gsrank': '1',
         'title': 'Few-shot learning with graph neural networks',
         'url': 'https://arxiv.org/abs/1711.04043',
         'venue': 'arXiv preprint arXiv:1711.04043',
         'year': '2017'},
 'citations_link': '/scholar?cites=15420545241088720867&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DFew-Shot%2BLearning%2Bwith%2BGraph%2BNeural%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=4__u-THIANYJ&ei=JNAqX624GZqGy9YPgY6TqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:4__u-THIANYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrSgGtzuW-vozDIpfcKi3jab4tVRlfX&scisig=AAGBfm0AAAAAXyrSgG0iZbjzExFqYrDqwiexXFcvPDlG&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
288
-------------------------------------------------
2020-08-05 15:28:40
Got the results of the query
{'bib': {'abstract': 'We propose a new algorithm for training generative '
                     'adversarial networks that jointly learns latent codes '
                     'for both identities (eg individual humans) and '
                     'observations (eg specific photographs). By fixing the '
                     'identity portion of the latent codes, we can generate '
                     'diverse images of the same subject, and by fixing the '
                     'observation portion, we can traverse the manifold of '
                     'subjects while maintaining contingent aspects such as '
                     'lighting and pose. Our algorithm features a pairwise '
                     'training scheme in which each sample from the generator',
         'author': ['C Donahue', 'ZC Lipton', 'A Balsubramani'],
         'cites': '68',
         'eprint': 'https://arxiv.org/pdf/1705.07904',
         'gsrank': '1',
         'title': 'Semantically decomposing the latent spaces of generative '
                  'adversarial networks',
         'url': 'https://arxiv.org/abs/1705.07904',
         'venue': 'arXiv preprint arXiv …',
         'year': '2017'},
 'citations_link': '/scholar?cites=8664262583947148240&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DSemantically%2BDecomposing%2Bthe%2BLatent%2BSpaces%2Bof%2BGenerative%2BAdversarial%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=0LvgNOqkPXgJ&ei=LdAqX4_bIIjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:0LvgNOqkPXgJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrSjB3wRdbJY2FzS95nebXBm3R0_4Pr&scisig=AAGBfm0AAAAAXyrSjLYGllBk6OANijkprXiG5wdGKB5H&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
68
-------------------------------------------------
2020-08-05 15:28:52
Got the results of the query
{'bib': {'abstract': 'Recent AI research has emphasised the importance of '
                     'learning disentangled representations of the explanatory '
                     'factors behind data. Despite the growing interest in '
                     'models which can learn such representations, visual '
                     'inspection remains the standard evaluation metric. While '
                     'various desiderata have been implied in recent '
                     'definitions, it is currently unclear what exactly makes '
                     'one disentangled representation better than another. In '
                     'this work we propose a framework for the quantitative '
                     'evaluation of disentangled',
         'author': ['C Eastwood', 'CKI Williams'],
         'cites': '76',
         'eprint': 'https://openreview.net/pdf?id=By-7dz-AZ',
         'gsrank': '1',
         'title': 'A framework for the quantitative evaluation of disentangled '
                  'representations',
         'url': 'https://openreview.net/forum?id=By-7dz-AZ&noteId=By-7dz-AZ',
         'venue': 'International Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=3224087322020629595&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BFramework%2Bfor%2Bthe%2BQuantitative%2BEvaluation%2Bof%2BDisentangled%2BRepresentations%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=W0zhYVZCviwJ&ei=NtAqX9ybM6iBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:W0zhYVZCviwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrSlJf3zRltR4T-tki9bitoHLrEsksj&scisig=AAGBfm0AAAAAXyrSlAxuidw3ZhlysPs-3i4bTjUOg9OG&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
76
-------------------------------------------------
2020-08-05 15:29:00
Got the results of the query
{'bib': {'abstract': 'In few-shot classification, we are interested in '
                     'learning algorithms that train a classifier from only a '
                     'handful of labeled examples. Recent progress in few-shot '
                     'classification has featured meta-learning, in which a '
                     'parameterized model for a learning algorithm is defined '
                     'and trained on episodes representing different '
                     'classification problems, each with a small labeled '
                     'training set and its corresponding test set. In this '
                     'work, we advance this few-shot classification paradigm '
                     'towards a scenario where unlabeled examples are also '
                     'available',
         'author': ['M Ren', 'E Triantafillou', 'S Ravi', 'J Snell'],
         'cites': '239',
         'eprint': 'https://arxiv.org/pdf/1803.00676.pdf?utm_campaign=affiliate-ir-Optimise%20media%28%20South%20East%20Asia%29%20Pte.%20ltd._156_-99_national_R_all_ACQ_cpa_en&utm_content=&utm_source=%20388939',
         'gsrank': '1',
         'title': 'Meta-learning for semi-supervised few-shot classification',
         'url': 'https://arxiv.org/abs/1803.00676',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=798380540199769906&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DMeta-Learning%2Bfor%2BSemi-Supervised%2BFew-Shot%2BClassification%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=MhcxDt9qFAsJ&ei=QNAqX9n4LIjHmAHV2pCoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:MhcxDt9qFAsJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrSny3qfPVjnVc95Ly5XN6gJfrT3-uz&scisig=AAGBfm0AAAAAXyrSnx1njGK76FZ7zBo9n22AcMqieoTk&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
239
-------------------------------------------------
2020-08-05 15:29:11
Got the results of the query
{'bib': {'abstract': 'Domain adaptation refers to the problem of leveraging '
                     'labeled data in a source domain to learn an accurate '
                     'model in a target domain where labels are scarce or '
                     'unavailable. A recent approach for finding a common '
                     'representation of the two domains is via domain '
                     'adversarial training (Ganin & Lempitsky, 2015), which '
                     'attempts to induce a feature extractor that matches the '
                     'source and target feature distributions in some feature '
                     'space. However, domain adversarial training faces two '
                     'critical limitations: 1) if the feature extraction '
                     'function has high',
         'author': ['R Shu', 'HH Bui', 'H Narui', 'S Ermon'],
         'cites': '185',
         'eprint': 'https://arxiv.org/pdf/1802.08735',
         'gsrank': '1',
         'title': 'A dirt-t approach to unsupervised domain adaptation',
         'url': 'https://arxiv.org/abs/1802.08735',
         'venue': 'arXiv preprint arXiv:1802.08735',
         'year': '2018'},
 'citations_link': '/scholar?cites=8960716763873957731&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DA%2BDIRT-T%2BApproach%2Bto%2BUnsupervised%2BDomain%2BAdaptation%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=Y1eYu2vcWnwJ&ei=StAqX9fRE6iBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:Y1eYu2vcWnwJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrSqKJmU5eU_ynE4B2MvdbjC5cILxUX&scisig=AAGBfm0AAAAAXyrSqMmlvplY0niKa0G-oaV3sNLhNWqt&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
185
-------------------------------------------------
2020-08-05 15:29:20
Got the results of the query
{'bib': {'abstract': 'We present CROSSGRAD, a method to use multi-domain '
                     'training data to learn a classifier that generalizes to '
                     'new domains. CROSSGRAD does not need an adaptation phase '
                     'via labeled or unlabeled data, or domain features in the '
                     'new domain. Most existing domain adaptation methods '
                     'attempt to erase domain signals using techniques like '
                     'domain adversarial training. In contrast, CROSSGRAD is '
                     'free to use domain signals for predicting labels, if it '
                     'can prevent overfitting on training domains. We '
                     'conceptualize the task in a',
         'author': ['S Shankar', 'V Piratla', 'S Chakrabarti'],
         'cites': '71',
         'eprint': 'https://arxiv.org/pdf/1804.10745',
         'gsrank': '1',
         'title': 'Generalizing across domains via cross-gradient training',
         'url': 'https://arxiv.org/abs/1804.10745',
         'venue': 'arXiv preprint arXiv …',
         'year': '2018'},
 'citations_link': '/scholar?cites=4167124586655060881&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DGeneralizing%2BAcross%2BDomains%2Bvia%2BCross-Gradient%2BTraining%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=kRNfsruZ1DkJ&ei=VtAqX56pD6iBy9YPmoOWqAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:kRNfsruZ1DkJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrStNdyKvbQybAnIy0gJUinySM_7_Ld&scisig=AAGBfm0AAAAAXyrStKjQtPHHObrrIud3ABaikfxB9Qtc&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
71
-------------------------------------------------
2020-08-05 15:29:32
Got the results of the query
{'bib': {'abstract': 'This paper introduces a novel method to perform transfer '
                     'learning across domains and tasks, formulating it as a '
                     'problem of learning to cluster. The key insight is that, '
                     'in addition to features, we can transfer similarity '
                     'information and this is sufficient to learn a similarity '
                     'function and clustering network to perform both domain '
                     'adaptation and cross-task transfer learning. We begin by '
                     'reducing categorical information to pairwise '
                     'constraints, which only considers whether two instances '
                     'belong to the same class or not. This similarity is '
                     'category',
         'author': ['YC Hsu', 'Z Lv', 'Z Kira'],
         'cites': '30',
         'eprint': 'https://arxiv.org/pdf/1711.10125',
         'gsrank': '1',
         'title': 'Learning to cluster in order to transfer across domains and '
                  'tasks',
         'url': 'https://arxiv.org/abs/1711.10125',
         'venue': 'arXiv preprint arXiv:1711.10125',
         'year': '2017'},
 'citations_link': '/scholar?cites=5063814464550490029&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DLearning%2Bto%2Bcluster%2Bin%2Border%2Bto%2Btransfer%2Bacross%2Bdomains%2Band%2Btasks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=rU8EIG1IRkYJ&ei=X9AqX_K2PIyimwG-4IKoDA',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:rU8EIG1IRkYJ:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrSverLJLLMN_EdSWEdzD4IFxWRwaum&scisig=AAGBfm0AAAAAXyrSvdKN4F5AxQDqkMu96JkDURJMfz4C&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
30
-------------------------------------------------
2020-08-05 15:29:41
Got the results of the query
{'bib': {'abstract': 'Our contributions can be summarized as follows: 1. We '
                     'propose a new neural architecture, Deep Complex U-Net, '
                     'which combines the advan- tages of both deep complex '
                     'networks and U-Net, yielding state-of-the-art '
                     'performance',
         'author': ['HS Choi', 'JH Kim', 'J Huh', 'A Kim', 'JW Ha'],
         'cites': '22',
         'eprint': 'https://openreview.net/pdf?id=SkeRTsAcYm',
         'gsrank': '1',
         'title': 'Phase-aware speech enhancement with deep complex u-net',
         'url': 'https://openreview.net/forum?id=SkeRTsAcYm',
         'venue': '… Conference on Learning …',
         'year': '2018'},
 'citations_link': '/scholar?cites=11525798829336957800&as_sdt=5,33&sciodt=0,33&hl=en',
 'filled': False,
 'source': 'scholar',
 'url_add_sclib': '/citations?hl=en&xsrf=&continue=/scholar%3Fq%3DDeep%2BComplex%2BNetworks%26hl%3Den%26as_sdt%3D0,33&citilm=1&json=&update_op=library_add&info=aBM-D_jc858J&ei=aNAqX623KrGUy9YPicCJsAw',
 'url_scholarbib': 'https://scholar.googleusercontent.com/scholar.bib?q=info:aBM-D_jc858J:scholar.google.com/&output=citation&scisdr=CgXVrNo9GAA:AAGBfm0AAAAAXyrSxKUqj5MJJ_iEO3U0ZwMLE7hBdVJC&scisig=AAGBfm0AAAAAXyrSxFitvRCyM3vx5x6BR7HIpkFDXP4j&scisf=4&ct=citation&cd=-1&hl=en'}
<class 'dict'>
22
-------------------------------------------------
2020-08-05 15:29:48
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://89.218.155.78:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://200.73.128.5:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://81.163.42.169:41258
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://200.73.128.5:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://84.210.183.61:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://84.210.183.61:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://84.210.183.61:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://191.232.170.36:80
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://79.155.30.205:8080
Trying new proxy
Working proxy: http://219.76.243.115:3128
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://81.163.42.169:41258
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://84.210.183.61:3128
Trying new proxy
Working proxy: http://13.75.114.68:25222
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://84.210.183.61:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://200.73.128.5:80
Trying new proxy
Working proxy: http://200.73.128.5:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://200.73.128.5:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://191.97.45.111:23500
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://89.218.155.78:3128
Trying new proxy
Working proxy: http://191.97.45.111:23500
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://117.20.26.21:8080
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://117.20.26.21:8080
Trying new proxy
Working proxy: http://103.102.15.90:11510
Trying new proxy
Working proxy: http://191.97.45.111:23500
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://103.28.121.58:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://191.97.45.111:23500
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://103.28.121.58:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://18.216.16.30:3838
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://18.219.130.108:3838
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://191.232.170.36:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://191.232.170.36:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://191.232.170.36:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://18.218.173.238:3838
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://80.23.125.226:3128
Trying new proxy
Working proxy: http://191.232.170.36:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://103.253.27.45:80
Trying new proxy
Working proxy: http://45.236.171.94:999
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://206.198.131.142:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://51.255.103.170:3129
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://51.255.103.170:3129
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://206.198.131.142:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://206.198.131.142:80
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://84.210.183.61:3128
Trying new proxy
Working proxy: http://125.212.218.43:8888
Trying new proxy
Working proxy: http://103.28.121.58:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://103.28.121.58:3128
Trying new proxy
Working proxy: http://103.28.121.58:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://78.110.2.90:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://103.28.121.58:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://103.28.121.58:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://103.28.121.58:3128
Trying new proxy
Working proxy: http://125.212.218.43:8888
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://139.99.105.185:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://34.91.135.38:80
Trying new proxy
Working proxy: http://18.218.173.238:3838
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://34.91.135.38:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://191.232.170.36:80
Trying new proxy
Working proxy: http://34.105.59.26:80
Trying new proxy
Working proxy: http://103.227.255.43:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://18.221.58.204:3838
Trying new proxy
Working proxy: http://209.250.244.46:32602
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://18.221.58.204:3838
Trying new proxy
Working proxy: http://54.37.154.97:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://80.23.125.226:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.23.125.226:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://80.23.125.226:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://18.219.130.108:3838
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://80.23.125.226:3128
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://54.39.148.239:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://95.179.228.207:32244
Trying new proxy
Working proxy: http://34.91.135.38:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://18.218.173.238:3838
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://82.200.233.4:3128
Trying new proxy
Working proxy: http://54.39.148.239:80
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://125.212.218.43:8888
Trying new proxy
Working proxy: http://91.234.195.154:8888
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://18.219.130.108:3838
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://18.219.130.108:3838
Trying new proxy
Working proxy: http://18.219.130.108:3838
Trying new proxy
Working proxy: http://18.218.173.238:3838
Trying new proxy
Working proxy: http://139.99.105.5:80
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://218.161.36.89:53438
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://200.73.128.5:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://68.183.221.156:37671
Trying new proxy
Working proxy: http://34.82.117.185:8888
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://200.73.128.5:80
Trying new proxy
Working proxy: http://218.161.36.89:53438
Trying new proxy
Working proxy: http://68.183.221.156:37671
Trying new proxy
Working proxy: http://13.233.253.127:3128
Trying new proxy
Working proxy: http://13.233.253.127:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://13.233.253.127:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://68.183.221.156:37671
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://13.233.253.127:3128
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://18.221.58.204:3838
Trying new proxy
Working proxy: http://91.234.195.154:8888
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://34.91.135.38:80
Trying new proxy
Working proxy: http://200.73.128.5:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://91.234.195.154:8888
Trying new proxy
Working proxy: http://13.233.253.127:3128
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://13.233.253.127:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://139.99.105.5:80
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://13.233.253.127:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://139.99.105.5:80
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://200.73.128.5:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://13.233.253.127:3128
Trying new proxy
Working proxy: http://13.233.253.127:3128
Trying new proxy
Working proxy: http://13.233.253.127:3128
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://13.233.253.127:3128
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://13.233.253.127:3128
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://110.78.23.194:3128
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://125.212.218.43:8888
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://110.78.23.194:3128
Trying new proxy
Working proxy: http://103.142.68.38:8080
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.78.23.194:3128
Trying new proxy
Working proxy: http://18.216.16.30:3838
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://18.221.58.204:3838
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://18.219.130.108:3838
Trying new proxy
Working proxy: http://51.75.73.2:80
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://34.91.135.38:80
Trying new proxy
Working proxy: http://34.91.135.38:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://51.75.73.2:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://180.183.28.196:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://34.91.135.38:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://201.149.34.167:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://34.105.59.26:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://54.37.154.101:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://13.75.114.68:25222
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://54.37.154.101:80
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://18.216.16.30:3838
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://80.23.125.226:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://103.253.27.108:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://80.23.125.226:3128
Trying new proxy
Working proxy: http://45.114.147.54:23500
Trying new proxy
Working proxy: http://110.78.23.194:3128
Trying new proxy
Working proxy: http://110.78.23.194:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://110.78.23.194:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://54.37.154.101:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://34.91.135.38:80
Trying new proxy
Working proxy: http://34.91.135.38:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://13.75.114.68:25222
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://34.91.135.38:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://34.91.135.38:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://34.217.107.252:8888
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://85.47.31.179:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://110.78.23.194:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://34.91.135.38:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://110.78.23.194:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://80.23.125.226:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://139.99.102.114:80
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://54.37.154.101:80
Trying new proxy
Working proxy: http://110.78.23.194:3128
Trying new proxy
Working proxy: http://54.37.154.101:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://54.37.154.101:80
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://192.248.151.236:30084
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://34.91.135.38:80
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://103.136.18.221:80
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://54.37.154.101:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://118.27.18.60:8081
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://18.216.16.30:3838
Trying new proxy
Working proxy: http://18.216.16.30:3838
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://34.217.107.252:8888
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://139.99.102.114:80
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://18.218.173.238:3838
Trying new proxy
Working proxy: http://18.218.173.238:3838
Trying new proxy
Working proxy: http://61.7.184.3:53281
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://110.78.23.194:3128
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://110.78.23.194:3128
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://139.99.105.185:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://61.7.184.3:53281
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://103.28.121.58:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://200.255.122.170:8080
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://80.187.140.74:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://110.164.253.85:8080
Trying new proxy
Working proxy: http://51.89.32.83:3128
Trying new proxy
Working proxy: http://37.19.89.96:8080
Trying new proxy
Working proxy: http://86.124.130.141:3128
Trying new proxy
Working proxy: http://36.79.243.13:3128
Trying new proxy
Working proxy: http://20.54.49.130:80
Trying new proxy
Working proxy: http://85.214.154.178:3128
Trying new proxy
Working proxy: http://200.116.233.36:999
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://197.81.195.28:3128
Trying new proxy
Working proxy: http://37.19.89.96:8080
Trying new proxy
Working proxy: http://172.105.113.134:3128
Trying new proxy
Working proxy: http://88.99.37.202:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://51.89.32.83:3128
Trying new proxy
Working proxy: http://157.230.242.253:3128
Trying new proxy
Working proxy: http://37.19.89.96:8080
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://172.105.113.134:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://134.122.93.160:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://139.99.105.185:80
Trying new proxy
Working proxy: http://202.69.48.122:3128
Trying new proxy
Working proxy: http://163.172.29.22:5836
Trying new proxy
Working proxy: http://85.214.154.178:3128
Trying new proxy
Working proxy: http://86.124.130.141:3128
Trying new proxy
Working proxy: http://161.35.61.201:8888
Trying new proxy
Working proxy: http://139.99.105.185:80
Trying new proxy
Working proxy: http://188.168.75.254:56899
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://161.35.61.201:8888
Trying new proxy
Working proxy: http://37.19.89.96:8080
Trying new proxy
Working proxy: http://161.35.61.201:8888
Trying new proxy
Working proxy: http://161.35.61.201:8888
Trying new proxy
Working proxy: http://172.105.113.134:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://34.217.82.196:8888
Trying new proxy
Working proxy: http://168.232.198.172:8888
Trying new proxy
Working proxy: http://85.214.154.178:3128
Trying new proxy
Working proxy: http://168.232.198.172:8888
Trying new proxy
Working proxy: http://161.35.61.201:8888
Trying new proxy
Working proxy: http://85.214.154.178:3128
Trying new proxy
Working proxy: http://51.89.32.83:3128
Trying new proxy
Working proxy: http://168.232.198.172:8888
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://172.105.113.134:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://200.255.122.170:8080
Trying new proxy
Working proxy: http://118.99.73.126:8080
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://118.99.73.126:8080
Trying new proxy
Working proxy: http://163.172.29.22:5836
Trying new proxy
Working proxy: http://200.255.122.170:8080
Trying new proxy
Working proxy: http://37.19.89.96:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://163.172.29.22:5836
Trying new proxy
Working proxy: http://202.69.48.122:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://139.99.105.5:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://34.82.117.185:8888
Trying new proxy
Working proxy: http://86.124.130.141:3128
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://51.89.32.83:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://85.214.154.178:3128
Trying new proxy
Working proxy: http://202.69.48.122:3128
Trying new proxy
Working proxy: http://34.82.117.185:8888
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://85.47.31.179:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://191.232.170.36:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://200.255.122.170:8080
Trying new proxy
Working proxy: http://85.47.31.179:3128
Trying new proxy
Working proxy: http://200.255.122.170:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://85.47.31.179:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://34.82.117.185:8888
Trying new proxy
Working proxy: http://34.82.117.185:8888
Trying new proxy
Working proxy: http://34.217.107.252:8888
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://191.97.34.31:23500
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://91.234.195.154:8888
Trying new proxy
Working proxy: http://191.232.170.36:80
Trying new proxy
Working proxy: http://200.255.122.170:8080
Trying new proxy
Working proxy: http://18.221.58.204:3838
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://82.200.233.4:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://200.255.122.170:8080
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://34.82.117.185:8888
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://78.141.201.90:33723
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://103.253.27.108:80
Trying new proxy
Working proxy: http://18.218.173.238:3838
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://188.156.240.240:8118
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://182.53.197.156:43060
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://18.221.58.204:3838
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://54.37.154.101:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://149.248.7.26:8081
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://91.234.195.154:8888
Trying new proxy
Working proxy: http://46.218.155.194:3128
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://188.156.240.240:8118
Trying new proxy
Working proxy: http://91.234.195.154:8888
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://139.99.105.5:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://91.234.195.154:8888
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://46.175.70.69:44239
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: http://206.127.88.18:80
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://80.23.125.226:3128
Trying new proxy
Working proxy: None
Trying new proxy
Working proxy: http://83.252.58.232:3128
Trying new proxy
Working proxy: http://199.91.203.210:3128
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://81.201.60.130:80
Trying new proxy
Working proxy: http://144.217.101.245:3129
Trying new proxy
Working proxy: http://83.252.58.232:3128
Trying new proxy
Working proxy: http://64.71.145.122:3128
Trying new proxy
Working proxy: http://83.252.58.232:3128
Trying new proxy
